#!/usr/bin/env python3
"""
ZenDB å®Œæ•´å®ç° - åŸºäºè®ºæ–‡è®¾è®¡çš„å­¦æœ¯æ–‡æ¡£åˆ†æç³»ç»Ÿ
"""

MAX_SENTENCES = 10

import pdfplumber
import sqlite3
import openai
import re
import json
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from datetime import datetime
import time
from openai import OpenAI
import random
from quest.core.datapack.doc import ZenDBDoc
from quest.core.llm.llm_query import LLMInfo
from quest.core.nlp.doc_summary import tfidf_summary
from quest.db.indexer.single_indexer import SingleIndexer
from quest.db.querier.querier import OpenGaussQuerier
import wordninja
import pyparsing as pp
import os
import pickle
from sklearn.metrics.pairwise import cosine_similarity
import faiss

# è®¾ç½®DeepSeek APIå¯†é’¥å’Œé…ç½®
DEEPSEEK_API_KEY = 'sk-DrStrjrlXohoCI2iBc14Eb41A38c428b88A0973aA29fCc3b'
DEEPSEEK_BASE_URL = 'https://aihubmix.com/v1'

# åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url=DEEPSEEK_BASE_URL
)

@dataclass
class SHTNode:
    """è¯­ä¹‰å±‚æ¬¡æ ‘èŠ‚ç‚¹"""
    node_id: int
    doc_id: int
    name: str
    granularity: int
    context: str
    summary: str
    start_pos: int
    end_pos: int
    parent: Optional['SHTNode'] = None
    children: List['SHTNode'] = None
    visual_pattern: Dict[str, Any] = None
    full_context: str = None  # æ–°å¢å­—æ®µï¼ŒåŒ…å«æœ¬èŠ‚ç‚¹åŠæ‰€æœ‰å­èŠ‚ç‚¹çš„å…¨æ–‡
    
    def __post_init__(self):
        if self.children is None:
            self.children = []
        if self.full_context is None:
            self.full_context = self.context
    
    def to_dict(self):
        return {
            "node_id": self.node_id,
            "doc_id": self.doc_id,
            "name": self.name,
            "granularity": self.granularity,
            "context": self.context,
            "summary": self.summary,
            "start_pos": self.start_pos,
            "end_pos": self.end_pos,
            "visual_pattern": self.visual_pattern,
            # "full_context": self.full_context, # ä¸éœ€è¦å­˜full_textï¼Œå¤ªå¤§
            "children": [child.to_dict() for child in self.children],
        }

    @staticmethod
    def from_dict(data, parent=None):
        node = SHTNode(
            node_id= int(data["node_id"]),
            doc_id=int(data["doc_id"]),
            name=data["name"],
            granularity=data["granularity"],
            context=data["context"],
            summary=data.get("summary", ""),
            start_pos= int(data.get("start_pos", 0)),
            end_pos= int(data.get("end_pos", 0)) ,
            parent=parent,
            children=[],
            visual_pattern=data.get("visual_pattern"),
            # full_context=data.get("full_context")
        )
        node.children = [SHTNode.from_dict(child, parent=node) for child in data.get("children", [])]
        return node


    def get_ancestors(self):
        """è·å–ç¥–å…ˆèŠ‚ç‚¹è·¯å¾„"""
        ancestors = []
        current = self.parent
        while current:
            ancestors.append(current.name)
            current = current.parent
        return ancestors[::-1]
    
    def is_leaf(self):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¶å­èŠ‚ç‚¹"""
        return len(self.children) == 0

from quest.conf.settings import ABS_PROJECT_ROOT_PATH, count_tokens

class ZenDBDocIndexer(SingleIndexer):
    """ZenDBIndexerä¸»ç±»"""
    
    def __init__(self, table_name: str, type: str = "ZenDBDoc", root_save_path = os.path.join(ABS_PROJECT_ROOT_PATH, "data/single_index"), need_clean_chunk = False, embedding_model = None, **kwargs):
        print("ğŸš€ å»ºç«‹ZenDB-singleIndexç´¢å¼•...")
        self.embedding_size = embedding_model.emb_size 
        self.embedding_model = embedding_model
        self.need_clean_chunk = need_clean_chunk
        self.table_name = table_name
        self.type = type

        self.table_save_path = os.path.join(root_save_path, f"{self.table_name}.json")
        self.embedding_save_path = os.path.join(root_save_path, f"{self.table_name}_embeddings.pkl")
        self.sht_tables = {}  # doc_id: int -> SHT nodes
        self.user_tables = {}  # table_name -> SQLite table
        self.templates = []    # è·¨æ–‡æ¡£æ¨¡æ¿ç¼“å­˜
        # æ–°å¢embeddingç›¸å…³æˆå‘˜å˜é‡
        self.node_embeddings: Dict[int, np.ndarray] = {}  # node_id -> embeddingå‘é‡
        # self.embedding_cache: Dict[str, np.ndarray] = {}  # ç¼“å­˜ç›¸åŒæ–‡æœ¬çš„embedding
        self.query_embedding_cache: Dict[str, np.ndarray] = {}  # ç¼“å­˜ç›¸åŒæ–‡æœ¬çš„embedding
        # self.querier = OpenGaussQuerier()        

    def build_indexer(self, docs: List[ZenDBDoc]) -> None:
        # æå–å¹¶å­˜å‚¨docs_meta
        self.docs_meta = {doc["doc_id"]: doc.metadata for doc in docs}
        # self.querier.build_cache_table(self.table_name, self.docs_meta)

        self.build_sht_for_all_docs(docs)
        # ä¸ºæ‰€æœ‰èŠ‚ç‚¹ç”Ÿæˆembedding
        self._generate_embeddings_for_all_docs()
        self.save_indexer()
        return

    def save_indexer(self):
        # ä¿å­˜SHTæ ‘ç»“æ„
        path = self.table_save_path
        sht_dict = {doc_id: root.to_dict() for doc_id, root in self.sht_tables.items()}
        data = {
            "docs_meta": self.docs_meta,
            "sht_trees": sht_dict
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜embeddingæ•°æ®
        with open(self.embedding_save_path, "wb") as f:
            pickle.dump(self.node_embeddings, f)

    def load_indexer(self):
        # åŠ è½½SHTæ ‘ç»“æ„
        path = self.table_save_path
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        self.docs_meta = data["docs_meta"]
        # æŠŠå­—å…¸ä¸­æ‰€æœ‰keyè½¬æ¢æˆintç±»å‹
        self.docs_meta = {int(k): v for k, v in self.docs_meta.items()}
        self.sht_tables = {
            int(doc_id) : SHTNode.from_dict(root_dict)
            for doc_id, root_dict in data["sht_trees"].items()
        }
        
        # åŠ è½½embeddingæ•°æ®
        if os.path.exists(self.embedding_save_path):
            with open(self.embedding_save_path, "rb") as f:
                self.node_embeddings = pickle.load(f)
        return

    def _generate_embeddings_for_all_docs(self):
        """ä¸ºæ‰€æœ‰æ–‡æ¡£çš„æ‰€æœ‰èŠ‚ç‚¹ç”Ÿæˆembedding"""
        print("\nğŸ§  ç”ŸæˆèŠ‚ç‚¹embedding...")
        total_nodes = 0
        for doc_id, root in self.sht_tables.items():
            self._generate_embeddings(root)
            total_nodes += self._count_nodes(root)
        print(f"  âœ“ å®Œæˆ {total_nodes} ä¸ªèŠ‚ç‚¹çš„embeddingç”Ÿæˆ")

    def _generate_embeddings(self, node: SHTNode) -> None:
        """ä¸ºèŠ‚ç‚¹ç”Ÿæˆembedding - ä½¿ç”¨æ‰¹é‡åµŒå…¥æé«˜æ•ˆç‡"""
        # è·å–æ‰€æœ‰èŠ‚ç‚¹çš„contextå’Œnode_idåˆ—è¡¨
        context_list, node_id_list = self.level_traverse(node.doc_id)
        
        if not context_list:
            return
        
        # æ‰¹é‡ç”Ÿæˆembedding
        embeddings = self.embedding_model.embed_documents(context_list)
        
        # æ„å»ºnode_idåˆ°embeddingçš„æ˜ å°„
        for node_id, embedding in zip(node_id_list, embeddings):
            self.node_embeddings[node_id] = embedding

    def _get_embedding(self, node_id: int) -> np.ndarray:
        """è·å–èŠ‚ç‚¹çš„contextçš„embeddingå‘é‡"""
        return self.node_embeddings.get(node_id, None)

    def _get_cached_query_embedding(self, query: str):
        """
        è·å–ç¼“å­˜çš„query embeddingï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¡ç®—å¹¶ç¼“å­˜
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            queryçš„embeddingå‘é‡
        """
        cache_key = query[:256]  # ä½¿ç”¨å‰256ä¸ªå­—ç¬¦ä½œä¸ºç¼“å­˜é”®
        
        if cache_key not in self.query_embedding_cache:
            # è®¡ç®—embeddingå¹¶ç¼“å­˜
            self.query_embedding_cache[cache_key] = self.embedding_model.embed_query(query)
        
        return self.query_embedding_cache[cache_key]


    def _semantic_similarity_search(self, query: str, node_ids: List[int], topk: int) -> List[int]:
        """åœ¨æŒ‡å®šèŠ‚ç‚¹é›†åˆä¸­è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ï¼ˆä½¿ç”¨faissåŠ é€Ÿï¼‰"""
        if not node_ids:
            return []
        
        query_embedding = self._get_cached_query_embedding(query)
        # query_embedding = self.embedding_model.embed_query(query)
        # æ”¶é›†èŠ‚ç‚¹embedding
        embeddings = []
        valid_node_ids = []
        for node_id in node_ids:
            emb = self._get_embedding(node_id)
            if emb is not None:
                embeddings.append(emb)
                valid_node_ids.append(node_id)
            else:
                raise ValueError(f"èŠ‚ç‚¹ {node_id} æ²¡æœ‰embedding")
        if not embeddings:
            return []
        # è½¬ä¸ºnpæ•°ç»„
        xb = np.array(embeddings).astype('float32')
        xq = np.array([query_embedding]).astype('float32')
        # å½’ä¸€åŒ–ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(xb)
        faiss.normalize_L2(xq)
        # å»ºç«‹faissç´¢å¼•
        index = faiss.IndexFlatIP(xb.shape[1])
        index.add(xb)
        D, I = index.search(xq, min(topk, xb.shape[0]))
        # è¿”å›å¯¹åº”çš„node_id
        result_ids = [valid_node_ids[i] for i in I[0]]
        return result_ids

    def build_indexer(self, docs: List[ZenDBDoc]) -> None:
        # æå–å¹¶å­˜å‚¨docs_meta
        self.docs_meta = {doc["doc_id"]: doc.metadata for doc in docs}
        # self.querier.build_cache_table(self.table_name, self.docs_meta)

        self.build_sht_for_all_docs(docs)
        # ä¸ºæ‰€æœ‰èŠ‚ç‚¹ç”Ÿæˆembedding
        self._generate_embeddings_for_all_docs()
        self.save_indexer()
        return

    def save_indexer(self):
        # ä¿å­˜SHTæ ‘ç»“æ„
        path = self.table_save_path
        sht_dict = {doc_id: root.to_dict() for doc_id, root in self.sht_tables.items()}
        data = {
            "docs_meta": self.docs_meta,
            "sht_trees": sht_dict
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜embeddingæ•°æ®
        with open(self.embedding_save_path, "wb") as f:
            pickle.dump(self.node_embeddings, f)

    def load_indexer(self):
        # åŠ è½½SHTæ ‘ç»“æ„
        path = self.table_save_path
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        self.docs_meta = data["docs_meta"]
        # æŠŠå­—å…¸ä¸­æ‰€æœ‰keyè½¬æ¢æˆintç±»å‹
        self.docs_meta = {int(k): v for k, v in self.docs_meta.items()}
        self.sht_tables = {
            int(doc_id) : SHTNode.from_dict(root_dict)
            for doc_id, root_dict in data["sht_trees"].items()
        }
        
        # åŠ è½½embeddingæ•°æ®
        if os.path.exists(self.embedding_save_path):
            with open(self.embedding_save_path, "rb") as f:
                self.node_embeddings = pickle.load(f)
        return

    def get_file_name_by_id(self, doc_id: int) -> str:
        """
        æ ¹æ®æ–‡æ¡£IDè·å–å¯¹åº”çš„æ–‡ä»¶å

        Args:
            doc_id: æ–‡æ¡£ç¼–å·

        Returns:
            str: å¯¹åº”çš„æ–‡ä»¶å
        """
        if doc_id in self.docs_meta:
            return self.docs_meta[doc_id].get("file_name", "")
        return ""

    def get_chunks_by_docid(self, doc_id):
        chunk_list, id_list = self.level_traverse(doc_id)
        return chunk_list

    def level_traverse(self, doc_id):
        """
        å±‚åºéå†æŒ‡å®šæ–‡æ¡£çš„åˆ†å—æ ‘ï¼Œè¿”å› (context_list, node_id_list)ã€‚
        """
        root = self.sht_tables.get(doc_id)
        if root is None:
            return [], []
        context_list = []
        node_id_list = []
        queue = [] 
        queue.extend(root.children) # ä¸è¦æŠŠrootèŠ‚ç‚¹çš„contextè¿”å›åˆ°ç»“æœä¸­ï¼Œå› ä¸ºå®ƒåŒ…å«å…¨æ–‡ã€‚
        while queue:
            node = queue.pop(0)
            context_list.append(node.context)
            node_id_list.append(node.node_id)
            queue.extend(node.children)
        return context_list, node_id_list


    def print_sht_tree(self, doc_id):
        """
        ä»¥å‰åºéå†çš„å½¢å¼é€’å½’åœ°æ‰“å°æ•´ä¸ªSHTåˆ†å—æ ‘çš„ç»“æ„ï¼ŒæŒ‰èŠ‚ç‚¹æ‰€åœ¨çš„æ·±åº¦è¿›è¡Œç¼©è¿›ã€‚
        
        Args:
            doc_id: æ–‡æ¡£ç¼–å·
        """
        root = self.sht_tables.get(doc_id)
        if root is None:
            print(f"æ–‡æ¡£ {doc_id} çš„SHTæ ‘ä¸å­˜åœ¨")
            return
        
        def _print_node(node, depth=0):
            """é€’å½’æ‰“å°èŠ‚ç‚¹åŠå…¶å­èŠ‚ç‚¹"""
            indent = "  " * depth
            print(f"{indent}node_id: {node.node_id}")
            for child in node.children:
                _print_node(child, depth + 1)
        
        print(f"æ–‡æ¡£ {doc_id} çš„SHTæ ‘ç»“æ„:")
        _print_node(root)


    def llm_judge_contain_attr(self, node, query):
        """
        è°ƒç”¨LLMï¼Œæ ¹æ®å½“å‰èŠ‚ç‚¹çš„summaryåˆ¤æ–­å½“å‰èŠ‚ç‚¹æ˜¯å¦å¯èƒ½åŒ…å«queryä¸­éœ€è¦æå–çš„å±æ€§ï¼Œ
        llmåªç»™å‡ºtrueæˆ–falseçš„ç»“æœä»¥åŠç»“æœçš„confidence(0åˆ°100çš„æ•´æ•°)ï¼Œ
        è¿”å› (bool, int)ï¼Œå¦‚æœæ ¼å¼ä¸å¯¹åˆ™é»˜è®¤(False, 100)ï¼Œå¹¶ç»Ÿè®¡tokenæ¶ˆè€—ã€‚
        """
        try:
            # æ„é€ æç¤º
            prompt = (
                f"Determine if the following section contains the attribute defined in the query based on the summary of the section.\n"
                f"Query: {query}\n"
                f"Section Title: {node.name}\n"
                f"Section Summary: {node.summary}\n"
                f"Answer format: true or false, confidence from 0 to 100. e.g. (true, 100)"
            )
            # ç»Ÿè®¡è¾“å…¥è¾“å‡ºtokenï¼ˆç®€æ˜“ä¼°ç®—ï¼‰
            input_tokens = count_tokens(prompt)
            response = self._llm_query(prompt)
            output_tokens = count_tokens(response)
            # æ›´æ–°tokenè®¡æ•°å’Œè°ƒç”¨æ¬¡æ•°è®¡æ•°
            LLMInfo.add_input_tokens(input_tokens)
            LLMInfo.add_output_tokens(output_tokens)
            LLMInfo.add_query_times(1)

            # è§£æè¿”å›ç»“æœ
            m = re.search(r"(true|false)\s*[,ï¼Œ]\s*(\d{1,3})", response.lower(), re.IGNORECASE)
            if m:
                result = m.group(1).lower() == 'true'
                confidence = min(max(int(m.group(2)), 0), 100)
                return result, confidence
        except Exception:
            pass
        # é»˜è®¤å€¼
        return False, 0

    def pre_order_dfs_search_related_node(self, node, query):
        """
        åŸºäºDFSçš„å‰åºéå†ï¼Œåˆ¤æ–­å¹¶æ”¶é›†å¯èƒ½åŒ…å«æŸ¥è¯¢å±æ€§çš„èŠ‚ç‚¹ï¼š
          - è‹¥ node ä¸ºç©ºæˆ– llm_judge_contain_attr è¿”å› Falseï¼Œè¿”å› False
          - è‹¥ llm_judge_contain_attr è¿”å› Trueï¼Œåˆ™å¯¹å­èŠ‚ç‚¹é€’å½’è°ƒç”¨ï¼Œ
            å½“ä¸”ä»…å½“æ‰€æœ‰å­èŠ‚ç‚¹é€’å½’å‡è¿”å› False æ—¶ï¼Œå°†å½“å‰èŠ‚ç‚¹åŠ å…¥å€™é€‰
        è¿”å›æ˜¯å¦è¯¥ subtree å¯èƒ½åŒ…å«å±æ€§çš„ bool
        é«˜çº§å‰ªæç­–ç•¥ï¼šåˆ©ç”¨topKï¼Œå¦‚æœå½“å‰åˆ¤æ–­åŒ…å«å±æ€§çš„å—æ•°å·²ç»è¾¾åˆ°topKï¼Œåˆ™ç›´æ¥è¿”å›ã€‚
        """
        if node is None:
            return False
        contain, confidence = self.llm_judge_contain_attr(node, query)
        if not contain:
            return False
        # æ ‡è®°å­èŠ‚ç‚¹ä¸­æ˜¯å¦æœ‰å‘½ä¸­
        child_hit = False
        for child in node.children:
            if self.pre_order_dfs_search_related_node(child, query):
                child_hit = True
        # è‹¥å­èŠ‚ç‚¹éƒ½æœªå‘½ä¸­ï¼Œåˆ™å°†å½“å‰èŠ‚ç‚¹ä½œä¸ºå€™é€‰
        if not child_hit:
            if not hasattr(self, 'candidate_nodes_with_confidence'):
                self.candidate_nodes = []
            self.candidate_nodes.append((node, confidence))
        return True


    def topk_semantic_beam_search_related_node(self, node: SHTNode, query: str, beam_topk: int) -> bool:
        """åŸºäºå±‚æ¬¡åŒ–æŸæœç´¢çš„èŠ‚ç‚¹æœç´¢æ–¹æ³•"""
        self.candidate_nodes = []
        only_one_1_level_title_flag = False
        if beam_topk <= 0:
            return False
        
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰granularity=1çš„èŠ‚ç‚¹
        # granularity_1_nodes = []
        # self._collect_nodes_by_granularity(node, 1, granularity_1_nodes)
        # ç›´æ¥å–node(å³rootèŠ‚ç‚¹)çš„æ‰€æœ‰å­èŠ‚ç‚¹ä½œä¸ºgranularity=1çš„èŠ‚ç‚¹
        # jztodo å¦‚æœ1çº§æ ‡é¢˜åªæœ‰1ä¸ªï¼Œåˆ™æŠŠ2çº§æ ‡é¢˜ä½œä¸ºgranularity_1_nodesã€‚
        granularity_1_nodes = node.children
        if len(granularity_1_nodes) == 1:
            granularity_1_nodes = granularity_1_nodes[0].children
            only_one_1_level_title_flag = True
        
        if len(granularity_1_nodes) == 0 or not granularity_1_nodes:
            # å¦‚æœæ²¡æœ‰granularity=1çš„èŠ‚ç‚¹ï¼Œç›´æ¥è®¤ä¸ºæ•´ç¯‡æ–‡ç« ä¸åŒ…å«attrï¼Œè¿”å›False
            return False
        
        # æŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦æ’åºgranularity=1çš„èŠ‚ç‚¹ï¼Œå–topKä¸ªè¿›è¡ŒLLMåˆ¤æ–­
        node_ids = [n.node_id for n in granularity_1_nodes]
        sorted_node_ids = self._semantic_similarity_search(query, node_ids, beam_topk)
        
        # ç¬¬äºŒé˜¶æ®µï¼šLLMåˆ¤æ–­è¯­ä¹‰ç›¸ä¼¼åº¦æœ€é«˜çš„topKä¸ªgranularity=1èŠ‚ç‚¹
        V1 = []  # å­˜å‚¨è¢«LLMåˆ¤å®šä¸ºtrueçš„granularity=1èŠ‚ç‚¹
        for node_id in sorted_node_ids:
            # æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹
            target_node = None
            for n in granularity_1_nodes:
                if n.node_id == node_id:
                    target_node = n
                    break
            
            if target_node:
                contain, confidence = self.llm_judge_contain_attr(target_node, query)
                if contain:
                    V1.append(target_node)
        
        # ç¬¬ä¸‰ã€å››é˜¶æ®µï¼šå±‚æ¬¡åŒ–æ”¶é›†å­èŠ‚ç‚¹
        all_candidate_nodes = V1.copy()  # ä»V1å¼€å§‹
        current_level_nodes = V1
        
        while current_level_nodes:
            # æ”¶é›†å½“å‰å±‚çº§æ‰€æœ‰èŠ‚ç‚¹çš„å­èŠ‚ç‚¹
            next_level_nodes = []
            for parent_node in current_level_nodes:
                next_level_nodes.extend(parent_node.children)
            
            if not next_level_nodes:
                break
            
            # æŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦æ’åºï¼Œå–topKä¸ª
            child_node_ids = [n.node_id for n in next_level_nodes]
            sorted_child_ids = self._semantic_similarity_search(query, child_node_ids, beam_topk)
            
            # è·å–topKä¸ªå­èŠ‚ç‚¹
            topk_children = []
            for node_id in sorted_child_ids:
                for child in next_level_nodes:
                    if child.node_id == node_id:
                        topk_children.append(child)
                        break
            
            # æ·»åŠ åˆ°å€™é€‰åˆ—è¡¨
            all_candidate_nodes.extend(topk_children)
            current_level_nodes = topk_children
        
        # ç¬¬äº”é˜¶æ®µï¼šåˆå¹¶æ‰€æœ‰å€™é€‰èŠ‚ç‚¹ï¼ŒæŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦æ’åºï¼Œå–æœ€ç»ˆtopK
        if all_candidate_nodes:
            # å»é‡ï¼ˆé¿å…é‡å¤èŠ‚ç‚¹ï¼‰
            unique_nodes = []
            seen_ids = set()
            for node in all_candidate_nodes:
                if node.node_id not in seen_ids:
                    unique_nodes.append(node)
                    seen_ids.add(node.node_id)
            
            # æŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº jztodo å¦‚æœ1çº§æ ‡é¢˜å°±1ä¸ªï¼ŒæŠŠå®ƒä¹ŸåŠ å…¥åˆ°åˆ—è¡¨ä¸­
            final_node_ids = [n.node_id for n in unique_nodes]
            if only_one_1_level_title_flag:
                final_node_ids.append(granularity_1_nodes[0].node_id)
            final_sorted_ids = self._semantic_similarity_search(query, final_node_ids, beam_topk)
            
            # å–topKä¸ªèŠ‚ç‚¹ï¼Œè¿›è¡Œæœ€ç»ˆçš„LLMåˆ¤æ–­å¹¶æ·»åŠ åˆ°å€™é€‰åˆ—è¡¨
            for i, node_id in enumerate(final_sorted_ids[:beam_topk]):
                # æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹
                target_node = None
                for n in unique_nodes:
                    if n.node_id == node_id:
                        target_node = n
                        unique_nodes.remove(n)
                        break
                
                if target_node:
                    self.candidate_nodes.append(target_node)
        
        return len(self.candidate_nodes) > 0

    def _collect_nodes_by_granularity(self, node: SHTNode, target_granularity: int, result_list: List[SHTNode]):
        """é€’å½’æ”¶é›†æŒ‡å®šgranularityçš„èŠ‚ç‚¹"""
        if node.granularity == target_granularity:
            result_list.append(node)
        
        # ç»§ç»­é€’å½’æœç´¢å­èŠ‚ç‚¹
        for child in node.children:
            self._collect_nodes_by_granularity(child, target_granularity, result_list)


    def topk_query_all_nodes(self, doc_id: int, query: str, topk: int) -> List[tuple[str, int]]:
        """
        å¯¹æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰èŠ‚ç‚¹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ï¼Œè¿”å›æœ€ç›¸å…³çš„topKä¸ªèŠ‚ç‚¹
        
        Args:
            doc_id: æ–‡æ¡£ID
            query: æŸ¥è¯¢æ–‡æœ¬
            topk: è¿”å›çš„èŠ‚ç‚¹æ•°é‡
            
        Returns:
            List[tuple[str, int]]: [(node_context, node_id), ...] æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        # è·å–æ ¹èŠ‚ç‚¹
        root = self.sht_tables.get(doc_id)
        if root is None:
            return []
        
        # ä½¿ç”¨level_traverseè·å–æ‰€æœ‰èŠ‚ç‚¹
        context_list, node_id_list = self.level_traverse(doc_id)
        
        if not node_id_list:
            return []
        
        # å¯¹æ‰€æœ‰èŠ‚ç‚¹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
        sorted_node_ids = self._semantic_similarity_search(query, node_id_list, topk)
        
        # æ„é€ ç»“æœåˆ—è¡¨
        results = []
        node_id_to_context = dict(zip(node_id_list, context_list))
        
        for node_id in sorted_node_ids:
            context = node_id_to_context.get(node_id, "")
            results.append((context, node_id))
        
        return results




    def get_relative_chunks_text_with_id(self, doc_id: int, query: str, topk: int) -> List[tuple[str, int]]:
        """
        æŸ¥è¯¢æŒ‡å®šæ–‡æ¡£ä¸­ä¸queryç›¸å…³çš„topkä¸ªæ–‡æœ¬å—
        è¿”å› [(chunk_text, chunk_id), ...]ï¼Œchunk_idä¸ºå—èµ·å§‹ä½ç½®
        """
        # è·å–æ ¹èŠ‚ç‚¹
        root = self.sht_tables.get(doc_id)
        if root is None:
            return []
        
        # é‡ç½®å€™é€‰åˆ—è¡¨
        self.candidate_nodes = []
        
        # åªæœ‰æ ¹èŠ‚ç‚¹çš„ä¸€çº§å­èŠ‚ç‚¹ä¼šå‚ä¸åˆ¤æ–­ï¼Œå…¶ä»–èŠ‚ç‚¹éƒ½æ˜¯ç›´æ¥ç”¨æŸæœç´¢å–topkã€‚
        has_candidate_flag = self.topk_semantic_beam_search_related_node(root, query, topk)  
        # å¦‚æœhas_candidate_flagä¸ºFalseï¼Œé€€åŒ–æˆå¯¹æ‰€æœ‰æ ‘èŠ‚ç‚¹è¿›è¡ŒtopKè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ã€‚å…ˆä½¿ç”¨level_traversalè·å¾—æ‰€æœ‰èŠ‚ç‚¹ï¼Œç„¶åè¿›è¡ŒtopKè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ã€‚        
        if not has_candidate_flag:
            return self.topk_query_all_nodes(doc_id, query, topk)    

        topk_nodes = self.candidate_nodes 
        
        # æ„é€ ç»“æœåˆ—è¡¨
        results = []
        for node  in topk_nodes:
            results.append((node.context, node.node_id))
        
        
        return results

    def depreacated_topk_semantic_dfs_search_related_node(self, node: SHTNode, query: str, remaining_topk: int) -> bool:
        """æ–°çš„åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„DFSæœç´¢æ–¹æ³•"""
        if remaining_topk <= 0:
            return False
        
        # å¯¹å½“å‰èŠ‚ç‚¹è¿›è¡ŒLLMåˆ¤æ–­
        contain, confidence = self.llm_judge_contain_attr(node, query)
        
        if contain and confidence >= 70:
            # å¦‚æœå½“å‰èŠ‚ç‚¹åŒ…å«å±æ€§ï¼ŒåŠ å…¥å€™é€‰é›†åˆ
            if not hasattr(self, 'candidate_nodes_with_confidence'):
                self.candidate_nodes = []
            
            if len(self.candidate_nodes) < remaining_topk:
                self.candidate_nodes.append((node, confidence))
                remaining_topk -= 1
        
        # å¦‚æœè¿˜éœ€è¦æ›´å¤šå€™é€‰èŠ‚ç‚¹ä¸”æœ‰å­èŠ‚ç‚¹ï¼Œç»§ç»­æœç´¢
        if remaining_topk > 0 and node.children:
            # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦å¯¹å­èŠ‚ç‚¹æ’åº
            child_ids = [child.node_id for child in node.children]
            sorted_child_ids = self._semantic_similarity_search(query, child_ids, len(child_ids))
            
            # æŒ‰ç›¸ä¼¼åº¦é¡ºåºå¤„ç†å­èŠ‚ç‚¹
            for child_id in sorted_child_ids:
                if remaining_topk <= 0:
                    break
                
                # æ‰¾åˆ°å¯¹åº”çš„å­èŠ‚ç‚¹
                child_node = None
                for child in node.children:
                    if child.node_id == child_id:
                        child_node = child
                        break
                
                if child_node:
                    if self.depreacated_topk_semantic_dfs_search_related_node(child_node, query, remaining_topk):
                        remaining_topk = max(0, remaining_topk - len(self.candidate_nodes))
        
        return contain

    def _clean_text(self, text):
            """æ·±åº¦æ¸…æ´—æ–‡æœ¬å†…å®¹ï¼Œå»é™¤PDFè½¬æ¢ä¹±ç å’Œæ— æ•ˆå­—ç¬¦"""
            if not text:
                return ""
            
            # 1. ç§»é™¤PDFå­—ç¬¦ç¼–ç ä¹±ç  (cid:XX) æ ¼å¼
            text = re.sub(r'\(cid:\d+\)', '', text)
            
            # 2. ç§»é™¤å…¶ä»–å¸¸è§PDFä¹±ç æ¨¡å¼
            text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)  # æ§åˆ¶å­—ç¬¦
            text = re.sub(r'ï¿½', '', text)  # Unicodeæ›¿æ¢å­—ç¬¦
            
            # 3. ç§»é™¤é‡å¤çš„ç©ºç™½å­—ç¬¦
            text = re.sub(r'\s+', ' ', text)  # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
            text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)  # å¤šä¸ªè¿ç»­æ¢è¡Œæœ€å¤šä¿ç•™ä¸¤ä¸ª
            
            # 4. æ¸…ç†è¡Œé¦–è¡Œå°¾ç©ºæ ¼
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line:  # è·³è¿‡ç©ºè¡Œ
                    cleaned_lines.append(line)
            
            text = '\n'.join(cleaned_lines)
            
            # 5. ç§»é™¤æ˜æ˜¾çš„é¡µçœ‰é¡µè„šæ¨¡å¼ï¼ˆå•ç‹¬çš„æ•°å­—å¯èƒ½æ˜¯é¡µç ï¼‰
            lines = text.split('\n')
            filtered_lines = []
            for line in lines:
                # è·³è¿‡å¯èƒ½çš„é¡µç ï¼ˆå•ç‹¬çš„æ•°å­—æˆ–çŸ­æ¨ªçº¿ï¼‰
                if re.match(r'^\s*[-\d\s]+\s*$', line) and len(line.strip()) < 5:
                    continue
                # è·³è¿‡åªæœ‰ç‰¹æ®Šå­—ç¬¦çš„è¡Œ
                if re.match(r'^[\s\-_=~`!@#$%^&*(){}[\]|\\:";\'<>?,.]+$', line):
                    continue
                filtered_lines.append(line)
            
            text = '\n'.join(filtered_lines)
            
            # 6. æœ€ç»ˆæ ¼å¼åŒ–
            text = text.strip()
            
            # ç¡®ä¿æ–‡æœ¬æœ‰æ„ä¹‰ï¼ˆè‡³å°‘æœ‰ä¸€äº›å­—æ¯æ•°å­—å­—ç¬¦ï¼‰
            if len(re.findall(r'[a-zA-Z0-9]', text)) < 10:
                return ""
            
            return text


    def _extract_phrases(self, doc):
        def smart_split_phrase(phrase):
        # åŒ¹é…ç¼–å·+æ­£æ–‡
            m = re.match(r'^((?:\d+\.)*\d+)\s*([A-Za-z]+.*)', phrase)
            if m:
                prefix = m.group(1)
                rest = m.group(2)
                # å¯¹æ­£æ–‡éƒ¨åˆ†åˆ†è¯
                rest_split = ' '.join(wordninja.split(rest))
                return f"{prefix} {rest_split}"
            # åªå¯¹å…¨è‹±æ–‡é•¿ä¸²åˆ†è¯
            if re.match(r'^[A-Za-z]{10,}$', phrase):
                return ' '.join(wordninja.split(phrase))
            return phrase
            
        """æ ¹æ®è§†è§‰ç‰¹å¾è¿ç»­åˆå¹¶çŸ­è¯­ï¼Œç¬¦åˆè®ºæ–‡å®šä¹‰"""
        words = doc["words"]
        if (not words) or  (doc["file_type"] == "txt") or (doc["file_type"] == "md"):
                # é’ˆå¯¹txtå’Œmdï¼ŒæŒ‰mdæ ‡é¢˜æ¥åˆ†phrases
            line  = doc["text"]
            phrases = []
            lines = [line.strip() for line in doc["text"].splitlines() if line.strip()]

            normal_features = {
                "size": 12,  # é»˜è®¤
                "fontname": "normal_text",
                "type": {"bold": False, "underline": False},
                "all_cap": line.isupper(),
                "num_st": line[:1].isdigit(),
                "alpha_st": line[:1].isalpha(),
                "center": False
            }

            title_features = {
                "size": 12,  # é»˜è®¤
                "fontname": "title",
                "type": {"bold": True, "underline": False},
                "all_cap": line.isupper(),
                "num_st": line[:1].isdigit(),
                "alpha_st": line[:1].isalpha(),
                "center": False
            }

            #####################################


            i = 0
            while i < len(lines):
                line = lines[i]
                # æ£€æŸ¥æ˜¯å¦ä¸ºmarkdownæ ‡é¢˜
                header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
                
                if header_match:
                    # å¦‚æœå‘ç°æ˜¯markdownæ ‡é¢˜
                    header_level = len(header_match.group(1))  # è·å–#çš„æ•°é‡
                    header_text = line  # è·å–å®Œæ•´æ ‡é¢˜ï¼ŒåŒ…æ‹¬å‰é¢çš„#
                    
                    # è®¾ç½®titleç‰¹å¾ï¼Œfontnameä¸ºtitle{level}
                    features = title_features.copy()
                    features["fontname"] = f"title{header_level}"
                    features["type"]["bold"] = True
                    
                    # æ·»åŠ åˆ°phrases
                    phrases.append((header_text, features))
                    i += 1
                else:
                    # ä¸æ˜¯æ ‡é¢˜ï¼Œæ”¶é›†è¿ç»­çš„éæ ‡é¢˜æ–‡æœ¬
                    normal_text = line
                    normal_features_copy = normal_features.copy()
                    
                    # å‘å‰æ”¶é›†è¿ç»­çš„éæ ‡é¢˜è¡Œ
                    j = i + 1
                    while j < len(lines):
                        next_line = lines[j]
                        # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦ä¸ºæ ‡é¢˜
                        if re.match(r'^(#{1,6})\s+(.+)$', next_line):
                            break  # é‡åˆ°æ ‡é¢˜è¡Œï¼Œåœæ­¢åˆå¹¶
                        normal_text += '\n' + next_line  # åˆå¹¶éæ ‡é¢˜è¡Œ
                        j += 1
                    
                    # æ·»åŠ åˆå¹¶åçš„æ­£æ–‡
                    phrases.append((normal_text, normal_features_copy))
                    
                    # è·³è¿‡å·²å¤„ç†çš„è¡Œ
                    i = j

            return phrases
            # for line in lines:
            #     features = normal_features

            #     # jztodo å¦‚æœå‘ç°æ˜¯markdownæ ‡é¢˜ï¼Œå…ˆæŠŠä¹‹å‰çš„normal_lines(å¦‚æœæœ‰çš„è¯)åˆå¹¶æˆä¸€ä¸ªphraseï¼Œç„¶åå†æŠŠtitle_featuresçš„fontnameè®¾ç½®æˆtitle{n}ï¼Œnæ˜¯markdownæ ‡é¢˜çš„çº§åˆ«, type -> boldè®¾ç½®æˆTrueï¼Œç„¶åappendåˆ°phrasesé‡Œé¢
            #     # å¦‚æœä¸æ˜¯æ ‡é¢˜ï¼Œåˆ™å’Œå…¶ä»–çš„lineåˆå¹¶ã€‚             
            #     phrases.append((line, features))

            # return phrases


            #################################

        phrases = []
        current_phrase = ""
        current_features = None
        current_visual = None

        def get_word_visual(word):
            # åªè€ƒè™‘å­—ä½“åã€å¤§å°ã€ç±»å‹ï¼ˆåŠ ç²—/ä¸‹åˆ’çº¿ï¼‰
            fontname = word.get("fontname", "")
            size = word.get("size", 12)
            is_bold = int("bold" in fontname.lower() or "Bold" in fontname)
            is_underline = int("underline" in fontname.lower())
            return (size, fontname, is_bold, is_underline)

        for i, word in enumerate(words):
            word_text = word.get("text", "")
            visual = get_word_visual(word)

            if current_phrase and visual != current_visual:
                # ç”ŸæˆçŸ­è¯­çº§è§†è§‰ç‰¹å¾
                phrase_text = current_phrase.strip()
                # è‡ªåŠ¨åœ¨ç¼–å·å’Œæ­£æ–‡ä¹‹é—´åŠ ç©ºæ ¼
                phrase_text = smart_split_phrase(phrase_text)
                
                features = {
                    "size": current_visual[0],
                    "fontname": current_visual[1],
                    "type": {
                        "bold": bool(current_visual[2]),
                        "underline": bool(current_visual[3])
                    },
                    "all_cap": phrase_text.isupper(),
                    "num_st": phrase_text[:1].isdigit(),
                    "alpha_st": phrase_text[:1].isalpha(),
                    "center": False  # å¯æ ¹æ®x0/x1åˆ¤æ–­æ˜¯å¦å±…ä¸­
                }
                phrases.append((phrase_text, features))
                current_phrase = word_text
                current_visual = visual
            else:
                current_phrase += ("" if current_phrase else "") + word_text
                current_visual = visual

        # æœ€åä¸€ä¸ªçŸ­è¯­
        if current_phrase:
            phrase_text = current_phrase.strip()
            features = {
                "size": current_visual[0],
                "fontname": current_visual[1],
                "type": {
                    "bold": bool(current_visual[2]),
                    "underline": bool(current_visual[3])
                },
                "all_cap": phrase_text.isupper(),
                "num_st": phrase_text[:1].isdigit(),
                "alpha_st": phrase_text[:1].isalpha(),
                "center": False
            }
            phrases.append((phrase_text, features))


        return phrases
    
    def _build_sht(self, doc):
        return self._build_markdown_sht(doc)



    def _build_markdown_sht(self, doc):
        # jzmod
        """æ„å»ºè¯­ä¹‰å±‚çº§æ ‘"""
        print(f"\nğŸ—ï¸ ä¸ºæ–‡æ¡£ '{doc['name']}' æ„å»ºSHT...")
        
        # 1. æå–çŸ­è¯­
        phrases = self._extract_phrases(doc)
        print(f"  âœ“ æå– {len(phrases)} ä¸ªçŸ­è¯­")
        
        # 2. è¯†åˆ«æ ‡é¢˜çŸ­è¯­
        headers = self._identify_headers(phrases)
        print(f"  âœ“ è¯†åˆ« {len(headers)} ä¸ªæ ‡é¢˜")
        
        # 3. æ„å»ºæ ‘ç»“æ„
        root = self._build_tree_structure(headers, doc)
        print(f"  âœ“ æ„å»ºå®Œæˆï¼Œæ ¹èŠ‚ç‚¹åŒ…å« {len(root.children)} ä¸ªå­èŠ‚ç‚¹")
        
        # 4. ç”Ÿæˆæ‘˜è¦
        self._generate_summaries(root)
        print(f"  âœ“ ç”ŸæˆèŠ‚ç‚¹æ‘˜è¦")
        
        return root

    def _identify_headers(self, phrases):
        # ç›´æ¥æ ¹æ®featuresåˆ¤æ–­å“ªäº›æ˜¯æ ‡é¢˜ã€‚
        headers = []
        for phrase, features in phrases:
            if self._is_markdown_header(features):
                headers.append((phrase, features))

        return headers

    def _build_pdf_sht(self, doc):
        """æ„å»ºè¯­ä¹‰å±‚æ¬¡æ ‘"""
        print(f"\nğŸ—ï¸ ä¸ºæ–‡æ¡£ '{doc['name']}' æ„å»ºSHT...")
        
        # 1. æå–çŸ­è¯­
        phrases = self._extract_phrases(doc)
        print(f"  âœ“ æå– {len(phrases)} ä¸ªçŸ­è¯­")
        
        # 2. è¯†åˆ«æ ‡é¢˜çŸ­è¯­
        headers = self._identify_pdf_headers(phrases)
        print(f"  âœ“ è¯†åˆ« {len(headers)} ä¸ªæ ‡é¢˜")
        
        # 3. æ„å»ºæ ‘ç»“æ„
        root = self._build_pdf_tree_structure(headers, doc)
        print(f"  âœ“ æ„å»ºå®Œæˆï¼Œæ ¹èŠ‚ç‚¹åŒ…å« {len(root.children)} ä¸ªå­èŠ‚ç‚¹")
        
        # 4. ç”Ÿæˆæ‘˜è¦
        self._generate_summaries(root)
        print(f"  âœ“ ç”ŸæˆèŠ‚ç‚¹æ‘˜è¦")
        
        return root
    
    def _identify_pdf_headers(self, phrases, k=10, n_clusters=8):
        """æ ¹æ®è§†è§‰ç‰¹å¾èšç±»+LLMè¯†åˆ«æ ‡é¢˜çŸ­è¯­ï¼Œç¬¦åˆè®ºæ–‡å®šä¹‰"""
        if not phrases:
            return []

        # å…ˆç”¨å¯å‘å¼è§„åˆ™è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯æ ‡é¢˜çš„çŸ­è¯­
        # phrases = [(phrase, info) for phrase, info in phrases if self._is_likely_header(phrase)]
        if not phrases:
            return []

        # 1. æå–è§†è§‰ç‰¹å¾å‘é‡
        features = []
        for phrase, info in phrases:
            size = info['size']
            fontname = hash(info['fontname']) % 10000
            bold = int(info['type']['bold'])
            underline = int(info['type']['underline'])
            features.append([size, fontname, bold, underline])

        # 2. èšç±»
        kmeans = KMeans(n_clusters=min(n_clusters, len(phrases)), random_state=42)
        cluster_ids = kmeans.fit_predict(features)

        # 3. ç»™æ¯ä¸ªphraseåŠ cluster id
        for idx, (phrase, info) in enumerate(phrases):
            info['cluster'] = int(cluster_ids[idx])

        # 4. æŒ‰clusteråˆ†ç»„
        cluster_groups = {}
        for phrase, info in phrases:
            cluster = info['cluster']
            if cluster not in cluster_groups:
                cluster_groups[cluster] = []
            cluster_groups[cluster].append((phrase, info))

        headers = []
        headers_set = set()
        for cluster, cluster_phrases in cluster_groups.items():
            # é‡‡æ ·å‰å†ç”¨å¯å‘å¼è¿‡æ»¤
            # filtered_cluster_phrases = [p for p in cluster_phrases if self._is_likely_header(p[0])]
            filtered_cluster_phrases = cluster_phrases
            if not filtered_cluster_phrases:
                continue
            sample = random.sample(filtered_cluster_phrases, min(k, len(filtered_cluster_phrases)))
            header_votes = 0
            for phrase, info in sample:
                prompt = (
        "You are analyzing a document. "
        "Section headers in a document typically indicate the start of a new section or subsection, such as chapter titles, main headings, or important topic divisions. "
        "Section headers are often short, descriptive, and may be formatted differently from the main text. "
        "Given the following phrase from the document:\n\n"
        f'"{phrase}"\n\n'
        "Is this phrase a section or subsection header in the document? "
        "Answer only 'yes' or 'no'."
)

                try:
                    response = self._llm_query(prompt)
                    #print(f"LLMåˆ¤æ–­: '{phrase}' -> {response}")
                    if response.strip().lower().startswith('y'):
                        header_votes += 1
                except Exception as e:
                    print(f"LLMå¼‚å¸¸: {e}")
                    continue
            if header_votes > len(sample) / 2:
                for phrase, info in filtered_cluster_phrases:
                    if phrase not in headers_set:
                        headers.append((phrase, cluster))
                        headers_set.add(phrase)
        return headers
    
    def _is_markdown_header(self, features):
        if  "title" in features["fontname"]:
            return True
        else:
            return False

    def _is_likely_header(self, phrase):
        """å¯å‘å¼è§„åˆ™åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜"""
        phrase = phrase.strip()
        
        # æ ‡é¢˜ç‰¹å¾

        if len(phrase) < 5 or len(phrase) > 100:
            return False
            
        # åŒ…å«å¸¸è§æ ‡é¢˜æ¨¡å¼
        header_patterns = [
            r'^\d+\.?\s+',  # æ•°å­—å¼€å¤´
            r'^[A-Z][A-Z\s]+$',  # å…¨å¤§å†™
            r'(abstract|introduction|related work|methodology|experiments|conclusion)',  # å¸¸è§æ ‡é¢˜è¯
            r'^(figure|table|algorithm)\s+\d+',  # å›¾è¡¨æ ‡é¢˜
        ]
        
        for pattern in header_patterns:
            if re.search(pattern, phrase, re.IGNORECASE):
                return True
                
        return False
    
    def _deprecated2_build_pdf_tree_structure(self, headers, doc):
        """æ„å»ºæ ‘å½¢ç»“æ„"""
        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root = SHTNode(
            node_id= 0 , # f"{doc['id']}_root",
            doc_id=doc['doc_id'],
            name=f"Document: {doc['name']}",
            granularity=0,
            context=doc['text'][:500],  # å‰500å­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
            summary="",
            start_pos=0,
            end_pos=len(doc['text'])
        )
        
        if not headers:
            return root
        
        # æŒ‰æ–‡æ¡£ä¸­çš„å‡ºç°é¡ºåºæ’åºæ ‡é¢˜
        sorted_headers = sorted(headers, key=lambda x: doc['text'].find(x[0]))
        
        # æ„å»ºå±‚æ¬¡ç»“æ„
        node_stack = [root]
        
        for i, (header_text, cluster) in enumerate(sorted_headers):
            # ç¡®å®šå±‚çº§
            granularity = self._determine_pdf_granularity(header_text)
            
            # æ‰¾åˆ°åˆé€‚çš„çˆ¶èŠ‚ç‚¹
            while len(node_stack) > 1 and node_stack[-1].granularity >= granularity:
                node_stack.pop()
            
            parent = node_stack[-1]
            
            # è®¡ç®—æ–‡æœ¬ä½ç½®
            start_pos = doc['text'].find(header_text)
            if start_pos == -1:
                start_pos = 0
            
            # ä¸‹ä¸€ä¸ªæ ‡é¢˜çš„ä½ç½®ä½œä¸ºç»“æŸä½ç½®
            if i + 1 < len(sorted_headers):
                next_header = sorted_headers[i + 1][0]
                end_pos = doc['text'].find(next_header)
                if end_pos == -1:
                    end_pos = len(doc['text'])
            else:
                end_pos = len(doc['text'])
            
            # æå–è¯¥èŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡
            context = doc['text'][start_pos:end_pos]
            if self.need_clean_chunk:
                context = self._clean_text(context)
            
            # åˆ›å»ºèŠ‚ç‚¹
            node = SHTNode(
                node_id= i+1 ,  # f"{doc['id']}_node_{i}",
                doc_id=doc['doc_id'],
                name=header_text.strip(),
                granularity=granularity,
                context=context,
                summary="",
                start_pos=start_pos,
                end_pos=end_pos,
                parent=parent
            )
            parent.children.append(node)
            node_stack.append(node)
        # åˆå¹¶å…¨æ–‡context
        self._merge_full_context(root)
        return root


    def _build_tree_structure(self, headers, doc):
        """æ„å»ºæ ‘å½¢ç»“æ„"""
        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root = SHTNode(
            node_id= 0 , # f"{doc['id']}_root",
            doc_id=doc['doc_id'],
            name=f"Document: {doc['name']}",
            granularity=0,
            context=doc['text'],  # æ ¹èŠ‚ç‚¹-å…¨æ–‡ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
            summary="",
            start_pos=0,
            end_pos=len(doc['text'])
        )
        
        if not headers:
            return root
        
        # æ„å»ºæ ‡é¢˜ä½ç½®æ˜ å°„ï¼Œå¤„ç†é‡å¤æ ‡é¢˜é—®é¢˜
        header_positions = self._find_all_header_positions(headers, doc['text'])
        
        # æŒ‰å®é™…ä½ç½®æ’åºæ ‡é¢˜
        sorted_headers = sorted(header_positions, key=lambda x: x[2])  # æŒ‰positionæ’åº
        
        # æ„å»ºå±‚æ¬¡ç»“æ„
        node_stack = [root]
        
        for i, (header_text, cluster, start_pos) in enumerate(sorted_headers):
            # ç¡®å®šå±‚çº§
            granularity = self._determine_granularity(header_text)
            
            # æ‰¾åˆ°åˆé€‚çš„çˆ¶èŠ‚ç‚¹
            while len(node_stack) > 1 and node_stack[-1].granularity >= granularity:
                node_stack.pop()
            
            parent = node_stack[-1]
            
            # ä¸‹ä¸€ä¸ªæ ‡é¢˜çš„ä½ç½®ä½œä¸ºç»“æŸä½ç½®
            if i + 1 < len(sorted_headers):
                end_pos = sorted_headers[i + 1][2]  # ä½¿ç”¨ä¸‹ä¸€ä¸ªæ ‡é¢˜çš„å®é™…ä½ç½®
            else:
                end_pos = len(doc['text'])
            
            # æå–è¯¥èŠ‚ç‚¹çš„ä¸Šä¸‹æ–‡
            context = doc['text'][start_pos:end_pos]
            if self.need_clean_chunk:
                context = self._clean_text(context)
            
            # åˆ›å»ºèŠ‚ç‚¹
            node = SHTNode(
                node_id= i+1 ,  # f"{doc['id']}_node_{i}", æ ¹èŠ‚ç‚¹æ˜¯0
                doc_id=doc['doc_id'],
                name=header_text.strip(),
                granularity=granularity,
                context=context,
                summary="",
                start_pos=start_pos,
                end_pos=end_pos,
                parent=parent
            )
            parent.children.append(node)
            node_stack.append(node)
        # åˆå¹¶å…¨æ–‡context
        self._merge_full_context(root)
        return root

    def _find_all_header_positions(self, headers, text):
        """
        æ‰¾åˆ°æ‰€æœ‰æ ‡é¢˜åœ¨æ–‡æœ¬ä¸­çš„å®é™…ä½ç½®ï¼Œå¤„ç†é‡å¤æ ‡é¢˜é—®é¢˜
        
        Args:
            headers: [(header_text, cluster), ...]
            text: æ–‡æ¡£å…¨æ–‡
            
        Returns:
            [(header_text, cluster, position), ...] æŒ‰å‡ºç°é¡ºåºæ’åº
        """
        header_positions = []
        used_positions = set()  # è®°å½•å·²ä½¿ç”¨çš„ä½ç½®ï¼Œé¿å…é‡å¤
        
        for header_text, cluster in headers:
            # æ‰¾åˆ°è¯¥æ ‡é¢˜çš„æ‰€æœ‰å‡ºç°ä½ç½®
            start = 0
            while True:
                pos = text.find(header_text, start)
                if pos == -1:
                    break
                    
                # å¦‚æœè¿™ä¸ªä½ç½®è¿˜æ²¡è¢«ä½¿ç”¨è¿‡ï¼Œåˆ™æ·»åŠ 
                if pos not in used_positions:
                    header_positions.append((header_text, cluster, pos))
                    used_positions.add(pos)
                    break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœªä½¿ç”¨çš„ä½ç½®å°±è·³å‡º
                    
                start = pos + 1  # ç»§ç»­æŸ¥æ‰¾ä¸‹ä¸€ä¸ªä½ç½®
        
        return header_positions


    def _merge_full_context(self, node):
        """é€’å½’åˆå¹¶æœ¬èŠ‚ç‚¹åŠæ‰€æœ‰å­èŠ‚ç‚¹çš„contextåˆ°full_context"""
        if not node.children:
            node.full_context = node.context
        else:
            merged = node.context + '\n'
            for child in node.children:
                self._merge_full_context(child)
                merged += child.full_context + '\n'
            node.full_context = merged

    def _determine_granularity(self, header_text):
        header_match = re.match(r'^(#{1,6})\s+(.+)$', header_text )
        header_level = len(header_match.group(1))  # è·å–#çš„æ•°é‡
        return header_level

    def _determine_pdf_granularity(self, header_text):
        header = header_text.strip()
        # åŒ¹é…ç¼–å·ï¼ˆå¦‚ 1ã€1.1ã€1.1.1ã€1.1.1.1ï¼‰
        m = re.match(r'^((\d+)(\.\d+)*)', header)
        if m:
            # å±‚çº§æ•° = ç‚¹çš„æ•°é‡ + 1
            level = m.group(1).count('.') + 1
            return level
        elif header.isupper() and len(header) < 50:  # ABSTRACT ç­‰å…¨å¤§å†™çŸ­æ ‡é¢˜
            return 1
        else:
            return 1  # é»˜è®¤å±‚çº§



    def _generate_summaries(self, node):
        """ä¸ºèŠ‚ç‚¹ç”Ÿæˆæ‘˜è¦"""
        if not node:
            return
        
        # ç”ŸæˆèŠ‚ç‚¹æ‘˜è¦
        if node.context:
            # æå–å…³é”®å¥å­ä½œä¸ºæ‘˜è¦
            sentences = self._extract_key_sentences(node.context, max_sentences=MAX_SENTENCES)
            ancestors_path = " â†’ ".join(node.get_ancestors())
            
            node.summary = f"""Title: {node.name}
Ancestors: {ancestors_path}
Key Content: {sentences}"""
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.children:
            self._generate_summaries(child)
    
    def _extract_key_sentences(self, text, max_sentences=MAX_SENTENCES):
        """æå–å…³é”®å¥å­"""
        if not text:
            return ""        
        summary = tfidf_summary(text, max_sentences)
        return summary
    


    def _find_node_by_id(self, root, node_id):
        """æ ¹æ®IDæ‰¾åˆ°èŠ‚ç‚¹"""
        if root.node_id == node_id:
            return root
        
        for child in root.children:
            result = self._find_node_by_id(child, node_id)
            if result:
                return result
        
        return None
    
    # è¾…åŠ©æ¨¡å—
    def _llm_query(self, prompt, model="gpt-4.1", max_retries=3):
        """LLMè°ƒç”¨å°è£…"""
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0,
                    max_tokens=500
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"    âš ï¸ LLMè°ƒç”¨å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{max_retries}: {e}")
                    time.sleep(1)
                else:
                    raise e
    
    # ä¸»è¦å·¥ä½œæµç¨‹
    def build_sht_for_all_docs(self, docs):
        """ä¸ºæ‰€æœ‰æ–‡æ¡£æ„å»ºSHT"""
        print("\nğŸ—ï¸ ä¸ºæ‰€æœ‰æ–‡æ¡£æ„å»ºè¯­ä¹‰å±‚æ¬¡æ ‘...")
        
        for doc in docs:
            sht_root = self._build_sht(doc)
            self.sht_tables[doc['doc_id']] = sht_root
            
        
        print(f"âœ“ å®Œæˆ {len(self.sht_tables)} ä¸ªæ–‡æ¡£çš„SHTæ„å»º")

    def get_docs_id(self) -> list[int]:
        """
        è·å–è¡¨ä¸­æ‰€æœ‰æ–‡æ¡£çš„doc_id

        Returns:
            list[int]: æ–‡æ¡£IDåˆ—è¡¨
        """
        return list(self.docs_meta.keys())


    def _count_nodes(self, node):
        """è®¡ç®—èŠ‚ç‚¹æ€»æ•°"""
        count = 1
        for child in node.children:
            count += self._count_nodes(child)
        return count
    
    def _get_max_depth(self, node):
        """è·å–æœ€å¤§æ·±åº¦"""
        if not node.children:
            return node.granularity
        
        return max(self._get_max_depth(child) for child in node.children)

