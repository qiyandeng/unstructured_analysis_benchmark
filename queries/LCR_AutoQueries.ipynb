{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pickle-mixin in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: openpyxl in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: pandas in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: collections-extended in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/liujinqi/anaconda3/envs/Benchmark/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pickle-mixin openpyxl pandas tqdm collections-extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82280/1122319782.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import openpyxl\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义输入输出路径，并加载数据\n",
    "- ```dataset_dir```：标注好的数据集表格路径\n",
    "- ```statistics_output_dir```：统计数据表输出路径，包括属性值、选择率、基数\n",
    "- ```valid_where_output_dir```：所有有效谓词组合的输出路径\n",
    "- **注意**：区分标注好的表格是 .csv 还是 .xlsx 格式，相应改变 pandas 读取形式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dir = r\"/home/sunzhaoze/Benchmark/LCR_All_Attr.csv\"\n",
    "dataset_dir = r\"/data2/liujinqi/Benchmark/Query/LCR_AutoConstruct/LCR_All_Attr.xlsx\"\n",
    "statistics_output_dir = r\"/data2/liujinqi/Benchmark/Query/LCR_AutoConstruct/LCR_Statistics.csv\"\n",
    "valid_where_output_dir = r\"/data2/liujinqi/Benchmark/Query/LCR_AutoConstruct/valid_WHERE.json\"\n",
    "# df = pd.read_csv(dataset_dir)\n",
    "df = pd.read_excel(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义属性，方便后面按不同属性类型设计不同的构造方法\n",
    "- ```attr_desc_dict```：全部属性的集合，以及对应的自然语言描述\n",
    "- ```non_numerical_attr```：非数值属性的集合\n",
    "- ```numerical_attr```：数值属性的集合\n",
    "- ```non_formatted_attr```：非格式化数值属性\n",
    "- ```formatted_attr```：格式化数值属性，如日期\n",
    "- ```category_attr```：固定类别的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_desc_dict = {\n",
    "     \"judge_name\": \"\",\n",
    "     \"plaintiff\": \"\",\n",
    "     \"defendant\": \"\",\n",
    "     \"hearing_year\": \"\",\n",
    "     \"judgment_year\": \"\",\n",
    "     \"charges\": \"\",\n",
    "     \"case_type\": \"\",\n",
    "     \"verdict\": \"\",\n",
    "     \"legal_basis_num\": \"\",\n",
    "     \"case_num\": \"\",\n",
    "     \"counsel_for_applicant\": \"\",\n",
    "     \"counsel_for_respondent\": \"\",\n",
    "     \"nationality_for_applicant\": \"\",\n",
    "     \"fine_amount\": \"\",\n",
    "     \"legal_fees\": \"\",\n",
    "     \"plaintiff_current_status\": \"\",\n",
    "     \"defendant_current_status\": \"\",\n",
    "     \"evidence\": \"\",\n",
    "     \"first_judge\": \"\",\n",
    "}\n",
    "non_numerical_attr = [\"judge_name\", \"plaintiff\", \"defendant\", \"charges\", \n",
    "                      \"case_type\", \"verdict\", \"counsel_for_applicant\", \"counsel_for_respondent\", \n",
    "                      \"nationality_for_applicant\",  \"plaintiff_current_status\" \n",
    "                      \"defendant_current_status\", \"evidence\", \"first_judge\"]\n",
    "numerical_attr = [\"hearing_year\", \"judgment_year\", \"legal_fees\", \"legal_basis_num\", \"case_num\", \"fine_amount\"]\n",
    "non_formatted_attr = [\"legal_fees\", \"legal_basis_num\", \"case_num\", \"fine_amount\"]\n",
    "formatted_attr = [\"hearing_year\", \"judgment_year\"]\n",
    "category_attr = [\"verdict\",  \"case_type\", \"evidence\", \"first_judge\"]\n",
    "multi_value_attributes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成统计信息\n",
    "- 属性 | 属性值 | 选择率 | 基数\n",
    "- 用于后续构造 Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judge_name</th>\n",
       "      <th>Count</th>\n",
       "      <th>Selectivity</th>\n",
       "      <th>plaintiff</th>\n",
       "      <th>Count</th>\n",
       "      <th>Selectivity</th>\n",
       "      <th>defendant</th>\n",
       "      <th>Count</th>\n",
       "      <th>Selectivity</th>\n",
       "      <th>hearing_year</th>\n",
       "      <th>...</th>\n",
       "      <th>Selectivity</th>\n",
       "      <th>defendant_current_status</th>\n",
       "      <th>Count</th>\n",
       "      <th>Selectivity</th>\n",
       "      <th>evidence</th>\n",
       "      <th>Count</th>\n",
       "      <th>Selectivity</th>\n",
       "      <th>first_judge</th>\n",
       "      <th>Count</th>\n",
       "      <th>Selectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tracey</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.052</td>\n",
       "      <td>Australian Competition and Consumer Commission</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.025</td>\n",
       "      <td>Minister for Immigration and Citizenship</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.269</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370</td>\n",
       "      <td>Government</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.567</td>\n",
       "      <td>1</td>\n",
       "      <td>467.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flick</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.048</td>\n",
       "      <td>Australian Securities and Investments Commission</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>Minister for Immigration and Multicultural Aff...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2006-11-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096</td>\n",
       "      <td>Company</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.220</td>\n",
       "      <td>1</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Greenwood</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.043</td>\n",
       "      <td>Cadbury Schweppes Pty Ltd</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>Minister for Immigration and Multicultural and...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.044</td>\n",
       "      <td>2006-03-09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057</td>\n",
       "      <td>Organization</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.102</td>\n",
       "      <td>Not explicitly mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>(null)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Collier</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.030</td>\n",
       "      <td>Commissioner of Taxation</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>Commissioner of Taxation</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2008-05-19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045</td>\n",
       "      <td>Employee</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>(null)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spender</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.030</td>\n",
       "      <td>Australian Prudential Regulation Authority</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Repatriation Commission</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2008-05-20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039</td>\n",
       "      <td>Chief Executive Officer</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>548 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    judge_name Count  Selectivity  \\\n",
       "0       Tracey  31.0        0.052   \n",
       "1        Flick  29.0        0.048   \n",
       "2    Greenwood  26.0        0.043   \n",
       "3      Collier  18.0        0.030   \n",
       "4      Spender  18.0        0.030   \n",
       "..         ...   ...          ...   \n",
       "543        NaN   NaN          NaN   \n",
       "544        NaN   NaN          NaN   \n",
       "545        NaN   NaN          NaN   \n",
       "546        NaN   NaN          NaN   \n",
       "547        NaN   NaN          NaN   \n",
       "\n",
       "                                            plaintiff Count  Selectivity  \\\n",
       "0      Australian Competition and Consumer Commission  15.0        0.025   \n",
       "1    Australian Securities and Investments Commission   5.0        0.008   \n",
       "2                           Cadbury Schweppes Pty Ltd   5.0        0.008   \n",
       "3                            Commissioner of Taxation   4.0        0.007   \n",
       "4          Australian Prudential Regulation Authority   3.0        0.005   \n",
       "..                                                ...   ...          ...   \n",
       "543                                               NaN   NaN          NaN   \n",
       "544                                               NaN   NaN          NaN   \n",
       "545                                               NaN   NaN          NaN   \n",
       "546                                               NaN   NaN          NaN   \n",
       "547                                               NaN   NaN          NaN   \n",
       "\n",
       "                                             defendant  Count  Selectivity  \\\n",
       "0             Minister for Immigration and Citizenship  160.0        0.269   \n",
       "1    Minister for Immigration and Multicultural Aff...   51.0        0.086   \n",
       "2    Minister for Immigration and Multicultural and...   26.0        0.044   \n",
       "3                             Commissioner of Taxation   12.0        0.020   \n",
       "4                              Repatriation Commission    7.0        0.012   \n",
       "..                                                 ...    ...          ...   \n",
       "543                                                NaN    NaN          NaN   \n",
       "544                                                NaN    NaN          NaN   \n",
       "545                                                NaN    NaN          NaN   \n",
       "546                                                NaN    NaN          NaN   \n",
       "547                                                NaN    NaN          NaN   \n",
       "\n",
       "    hearing_year  ...  Selectivity  defendant_current_status  Count  \\\n",
       "0            NaT  ...        0.370                Government  306.0   \n",
       "1     2006-11-06  ...        0.096                   Company  137.0   \n",
       "2     2006-03-09  ...        0.057              Organization   55.0   \n",
       "3     2008-05-19  ...        0.045                  Employee    7.0   \n",
       "4     2008-05-20  ...        0.039   Chief Executive Officer    2.0   \n",
       "..           ...  ...          ...                       ...    ...   \n",
       "543          NaN  ...          NaN                       NaN    NaN   \n",
       "544          NaN  ...          NaN                       NaN    NaN   \n",
       "545          NaN  ...          NaN                       NaN    NaN   \n",
       "546          NaN  ...          NaN                       NaN    NaN   \n",
       "547          NaN  ...          NaN                       NaN    NaN   \n",
       "\n",
       "     Selectivity                  evidence  Count  Selectivity  first_judge  \\\n",
       "0          0.567                         1  467.0        0.778            0   \n",
       "1          0.254                         0  132.0        0.220            1   \n",
       "2          0.102  Not explicitly mentioned    1.0        0.002       (null)   \n",
       "3          0.013                    (null)    0.0        0.000          NaN   \n",
       "4          0.004                       NaN    NaN          NaN          NaN   \n",
       "..           ...                       ...    ...          ...          ...   \n",
       "543          NaN                       NaN    NaN          NaN          NaN   \n",
       "544          NaN                       NaN    NaN          NaN          NaN   \n",
       "545          NaN                       NaN    NaN          NaN          NaN   \n",
       "546          NaN                       NaN    NaN          NaN          NaN   \n",
       "547          NaN                       NaN    NaN          NaN          NaN   \n",
       "\n",
       "     Count  Selectivity  \n",
       "0    319.0        0.532  \n",
       "1    281.0        0.468  \n",
       "2      0.0        0.000  \n",
       "3      NaN          NaN  \n",
       "4      NaN          NaN  \n",
       "..     ...          ...  \n",
       "543    NaN          NaN  \n",
       "544    NaN          NaN  \n",
       "545    NaN          NaN  \n",
       "546    NaN          NaN  \n",
       "547    NaN          NaN  \n",
       "\n",
       "[548 rows x 57 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics = pd.DataFrame()\n",
    "\n",
    "for column in attr_desc_dict.keys():\n",
    "    if column in multi_value_attributes:\n",
    "        expanded_values = []\n",
    "        original_row_count = {} \n",
    "        \n",
    "        for idx, value in enumerate(df[column]):\n",
    "            if pd.isna(value):\n",
    "                expanded_values.append(None)\n",
    "                original_row_count[None] = original_row_count.get(None, 0) + 1\n",
    "            elif isinstance(value, str) and '||' in value:\n",
    "                split_values = [v.strip() for v in value.split('||') if v.strip()]\n",
    "                expanded_values.extend(split_values)\n",
    "                for split_val in split_values:\n",
    "                    original_row_count[split_val] = original_row_count.get(split_val, 0) + 1\n",
    "            else:\n",
    "                expanded_values.append(value)\n",
    "                original_row_count[value] = original_row_count.get(value, 0) + 1\n",
    "        \n",
    "        expanded_series = pd.Series(expanded_values)\n",
    "        value_counts = expanded_series.value_counts()\n",
    "        \n",
    "        selectivities = pd.Series({\n",
    "            k: round(v / len(df), 3) for k, v in original_row_count.items()\n",
    "        })\n",
    "        \n",
    "    else:\n",
    "        value_counts = df[column].value_counts()\n",
    "        selectivities = df[column].value_counts(normalize=True).round(3)\n",
    "    \n",
    "    null_count = df[column].isnull().sum()\n",
    "    \n",
    "    result_df = pd.DataFrame({\n",
    "        f\"{column}\": list(value_counts.index) + [\"(null)\"],\n",
    "        'Count': list(value_counts.values) + [null_count],\n",
    "        'Selectivity': [selectivities.get(val, 0) for val in value_counts.index] + [round(null_count / len(df), 3)]\n",
    "    })\n",
    "    \n",
    "    if statistics.empty:\n",
    "        statistics = result_df\n",
    "    else:\n",
    "        statistics = pd.concat([statistics, result_df], axis=1)\n",
    "\n",
    "statistics.to_csv(statistics_output_dir, index=False)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 汇总可取属性值，以便后续据此构造 Filter\n",
    "- ```attr_value_dict```：所有可取属性值的集合，每个属性有多个可取属性值，根据统计信息来定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_value_dict = {\n",
    "     \"judge_name\": [\"Tracey\", \"Flick\", \"Greenwood\", \"Collier\", \"Spender\", \"Besanko\", \"Moore\", \"Marshall\", \"Heerey\", \"Rares\", \"Bennett\", \"Jagot\", \"Cowdroy\", \"Conti\", \"Middleton\", \"Siopis\", \"Kenny\", \"Gyles\", \"Weinberg\", \"Graham\", \"French\", \"Gilmour\", \"Lander\", \"Gray\", \"Logan\", \"Gordon\", \"Finkelstein\", \"Mckerracher\", \"Edmonds\", \"Tamberlin\", \"Lindgren\", \"Buchanan\"],\n",
    "     \"plaintiff\": [\"Australian Competition and Consumer Commission\", \"Australian Securities and Investments Commission\", \"Cadbury Schweppes Pty Ltd\", \"Commissioner of Taxation\"],\n",
    "     \"defendant\": [\"Minister for Immigration and Citizenship\", \"Minister for Immigration and Multicultural Affairs\", \"Minister for Immigration and Multicultural and Indigenous Affairs\", ],\n",
    "     \"charges\": [],\n",
    "     \"case_type\": [\"Administrative Case\", \"Civil Case\", \"Commercial Case\", \"Criminal Case\"],\n",
    "     \"verdict\": [\"Dismissed\", \"Approved\", \"Guilty\", \"Others\"],\n",
    "     \"counsel_for_applicant\": [],\n",
    "     \"counsel_for_respondent\": [],\n",
    "     \"nationality_for_applicant\": [\"Australia\", \"China\", \"India\", \"Fiji\", \"Pakistan\"],\n",
    "     \"plaintiff_current_status\": [\"Company\", \"Organization\", \"Government\"],\n",
    "     \"defendant_current_status\": [\"Government\", \"Company\", \"Organization\"],\n",
    "     \"evidence\": [\"1\", \"0\"],\n",
    "     \"first_judge\": [\"1\", \"0\"],\n",
    "     \"hearing_year\": [\"2005\", \"2006\", \"2007\", \"2009\", \"2005-3\", \"2005-4\", \"2005-7\", \"2005-10\", \"2005-11\", \"2006-1\", \"2006-4\", \"2006-5\", \"2006-6\", \"2006-8\",\"2006-10\", \"2006-12\", \"2007-2\", \"2007-3\", \"2007-4\", \"2007-5\", \"2007-8\", \"2007-9\", \"2007-10\", \"2007-11\", \"2007-12\", \"2008-1\", \"2008-5\", \"2008-7\", \"2008-9\", \"2008-12\", \"2009-4\", \"2009-5\", \"2009-10\", \"2009-11\", \"2008-5-20\", \"2007-5-2\", \"2006-11-13\", \"2006-3-9\"],\n",
    "     \"judgment_year\": [\"2005\", \"2006\", \"2008\", \"2009\", \"2005-2\", \"2005-3\", \"2005-4\", \"2005-8\", \"2005-9\", \"2005-10\", \"2005-11\", \"2005-12\", \"2006-1\", \"2006-2\", \"2006-3\", \"2006-4\", \"2006-5\", \"2006-6\", \"2006-7\", \"2006-8\", \"2006-9\", \"2006-10\", \"2006-11\", \"2006-12\", \"2007-1\", \"2007-2\", \"2007-3\", \"2007-7\", \"2007-8\", \"2007-10\", \"2007-12\", \"2008-3\", \"2008-8\", \"2008-12\", \"2009-1\", \"2009-5\", \"2009-6\", \"2009-9\"],\n",
    "     \"legal_fees\": [200000, 100000, 10000, 5000, 2000, 1200, 1000, 700, 300, 0],\n",
    "     \"legal_basis_num\": [20, 10, 6, 3, 1, 0],\n",
    "     \"case_num\": [50, 20, 10, 5, 2, 0],\n",
    "     \"fine_amount\": [1000000, 100000, 50000, 10000, 5000, 1000, 100, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义查询构造数量\n",
    "- ```max_filters```：WHERE 语句最大 Filter 数量\n",
    "- ```min_rows```：结果表最少行数\n",
    "- ```max_select```：SELECT 语句最大属性数量\n",
    "- ```limit_list```：LIMIT 语句候选值\n",
    "- ```sample_sfw```：SELECT | FROM | WHERE 查询数量\n",
    "- ```sample_sfwt```：SELECT | FROM | WHERE | TOP-K 查询数量\n",
    "- ```sample_sfwg```：SELECT | FROM | WHERE | GROUP BY 查询数量\n",
    "- ```sample_sfwa```：SELECT | FROM | WHERE | AGGREGATION 查询数量\n",
    "- ```sample_sfwga```：SELECT | FROM | WHERE | GROUP BY | AGGREGATION 查询数量\n",
    "- ```sample_sfwgat```：SELECT | FROM | WHERE | GROUP BY | AGGREGATION | TOP-K 查询数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_filters = 5\n",
    "min_rows = 5\n",
    "max_select = 3\n",
    "limit_list = [1, 5, 10, 20, 50]\n",
    "sample_sf = 10\n",
    "sample_sfw = 10\n",
    "sample_sfwt = 20\n",
    "sample_sfwg = 20\n",
    "sample_sfwa = 20\n",
    "sample_sfwga = 30\n",
    "sample_sfwgat = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义 Filter 具体执行方法\n",
    "- 不同类型的数据有不同的执行方法\n",
    "- 区分非数值型、数值型、日期型\n",
    "- 比较运算包括：大于（只限于数值型）、小于（只限于数值型）、等于（任意类型）\n",
    "- **注意**：需要提前确认好日期型数据的具体格式，做好特殊情况相应处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_numerical_equal_to(value, condition):\n",
    "     #print(f'value:{value}, condition:{condition}')\n",
    "     try:\n",
    "          value = str(value).lower().strip()\n",
    "          condition = str(condition).lower().strip()\n",
    "     except:\n",
    "          print(\"Invalid data (non_numerical_equal_to). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "     return value == condition\n",
    "\n",
    "def number_greater_than(value, condition):\n",
    "     if pd.isna(value):\n",
    "          value = 0.00\n",
    "     try:\n",
    "          value = round(float(value), 2)\n",
    "          condition = round(float(condition), 2)\n",
    "     except:\n",
    "          print(\"Invalid data (number_greater_than). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "     return value > condition\n",
    "\n",
    "def number_less_than(value, condition):\n",
    "     if pd.isna(value):\n",
    "          value = 0.00\n",
    "     try:\n",
    "          value = round(float(value), 2)\n",
    "          condition = round(float(condition), 2)\n",
    "     except:\n",
    "          print(\"Invalid data (number_less_than). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "     return value < condition\n",
    "\n",
    "def number_equal_to(value, condition):\n",
    "     if pd.isna(value):\n",
    "          value = 0.00\n",
    "     try:\n",
    "          value = round(float(value), 2)\n",
    "          condition = round(float(condition), 2)\n",
    "     except:\n",
    "          print(\"Invalid data (number_equal_to). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "     return value == condition\n",
    "\n",
    "def parse_date(date_str):\n",
    "     if pd.isna(date_str):\n",
    "          return None\n",
    "     for fmt in ['%Y/%m/%d', '%Y/%m', '%Y','%Y-%m-%d', '%Y-%m']:\n",
    "          try:\n",
    "               return datetime.strptime(date_str, fmt)\n",
    "          except ValueError:\n",
    "               continue\n",
    "     return None\n",
    "\n",
    "def date_greater_than(value, condition):\n",
    "     date_value = parse_date(value)\n",
    "     condition_date = parse_date(condition)\n",
    "     if date_value is None or condition_date is None:\n",
    "          return False\n",
    "     return date_value > condition_date\n",
    "\n",
    "def date_less_than(value, condition):\n",
    "     date_value = parse_date(value)\n",
    "     condition_date = parse_date(condition)\n",
    "     if date_value is None or condition_date is None:\n",
    "          return False\n",
    "     return date_value < condition_date\n",
    "\n",
    "def date_equal_to(value, condition):\n",
    "     date_value = parse_date(value)\n",
    "     condition_date = parse_date(condition)\n",
    "     if date_value is None or condition_date is None:\n",
    "          return False\n",
    "     return date_value == condition_date\n",
    "\n",
    "def parse_century(value):\n",
    "    \"\"\"将世纪字符串解析为 (起始年份, 结束年份) 的元组。\n",
    "\n",
    "    优雅地处理 NaN 值和不正确的格式。\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        match = re.match(r\"(\\d+)(?:th|st|nd|rd)(?:-(\\d+)(?:th|st|nd|rd))?\", value)\n",
    "        if not match:\n",
    "            print(f\"无效的世纪格式: {value}\")  # 更具体的错误信息\n",
    "            return None\n",
    "\n",
    "        start_century = int(match.group(1))\n",
    "        end_century = int(match.group(2)) if match.group(2) else start_century\n",
    "        return (start_century - 1) * 100, end_century * 100\n",
    "\n",
    "    except ValueError:\n",
    "        print(f\"无效的世纪格式 (ValueError): {value}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def century_greater_than(value, condition):\n",
    "    \"\"\"检查一个世纪值是否大于另一个世纪值。\n",
    "\n",
    "    优雅地处理 NaN 值和不正确的格式。\n",
    "    \"\"\"\n",
    "    value_parsed = parse_century(value)\n",
    "    condition_parsed = parse_century(condition)\n",
    "\n",
    "    if value_parsed is None or condition_parsed is None:\n",
    "        return False\n",
    "\n",
    "    return value_parsed[0] > condition_parsed[1]\n",
    "\n",
    "def century_less_than(value, condition):\n",
    "    \"\"\"检查一个世纪值是否大于另一个世纪值。\n",
    "\n",
    "    优雅地处理 NaN 值和不正确的格式。\n",
    "    \"\"\"\n",
    "    value_parsed = parse_century(value)\n",
    "    condition_parsed = parse_century(condition)\n",
    "\n",
    "    if value_parsed is None or condition_parsed is None:\n",
    "        return False\n",
    "\n",
    "    return value_parsed[0] < condition_parsed[1]\n",
    "\n",
    "def number_greater_equal(value, condition):\n",
    "    if pd.isna(value):\n",
    "        value = 0.00\n",
    "    try:\n",
    "        value = round(float(value), 2)\n",
    "        condition = round(float(condition), 2)\n",
    "    except:\n",
    "        print(\"Invalid data (number_greater_equal). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "    return value >= condition\n",
    "\n",
    "def number_less_equal(value, condition):\n",
    "    if pd.isna(value):\n",
    "        value = 0.00\n",
    "    try:\n",
    "        value = round(float(value), 2)\n",
    "        condition = round(float(condition), 2)\n",
    "    except:\n",
    "        print(\"Invalid data (number_less_equal). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "    return value <= condition\n",
    "\n",
    "def date_greater_equal(value, condition):\n",
    "    date_value = parse_date(value)\n",
    "    condition_date = parse_date(condition)\n",
    "    if date_value is None or condition_date is None:\n",
    "        return False\n",
    "    return date_value >= condition_date\n",
    "\n",
    "def date_less_equal(value, condition):\n",
    "    date_value = parse_date(value)\n",
    "    condition_date = parse_date(condition)\n",
    "    if date_value is None or condition_date is None:\n",
    "        return False\n",
    "    return date_value <= condition_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有可能的 Filter\n",
    "- Filter 的可能取值来自于字典：```attr_value_dict```\n",
    "- 用一个列表记录满足 Filter 的行索引\n",
    "- 输入：```dataset_dir```，即原始标注数据表\n",
    "- 输出：```filter_dict.json```，包含各属性，每个属性中包含所有可能的过滤条件及对应满足条件的行索引号\n",
    "- 这一步构建好所有可能的 Filter，后面从此 Filter 集合中选取 Filter 进行排列组合，构造 WHERE 语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0b80da8ce34ee9809cc816091fddd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_excel(dataset_dir)\n",
    "\n",
    "filter_dict = {}\n",
    "\n",
    "for key in attr_value_dict.keys():\n",
    "    filter_dict[key] = {}\n",
    "\n",
    "for key, value in tqdm(filter_dict.items()):\n",
    "    condition_dict = {}\n",
    "\n",
    "    ###### 非数值属性，只取等于操作 ######\n",
    "    if key in non_numerical_attr:\n",
    "        for possible_value in attr_value_dict[key]:\n",
    "            result = df[key].apply(non_numerical_equal_to, condition=possible_value)\n",
    "            result_indices = df[result].index.tolist()\n",
    "            condition_dict[\"=='\" + str(possible_value) + \"'\"] = result_indices\n",
    "\n",
    "    ###### 数值属性，取5种操作：<, >, =, <=, >= ######\n",
    "    if key in numerical_attr:\n",
    "        for possible_value in attr_value_dict[key]:\n",
    "            \n",
    "            # 初始化所有结果索引列表\n",
    "            result_indices_less = []\n",
    "            result_indices_greater = []\n",
    "            result_indices_equal = []\n",
    "            result_indices_less_equal = []\n",
    "            result_indices_greater_equal = []\n",
    "\n",
    "            ###### 非格式化数值属性（普通数值）######\n",
    "            if key in non_formatted_attr:\n",
    "                result_index_less = df[key].apply(number_less_than, condition=possible_value)\n",
    "                result_index_greater = df[key].apply(number_greater_than, condition=possible_value)\n",
    "                result_index_equal = df[key].apply(number_equal_to, condition=possible_value)\n",
    "                result_index_less_equal = df[key].apply(number_less_equal, condition=possible_value)\n",
    "                result_index_greater_equal = df[key].apply(number_greater_equal, condition=possible_value)\n",
    "                \n",
    "                result_indices_less = df[result_index_less].index.tolist()\n",
    "                result_indices_greater = df[result_index_greater].index.tolist()\n",
    "                result_indices_equal = df[result_index_equal].index.tolist()\n",
    "                result_indices_less_equal = df[result_index_less_equal].index.tolist()\n",
    "                result_indices_greater_equal = df[result_index_greater_equal].index.tolist()\n",
    "\n",
    "            ###### 格式化数值属性（日期）######\n",
    "            elif key in formatted_attr:\n",
    "                result_index_less = df[key].apply(date_less_than, condition=possible_value)\n",
    "                result_index_greater = df[key].apply(date_greater_than, condition=possible_value)\n",
    "                result_index_equal = df[key].apply(date_equal_to, condition=possible_value)\n",
    "                result_index_less_equal = df[key].apply(date_less_equal, condition=possible_value)\n",
    "                result_index_greater_equal = df[key].apply(date_greater_equal, condition=possible_value)\n",
    "                \n",
    "                result_indices_less = df[result_index_less].index.tolist()\n",
    "                result_indices_greater = df[result_index_greater].index.tolist()\n",
    "                result_indices_equal = df[result_index_equal].index.tolist()\n",
    "                result_indices_less_equal = df[result_index_less_equal].index.tolist()\n",
    "                result_indices_greater_equal = df[result_index_greater_equal].index.tolist()\n",
    "\n",
    "            # 将所有5种操作添加到条件字典中\n",
    "            condition_dict[\"<\" + str(possible_value)] = result_indices_less\n",
    "            condition_dict[\">\" + str(possible_value)] = result_indices_greater\n",
    "            condition_dict[\"==\" + str(possible_value)] = result_indices_equal\n",
    "            condition_dict[\"<=\" + str(possible_value)] = result_indices_less_equal\n",
    "            condition_dict[\">=\" + str(possible_value)] = result_indices_greater_equal\n",
    "    \n",
    "    filter_dict[key] = condition_dict\n",
    "\n",
    "with open(\"./filter_dict.json\", 'w') as f:\n",
    "    json.dump(filter_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有 Filter 的可能排列组合，构造 WHERE 语句\n",
    "- ```balanced_sample()```：控制均匀采样 Filter\n",
    "     - 保证每个属性出现的次数是平衡的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 均匀采样函数 ######\n",
    "def balanced_sample(filters, sample_size=10, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "    col_to_filters = defaultdict(list)\n",
    "    for filter_item in filters:\n",
    "        col = filter_item[0]\n",
    "        col_to_filters[col].append(filter_item)\n",
    "    \n",
    "    unique_cols = list(col_to_filters.keys())\n",
    "    num_cols = len(unique_cols)\n",
    "    \n",
    "    if num_cols == 0:\n",
    "        return []\n",
    "    \n",
    "    base_num = sample_size // num_cols\n",
    "    remainder = sample_size % num_cols\n",
    "    \n",
    "    col_sample_counts = {col: base_num for col in unique_cols}\n",
    "\n",
    "    for col in random.sample(unique_cols, remainder):\n",
    "        col_sample_counts[col] += 1\n",
    "    \n",
    "    sampled_filters = []\n",
    "    overflow = 0\n",
    "    \n",
    "    for col in unique_cols:\n",
    "        available = len(col_to_filters[col])\n",
    "        required = col_sample_counts[col]\n",
    "        if available >= required:\n",
    "            sampled = random.sample(col_to_filters[col], required)\n",
    "            sampled_filters.extend(sampled)\n",
    "        else:\n",
    "            sampled = col_to_filters[col]\n",
    "            sampled_filters.extend(sampled)\n",
    "            overflow += (required - available)\n",
    "    \n",
    "    if overflow > 0:\n",
    "        remaining_cols = [col for col in unique_cols if len(col_to_filters[col]) > col_sample_counts[col]]\n",
    "        while overflow > 0 and remaining_cols:\n",
    "            for col in remaining_cols.copy():\n",
    "                available = len(col_to_filters[col])\n",
    "                current = col_sample_counts[col]\n",
    "                if available > current:\n",
    "                    sampled = random.sample(\n",
    "                        list(set(col_to_filters[col]) - set(sampled_filters)), 1\n",
    "                    )\n",
    "                    sampled_filters.extend(sampled)\n",
    "                    col_sample_counts[col] += 1\n",
    "                    overflow -= 1\n",
    "                    if overflow == 0:\n",
    "                        break\n",
    "                else:\n",
    "                    remaining_cols.remove(col)\n",
    "\n",
    "    return sampled_filters[:sample_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有 Filter 的可能排列组合，构造 WHERE 语句\n",
    "- 枚举所有可能的 Filter 排列组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51e590e0d514c898a3d6b91d36ac7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"./filter_dict.json\", 'r') as f:\n",
    "     filter_dict = json.load(f)\n",
    "\n",
    "filters = []\n",
    "for col, conditions in filter_dict.items():\n",
    "     for cond, indices in conditions.items():\n",
    "          filters.append((col, cond, set(indices)))\n",
    "\n",
    "# filters = random.sample(filters, 20)\n",
    "filters = balanced_sample(filters, sample_size=20, random_seed=42)\n",
    "\n",
    "all_combinations = []\n",
    "for n in tqdm(range(1, max_filters + 1)):\n",
    "     all_combinations.extend(itertools.permutations(filters, n))\n",
    "\n",
    "with open(\"./all_combinations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_combinations, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有 Filter 的可能排列组合，构造 WHERE 语句\n",
    "- ```select_combinations_with_ratio()```：按不同 Filter 数量的比例构造 WHERE 语句\n",
    "- 1 - 5 个 Filter 的比例为：2 : 3 : 3 : 1 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_combinations_with_ratio(all_combinations, total=1000, ratio=[2, 3, 3, 1, 1], min_len=1, max_len=5):\n",
    "\n",
    "    length_to_combinations = defaultdict(list)\n",
    "    for combo in all_combinations:\n",
    "        l = len(combo)\n",
    "        if min_len <= l <= max_len:\n",
    "            length_to_combinations[l].append(combo)\n",
    "\n",
    "    total_ratio = sum(ratio)\n",
    "    \n",
    "    samples_per_length = {}\n",
    "    remaining_total = total\n",
    "    \n",
    "    for i, l in enumerate(range(min_len, max_len + 1)):\n",
    "        expected_samples = int(total * ratio[i] / total_ratio)\n",
    "        available_samples = len(length_to_combinations[l])\n",
    "        if available_samples < expected_samples:\n",
    "            samples_per_length[l] = available_samples\n",
    "            remaining_total -= available_samples\n",
    "            ratio[i] = 0\n",
    "        else:\n",
    "            samples_per_length[l] = expected_samples\n",
    "            remaining_total -= expected_samples\n",
    "    \n",
    "    if remaining_total > 0:\n",
    "        for i, l in enumerate(range(min_len, max_len + 1)):\n",
    "            if ratio[i] > 0:\n",
    "                possible_to_add = len(length_to_combinations[l]) - samples_per_length[l]\n",
    "                if possible_to_add > 0:\n",
    "                    additional_samples = min(possible_to_add, remaining_total)\n",
    "                    samples_per_length[l] += additional_samples\n",
    "                    remaining_total -= additional_samples\n",
    "                if remaining_total == 0:\n",
    "                    break\n",
    "\n",
    "    if remaining_total != 0:\n",
    "        raise ValueError(f\"The number of combinations cannot meet the requirements, with {remaining_total} combinations remaining.\")\n",
    "    \n",
    "    selected = []\n",
    "    for l in range(min_len, max_len + 1):\n",
    "        available = length_to_combinations[l]\n",
    "        required = samples_per_length[l]\n",
    "        selected += random.sample(available, required)\n",
    "    \n",
    "    random.shuffle(selected)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有 Filter 的可能排列组合，构造 WHERE 语句\n",
    "- 析取 + 合取 + 析取与合取混合\n",
    "- 去掉等效表达式\n",
    "- 每个查询的结果表至少包含 ```min_rows``` 行 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2703de81b2de475daaeaf50cda5ed511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"./all_combinations.pkl\", \"rb\") as f:\n",
    "     all_combinations = pickle.load(f)\n",
    "\n",
    "def normalize_expression(expression):\n",
    "\n",
    "    def sort_expression(expr):\n",
    "        if \" AND \" in expr:\n",
    "            parts = sorted(expr.strip(\"()\").split(\" AND \"))\n",
    "            return f\"({' AND '.join(parts)})\"\n",
    "        elif \" OR \" in expr:\n",
    "            parts = sorted(expr.strip(\"()\").split(\" OR \"))\n",
    "            return f\"({' OR '.join(parts)})\"\n",
    "        return expr\n",
    "    \n",
    "    if \" AND \" in expression or \" OR \" in expression:\n",
    "        return sort_expression(expression)\n",
    "    return expression\n",
    "\n",
    "valid_where = []\n",
    "seen_expressions = set()\n",
    "\n",
    "# sampled_combinations = random.sample(all_combinations, 1000)\n",
    "sampled_combinations = select_combinations_with_ratio(all_combinations, total=1000, ratio=[2, 3, 3, 1, 1], min_len=1, max_len=max_filters)\n",
    "\n",
    "for combo in tqdm(sampled_combinations):\n",
    "    condition_sets = [set(item[2]) for item in combo]\n",
    "    predicates = [f\"{col}{cond}\" for col, cond, _ in combo]\n",
    "\n",
    "    for op in itertools.product([\"and\", \"or\"], repeat=len(condition_sets) - 1):\n",
    "        # 使用更直接的逻辑计算\n",
    "        result_set = condition_sets[0].copy()  # 从第一个集合开始\n",
    "        result_expr = predicates[0]\n",
    "        \n",
    "        # 依次应用每个操作符\n",
    "        for i, logic in enumerate(op):\n",
    "            next_set = condition_sets[i + 1]\n",
    "            next_pred = predicates[i + 1]\n",
    "            \n",
    "            if logic == \"and\":\n",
    "                result_set = result_set & next_set\n",
    "                result_expr = f\"({result_expr} AND {next_pred})\"\n",
    "            elif logic == \"or\":\n",
    "                result_set = result_set | next_set\n",
    "                result_expr = f\"({result_expr} OR {next_pred})\"\n",
    "        \n",
    "        # 检查结果\n",
    "        if len(result_set) >= min_rows:\n",
    "            normalized_expression = normalize_expression(result_expr)\n",
    "            if normalized_expression not in seen_expressions:\n",
    "                seen_expressions.add(normalized_expression)\n",
    "                \n",
    "                combo_list = [[c[0], c[1]] for c in combo]\n",
    "                query_dict = {\n",
    "                    \"WHERE Indices\": list(result_set),\n",
    "                    \"WHERE Total Rows\": len(result_set),\n",
    "                    \"Combination\": combo_list,\n",
    "                    \"Operators\": list(op),\n",
    "                    \"WHERE\": result_expr\n",
    "                }\n",
    "                valid_where.append(query_dict)\n",
    "                # if len(valid_where) > 1000:\n",
    "                #         with open(\"/data/sunzhaoze/benchmark/valid_WHERE.json\", 'a') as f:\n",
    "                #             json.dump(valid_where, f, ensure_ascii=False, indent=4, separators=(',', ': '))\n",
    "                #         valid_where.clear()\n",
    "\n",
    "    if valid_where:\n",
    "        with open(valid_where_output_dir, 'w') as f:\n",
    "            json.dump(valid_where, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义表头 SCHEMA\n",
    "- ```create_schema()```：SCHEMA 定义函数，数据共三种类型：VARCHAR(255) | DATE | FLOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema(attr_desc_dict, query_attr_list):\n",
    "     schema = {}\n",
    "     difference = set(attr_desc_dict.keys()) - set(query_attr_list)\n",
    "     redundant_attr_list = random.sample(list(difference), random.randint(0, 4))\n",
    "     schema_attr_list = query_attr_list + redundant_attr_list\n",
    "     for key in schema_attr_list:\n",
    "          if key in non_numerical_attr:\n",
    "               schema[key] = [\"VARCHAR(255)\", attr_desc_dict[key]]\n",
    "          elif key in formatted_attr:\n",
    "               schema[key] = [\"DATE\", attr_desc_dict[key]]\n",
    "          elif key in non_formatted_attr:\n",
    "               schema[key] = [\"FLOAT\", attr_desc_dict[key]]\n",
    "     return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json 自定义格式化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_json_dump(data, filename):\n",
    "    json_str = json.dumps(data, ensure_ascii=False)\n",
    "    json_str = json_str.replace(\", {\", \",\\n{\").replace(\"{\\\"\", \"{\\n\\t\\\"\").replace(\"]},\\n\", \"]\\n},\\n\").replace(\"e},\\n\", \"e\\n},\\n\").replace(\"}},\\n\", \"}\\n},\\n\")\n",
    "    json_str = re.sub(r',\\s*\"([^\"]+)\":', r', \\n\\t\"\\1\":', json_str)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db73e358df384532853efb6b4956f2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_sf_queries():\n",
    "    valid_sf = []\n",
    "    \n",
    "    for i in tqdm(range(sample_sf)):\n",
    "        # 随机选择1到max_select个列\n",
    "        selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_select))\n",
    "        \n",
    "        # 创建查询字典\n",
    "        query_dict = {\n",
    "            \"Type\": \"SF\",\n",
    "            \"SELECT\": selected_columns,\n",
    "            \"WHERE Indices\": list(range(len(df))),  # 包含所有行\n",
    "            \"WHERE Total Rows\": len(df),            # 总行数\n",
    "            \"Combination\": [],                      # 没有Filter组合\n",
    "            \"Operators\": [],                        # 没有操作符\n",
    "            \"WHERE\": \"None\"                         # 没有WHERE条件\n",
    "        }\n",
    "        \n",
    "        # 创建SCHEMA\n",
    "        query_attr_list = selected_columns.copy()\n",
    "        query_dict[\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "        \n",
    "        valid_sf.append(query_dict)\n",
    "    \n",
    "    return valid_sf\n",
    "valid_sf = create_sf_queries()\n",
    "custom_json_dump(valid_sf, \"./SELECT_FROM.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742d4096a85e4a1d917b7e4a4b33577a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_sfw = json.load(f)\n",
    "\n",
    "for i in tqdm(range(0, len(valid_sfw))):\n",
    "    selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_select))\n",
    "    valid_sfw[i][\"Type\"] = \"SFW\"\n",
    "    valid_sfw[i][\"SELECT\"] = selected_columns\n",
    "    query_attr_list = [i for i in selected_columns]\n",
    "    for k in valid_sfw[i][\"Combination\"]:\n",
    "         if k[0] not in query_attr_list:\n",
    "             query_attr_list += [k[0]]\n",
    "    valid_sfw[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "\n",
    "sampled_valid_sfw = random.sample(valid_sfw, sample_sfw)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE.json\", 'w') as f:\n",
    "#      json.dump(sampled_valid_sfw, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfw, \"./SELECT_FROM_WHERE.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | GROUP BY\n",
    "- 返回多个表，每个组一个表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8240fc03964423b99f5298695bff724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_where = json.load(f)\n",
    "\n",
    "valid_sfwg = []\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "     # while True:\n",
    "     #      selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_groupby))\n",
    "     #      if not (set(selected_columns) & set(formatted_attr)):\n",
    "     #           break\n",
    "     selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_select))\n",
    "     valid_groupby_attrs = [attr for attr in category_attr if attr not in multi_value_attributes]\n",
    "     groupby_columns = random.sample(valid_groupby_attrs, 1)\n",
    "     row_indices = valid_where[i].get(\"WHERE Indices\", [])\n",
    "     filtered_df = df.loc[row_indices]\n",
    "     grouped = filtered_df.groupby(groupby_columns)\n",
    "     remaining_rows = grouped.size().reset_index()\n",
    "     if len(remaining_rows) > (min_rows // 2):\n",
    "          valid_where[i][\"Type\"] = \"SFWG\"\n",
    "          valid_where[i][\"SELECT\"] = selected_columns + groupby_columns if groupby_columns not in selected_columns else selected_columns\n",
    "          valid_where[i][\"GROUP BY Total Rows (Groups)\"] = len(remaining_rows)\n",
    "          valid_where[i][\"GROUP BY\"] = groupby_columns\n",
    "          query_attr_list = [i for i in groupby_columns]\n",
    "          for k in valid_where[i][\"Combination\"]:\n",
    "               if k[0] not in query_attr_list:\n",
    "                    query_attr_list += [k[0]]\n",
    "          valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "          valid_sfwg.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwg = random.sample(valid_sfwg, sample_sfwg)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_GROUPBY.json\", 'w') as f:\n",
    "#      json.dump(sampled_valid_sfwg, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwg, \"./SELECT_FROM_WHERE_GROUPBY.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | AGGREGATION\n",
    "- 支持的聚合函数：SUM | MAX | MIN | AVG | COUNT\n",
    "- TODO：加入语义聚合\n",
    "- 只在一个属性上进行聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ccbc9b7d3349caa54fbf1ca6e777f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_where = json.load(f)\n",
    "\n",
    "valid_sfwa = []\n",
    "aggregation_functions = ['COUNT', 'MAX', 'MIN', 'AVG', 'SUM']\n",
    "max_COUNT = len(valid_where) // 4\n",
    "\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "     numerical = False\n",
    "     while True:\n",
    "          if max_COUNT > 0:\n",
    "               selected_aggregation_column = random.sample(list(attr_value_dict.keys()), 1)\n",
    "               if selected_aggregation_column not in formatted_attr:\n",
    "                    break\n",
    "          else:\n",
    "               selected_aggregation_column = random.sample(list(non_formatted_attr), 1)\n",
    "               break\n",
    "          \n",
    "     if selected_aggregation_column[0] in non_formatted_attr:\n",
    "          function = random.choice(aggregation_functions[1:])\n",
    "          numerical = True\n",
    "     else:\n",
    "          max_COUNT -= 1\n",
    "          function = 'COUNT'\n",
    "     if function == \"COUNT\" and random.uniform(0, 1) > 0.5:\n",
    "          selected_aggregation_column = [\"*\"]\n",
    "     valid_where[i][\"Type\"] = \"SFWA\"\n",
    "     valid_where[i][\"SELECT\"] = [f\"{function}({selected_aggregation_column[0]})\"]\n",
    "     query_attr_list = [i for i in selected_aggregation_column]\n",
    "     for k in valid_where[i][\"Combination\"]:\n",
    "          if k[0] not in query_attr_list:\n",
    "               query_attr_list += [k[0]]\n",
    "     valid_where[i][\"AGGREGATION\"] = selected_aggregation_column\n",
    "     valid_where[i][\"AGGREGATION Function\"] = function\n",
    "     valid_where[i][\"Numerical\"] = numerical\n",
    "     valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "     valid_sfwa.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwa = random.sample(valid_sfwa, sample_sfwa)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_AGGREGATION.json\", 'w') as f:\n",
    "#      json.dump(sampled_valid_sfwa, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwa, \"./SELECT_FROM_WHERE_AGGREGATION.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | GROUP BY | AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b19277e61240699114309a69f5cacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_sfag = []\n",
    "aggregation_functions = ['COUNT', 'MAX', 'MIN', 'MEAN', 'SUM']\n",
    "sample_sfag = 20  # 定义生成数量\n",
    "max_COUNT = sample_sfag // 4\n",
    "\n",
    "for i in tqdm(range(sample_sfag)):\n",
    "    # 选择分组列（排除多值属性）\n",
    "    valid_groupby_attrs = [attr for attr in category_attr if attr not in multi_value_attributes]\n",
    "    selected_groupby_columns = random.sample(valid_groupby_attrs, 1)\n",
    "    \n",
    "    # 选择聚合函数和列\n",
    "    if max_COUNT > 0:\n",
    "        selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "        if selected_aggregation_column[0] not in formatted_attr:\n",
    "            function = random.choice(aggregation_functions[1:])\n",
    "            numerical = True\n",
    "        else:\n",
    "            max_COUNT -= 1\n",
    "            function = 'COUNT'\n",
    "            numerical = False\n",
    "    else:\n",
    "        selected_aggregation_column = random.sample(list(set(non_formatted_attr)-set(selected_groupby_columns)), 1)\n",
    "        function = random.choice(aggregation_functions[1:])\n",
    "        numerical = True\n",
    "    \n",
    "    if function == \"COUNT\" and random.uniform(0, 1) > 0.5:\n",
    "        selected_aggregation_column = [\"*\"]\n",
    "    \n",
    "    query_dict = {\n",
    "        \"Type\": \"SFAG\",\n",
    "        \"GROUP BY\": selected_groupby_columns,\n",
    "        \"SELECT\": selected_groupby_columns + [f\"{function}({selected_aggregation_column[0]})\"],\n",
    "        \"WHERE Indices\": list(range(len(df))),  # 包含所有行\n",
    "        \"WHERE Total Rows\": len(df),            # 总行数\n",
    "        \"Combination\": [],                      # 没有Filter组合\n",
    "        \"Operators\": [],                        # 没有操作符\n",
    "        \"WHERE\": \"None\",                        # 没有WHERE条件\n",
    "        \"AGGREGATION\": selected_aggregation_column,\n",
    "        \"AGGREGATION Function\": \"AVG\" if function == \"MEAN\" else function,\n",
    "        \"Numerical\": numerical\n",
    "    }\n",
    "    \n",
    "    # 创建SCHEMA\n",
    "    query_attr_list = selected_aggregation_column + selected_groupby_columns\n",
    "    query_dict[\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "    \n",
    "    valid_sfag.append(query_dict)\n",
    "\n",
    "custom_json_dump(valid_sfag, \"./SELECT_FROM_AGGREGATION_GROUPBY.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | GROUP BY | AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_to_float(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(pd.to_numeric, errors='coerce')\n",
    "    return df\n",
    "\n",
    "for i in non_formatted_attr:\n",
    "    convert_column_to_float(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b650cf5deae4e21a1ed9e0b63c6e431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_where = json.load(f)\n",
    "\n",
    "valid_sfwga = []\n",
    "aggregation_functions = ['COUNT', 'MAX', 'MIN', 'MEAN', 'SUM']\n",
    "max_COUNT = sample_sfwga // 4\n",
    "\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "     row_indices = valid_where[i].get(\"WHERE Indices\", [])\n",
    "     filtered_df = df.loc[row_indices]\n",
    "     # while True:\n",
    "     #      selected_groupby_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_groupby))\n",
    "     #      if not set(selected_groupby_columns) & set(formatted_attr):\n",
    "     #           break\n",
    "     valid_groupby_attrs = [attr for attr in category_attr if attr not in multi_value_attributes]\n",
    "     selected_groupby_columns = random.sample(valid_groupby_attrs, 1)\n",
    "     grouped = filtered_df.groupby(selected_groupby_columns)\n",
    "     remaining_rows = grouped.size().reset_index(name='Group Size')\n",
    "\n",
    "     if len(remaining_rows) > (min_rows // 2):\n",
    "          while True:\n",
    "               if max_COUNT > 0:\n",
    "                    selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "                    if selected_aggregation_column not in formatted_attr:\n",
    "                         break\n",
    "               else:\n",
    "                    selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "                    break\n",
    "               \n",
    "          if selected_aggregation_column[0] in non_formatted_attr:\n",
    "               function = random.choice(aggregation_functions[1:])\n",
    "               numerical = True\n",
    "          else:\n",
    "               max_COUNT -= 1\n",
    "               function = 'COUNT'\n",
    "               numerical = False\n",
    "          \n",
    "          if function == \"COUNT\" and random.uniform(0, 1) > 0.5:\n",
    "               selected_aggregation_column = [\"*\"]\n",
    "          agg_result = grouped[selected_aggregation_column].agg(function.lower()) if numerical else grouped.size()\n",
    "          valid_where[i][\"Type\"] = \"SFWGA\"\n",
    "          valid_where[i][\"GROUP BY\"] = selected_groupby_columns\n",
    "          valid_where[i][\"GROUP BY Total Rows\"] = len(remaining_rows)\n",
    "          valid_where[i][\"SELECT\"] = selected_groupby_columns + [f\"{function}({selected_aggregation_column[0]})\"]\n",
    "\n",
    "          valid_where[i][\"AGGREGATION\"] = selected_aggregation_column\n",
    "          if function == \"MEAN\":\n",
    "               valid_where[i][\"AGGREGATION Function\"] = \"AVG\"\n",
    "          else:\n",
    "               valid_where[i][\"AGGREGATION Function\"] = function\n",
    "          valid_where[i][\"Numerical\"] = numerical\n",
    "          query_attr_list = [i for i in selected_aggregation_column]\n",
    "          for k in valid_where[i][\"Combination\"]:\n",
    "               if k[0] not in query_attr_list:\n",
    "                    query_attr_list += [k[0]]\n",
    "          for k in selected_groupby_columns:\n",
    "               if k not in query_attr_list:\n",
    "                    query_attr_list += [k]\n",
    "          valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "          valid_sfwga.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwga = random.sample(valid_sfwga, sample_sfwga)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION.json\", 'w') as f:\n",
    "#      json.dump(sampled_valid_sfwga, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwga, \"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | TOP-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5314e1ee0ab84f3898ad3e82f4f6ac7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "    valid_where = json.load(f)\n",
    "\n",
    "valid_sfwt = []\n",
    "order_options = ['ASC', 'DESC']\n",
    "\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "    row_indices = valid_where[i].get(\"WHERE Indices\", [])\n",
    "    filtered_df = df.loc[row_indices]\n",
    "    while True:\n",
    "        selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_select))\n",
    "        if set(selected_columns) & set(non_formatted_attr):\n",
    "            order_column = random.choice(list(set(selected_columns) & set(non_formatted_attr)))\n",
    "            break\n",
    "\n",
    "    order_type = random.choice(order_options)\n",
    "\n",
    "    limit_value = min(random.sample(limit_list, 1)[0], len(filtered_df) // 2)\n",
    "    valid_where[i][\"Type\"] = \"SFWT\"\n",
    "    valid_where[i][\"SELECT\"] = selected_columns\n",
    "    valid_where[i][\"LIMIT\"] = limit_value\n",
    "    valid_where[i][\"ORDER BY\"] = [order_column, order_type]\n",
    "    query_attr_list = [i for i in selected_columns]\n",
    "    for k in valid_where[i][\"Combination\"]:\n",
    "         if k[0] not in query_attr_list:\n",
    "             query_attr_list += [k[0]]\n",
    "    valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "\n",
    "    valid_sfwt.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwt = random.sample(valid_sfwt, sample_sfwt)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_TOPK.json\", 'w') as f:\n",
    "#     json.dump(sampled_valid_sfwt, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwt, \"./SELECT_FROM_WHERE_TOPK.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | GROUP BY | AGGREGATION | TOP-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9eb6f50b1924c0e9f366db401eeebe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_where = json.load(f)\n",
    "\n",
    "valid_sfwgat = []\n",
    "aggregation_functions = ['COUNT', 'MAX', 'MIN', 'MEAN', 'SUM']\n",
    "order_options = ['ASC', 'DESC']\n",
    "max_COUNT = sample_sfwgat // 4\n",
    "\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "     row_indices = valid_where[i].get(\"WHERE Indices\", [])\n",
    "     filtered_df = df.loc[row_indices]\n",
    "\n",
    "     # while True:\n",
    "     #      selected_groupby_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_groupby))\n",
    "     #      if not set(selected_groupby_columns) & set(formatted_attr):\n",
    "     #           break\n",
    "     valid_groupby_attrs = [attr for attr in category_attr if attr not in multi_value_attributes]\n",
    "     selected_groupby_columns = random.sample(valid_groupby_attrs, 1)\n",
    "     grouped = filtered_df.groupby(selected_groupby_columns)\n",
    "     remaining_rows = grouped.size().reset_index(name='Group Size')\n",
    "\n",
    "     if len(remaining_rows) > (min_rows // 2):\n",
    "          while True:\n",
    "               if max_COUNT > 0:\n",
    "                    selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "                    if selected_aggregation_column not in formatted_attr:\n",
    "                         break\n",
    "               else:\n",
    "                    selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "                    break\n",
    "          if selected_aggregation_column[0] in non_formatted_attr:\n",
    "               function = random.choice(aggregation_functions[1:])\n",
    "               numerical = True\n",
    "          else:\n",
    "               max_COUNT -= 1\n",
    "               function = 'COUNT'\n",
    "               numerical = False\n",
    "\n",
    "          if function == \"COUNT\" and random.uniform(0, 1) > 0.5:\n",
    "               selected_aggregation_column = [\"*\"]\n",
    "\n",
    "          if selected_aggregation_column == [\"*\"]:\n",
    "               agg_result = grouped.transform(lambda x: x.notna().sum())\n",
    "          else:\n",
    "               agg_result = grouped[selected_aggregation_column[0]].transform(lambda x: x.notna().sum())\n",
    "\n",
    "          # agg_result = grouped[selected_aggregation_column].agg(function.lower()) if numerical else grouped.size()\n",
    "          select_set = set(selected_groupby_columns).union(set(selected_aggregation_column))\n",
    "          order_set = select_set & set(non_formatted_attr)\n",
    "          if not order_set:\n",
    "               continue\n",
    "          order_column = random.choice(list(order_set))\n",
    "          order_type = random.choice(order_options)\n",
    "          limit_value = min(random.sample(limit_list, 1)[0], len(agg_result // 2))\n",
    "          valid_where[i][\"Type\"] = \"SFWGAT\"\n",
    "          valid_where[i][\"GROUP BY\"] = selected_groupby_columns\n",
    "          valid_where[i][\"GROUP BY Total Rows\"] = len(remaining_rows)\n",
    "          valid_where[i][\"SELECT\"] = selected_groupby_columns + [f\"{function}({selected_aggregation_column[0]})\"]\n",
    "          valid_where[i][\"AGGREGATION\"] = selected_aggregation_column\n",
    "          if function == \"MEAN\":\n",
    "               valid_where[i][\"AGGREGATION Function\"] = \"AVG\"\n",
    "          else:\n",
    "               valid_where[i][\"AGGREGATION Function\"] = function\n",
    "          valid_where[i][\"Numerical\"] = numerical\n",
    "          valid_where[i][\"LIMIT\"] = limit_value\n",
    "          valid_where[i][\"ORDER BY\"] = [order_column, order_type]\n",
    "          query_attr_list = [i for i in selected_aggregation_column]\n",
    "          for k in valid_where[i][\"Combination\"]:\n",
    "               if k[0] not in query_attr_list:\n",
    "                    query_attr_list += [k[0]]\n",
    "          for k in selected_groupby_columns:\n",
    "               if k not in query_attr_list:\n",
    "                    query_attr_list += [k]\n",
    "          valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "                    \n",
    "          valid_sfwgat.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwgat = random.sample(valid_sfwgat, sample_sfwgat)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION_TOPK.json\", 'w') as f:\n",
    "#     json.dump(sampled_valid_sfwgat, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwgat, \"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION_TOPK.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已生成SQL文件: ./sql_queries/SELECT_FROM.sql\n",
      "已生成SQL文件: ./sql_queries/SELECT_FROM_WHERE.sql\n",
      "已生成SQL文件: ./sql_queries/SELECT_FROM_WHERE_TOPK.sql\n",
      "已生成SQL文件: ./sql_queries/SELECT_FROM_WHERE_GROUPBY.sql\n",
      "已生成SQL文件: ./sql_queries/SELECT_FROM_WHERE_AGGREGATION.sql\n",
      "已生成SQL文件: ./sql_queries/SELECT_FROM_AGGREGATION_GROUPBY.sql\n",
      "已生成SQL文件: ./sql_queries/SELECT_FROM_WHERE_GROUPBY_AGGREGATION.sql\n",
      "已生成SQL文件: ./sql_queries/SELECT_FROM_WHERE_GROUPBY_AGGREGATION_TOPK.sql\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#### 根据构造的查询结构生成对应的SQL语句\n",
    "def generate_sql_query(query_dict, table_name=\"LCR\"):\n",
    "    \"\"\"\n",
    "    根据查询字典生成对应的SQL语句（支持SF类型）\n",
    "    \"\"\"\n",
    "    query_type = query_dict.get(\"Type\", \"\")\n",
    "    \n",
    "    # 构建SELECT子句\n",
    "    select_clause = \"SELECT \" + \", \".join(query_dict[\"SELECT\"])\n",
    "    \n",
    "    # 构建FROM子句\n",
    "    from_clause = f\"FROM {table_name}\"\n",
    "    \n",
    "    # 构建WHERE子句（SF类型没有WHERE）\n",
    "    where_clause = \"\"\n",
    "    if \"WHERE\" in query_dict and query_dict[\"WHERE\"] != \"None\":\n",
    "        where_clause = f\"WHERE {query_dict['WHERE']}\"\n",
    "    \n",
    "    # 构建GROUP BY子句\n",
    "    group_by_clause = \"\"\n",
    "    if \"GROUP BY\" in query_dict:\n",
    "        group_by_clause = f\"GROUP BY {', '.join(query_dict['GROUP BY'])}\"\n",
    "    \n",
    "    # 构建ORDER BY子句\n",
    "    order_by_clause = \"\"\n",
    "    if \"ORDER BY\" in query_dict:\n",
    "        order_col, order_type = query_dict[\"ORDER BY\"]\n",
    "        order_by_clause = f\"ORDER BY {order_col} {order_type}\"\n",
    "    \n",
    "    # 构建LIMIT子句\n",
    "    limit_clause = \"\"\n",
    "    if \"LIMIT\" in query_dict:\n",
    "        limit_clause = f\"LIMIT {query_dict['LIMIT']}\"\n",
    "    \n",
    "    # 组合完整SQL\n",
    "    sql_parts = [select_clause, from_clause]\n",
    "    if where_clause:\n",
    "        sql_parts.append(where_clause)\n",
    "    if group_by_clause:\n",
    "        sql_parts.append(group_by_clause)\n",
    "    if order_by_clause:\n",
    "        sql_parts.append(order_by_clause)\n",
    "    if limit_clause:\n",
    "        sql_parts.append(limit_clause)\n",
    "    \n",
    "    return \"\\n\".join(sql_parts) + \";\"\n",
    "\n",
    "def generate_schema_sql(schema_dict, table_name=\"LCR\"):\n",
    "    \"\"\"\n",
    "    根据SCHEMA字典生成建表SQL语句\n",
    "    \"\"\"\n",
    "    create_table = f\"CREATE TABLE {table_name} (\\n\"\n",
    "    columns = []\n",
    "    \n",
    "    for col_name, (data_type, description) in schema_dict.items():\n",
    "        comment = f\" COMMENT '{description}'\" if description else \"\"\n",
    "        columns.append(f\"    {col_name} {data_type}{comment}\")\n",
    "    \n",
    "    create_table += \",\\n\".join(columns)\n",
    "    create_table += \"\\n);\"\n",
    "    \n",
    "    return create_table\n",
    "\n",
    "def save_queries_with_sql(input_files, output_dir=\"./sql_queries/\"):\n",
    "    \"\"\"\n",
    "    读取所有生成的查询文件，为每个查询生成对应的SQL语句并保存（支持SF类型）\n",
    "    修正：使用正确的换行符格式\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for file_path in input_files:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"文件不存在: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            queries = json.load(f)\n",
    "        \n",
    "        # 为每个查询生成SQL\n",
    "        for i, query in enumerate(queries):\n",
    "            # 生成建表SQL\n",
    "            schema_sql = generate_schema_sql(query[\"SCHEMA\"])\n",
    "            \n",
    "            # 生成查询SQL\n",
    "            query_sql = generate_sql_query(query)\n",
    "            \n",
    "            # 计算SELECT数量\n",
    "            select_count = len(query.get(\"SELECT\", []))\n",
    "            \n",
    "            # 计算Filter数量 - 使用Combination字段获取最准确的数量\n",
    "            filter_count = len(query.get(\"Combination\", []))\n",
    "            \n",
    "            # 组合完整SQL - 修正：使用正确的换行符 \\n 而不是 \\\\n\n",
    "            complete_sql = f\"-- Query {i+1} ({query['Type']})\\n\"\n",
    "            complete_sql += f\"-- Total Rows: {query.get('WHERE Total Rows', 'N/A')}\\n\"\n",
    "            complete_sql += f\"-- SELECT: {select_count}\\n\"\n",
    "            complete_sql += f\"-- FILTER: {filter_count}\\n\\n\"\n",
    "            complete_sql += schema_sql + \"\\n\\n\"\n",
    "            complete_sql += query_sql + \"\\n\"\n",
    "            complete_sql += \"-\" * 50 + \"\\n\\n\"\n",
    "            \n",
    "            query[\"SQL\"] = {\n",
    "                \"schema\": schema_sql,\n",
    "                \"query\": query_sql,\n",
    "                \"complete\": complete_sql\n",
    "            }\n",
    "        \n",
    "        # 保存包含SQL的查询文件\n",
    "        output_file = output_dir + os.path.basename(file_path).replace('.json', '_with_sql.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(queries, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # 单独保存纯SQL文件\n",
    "        sql_file = output_dir + os.path.basename(file_path).replace('.json', '.sql')\n",
    "        with open(sql_file, 'w', encoding='utf-8') as f:\n",
    "            for query in queries:\n",
    "                f.write(query[\"SQL\"][\"complete\"])\n",
    "        \n",
    "        print(f\"已生成SQL文件: {sql_file}\")\n",
    "\n",
    "# 使用示例：为所有查询类型生成SQL\n",
    "query_files = [\n",
    "    \"./SELECT_FROM.json\",\n",
    "    \"./SELECT_FROM_WHERE.json\",\n",
    "    \"./SELECT_FROM_WHERE_TOPK.json\", \n",
    "    \"./SELECT_FROM_WHERE_GROUPBY.json\",\n",
    "    \"./SELECT_FROM_WHERE_AGGREGATION.json\",\n",
    "    \"./SELECT_FROM_AGGREGATION_GROUPBY.json\",\n",
    "    \"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION.json\",\n",
    "    \"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION_TOPK.json\"\n",
    "]\n",
    "\n",
    "# 生成所有SQL查询\n",
    "save_queries_with_sql(query_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成SQL文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing templates: 100%|██████████| 10/10 [00:04<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成完成!\n",
      "成功: 174 | 失败: 26 | 总计: 200\n",
      "模板: 10 | 查询类型: 7\n",
      "注意: 26 个查询因Filter组合过于严格而失败，但保持了完整的Filter数量\n",
      "文件保存在: ./Fixed_filters/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### SQL文件生成器 - 按类别和类型分文件夹\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 法律数据集的语义模板定义 - 按Filter数量2:3:3:1:1分布\n",
    "TEMPLATES = [\n",
    "    # 1个Filter (2个类别)\n",
    "    {\"name\": \"judge_focus\", \"description\": \"特定法官案件分析\", \"filters\": [\"judge_name\"]},\n",
    "    {\"name\": \"case_type_focus\", \"description\": \"特定案件类型研究\", \"filters\": [\"case_type\"]},\n",
    "    \n",
    "    # 2个Filter (3个类别) \n",
    "    {\"name\": \"judge_verdict\", \"description\": \"法官与判决结果关联\", \"filters\": [\"judge_name\", \"verdict\"]},\n",
    "    {\"name\": \"location_year\", \"description\": \"审理地点与年份分析\", \"filters\": [\"hearing_location\", \"hearing_year\"]},\n",
    "    {\"name\": \"type_outcome\", \"description\": \"案件类型与结果关系\", \"filters\": [\"case_type\", \"verdict\"]},\n",
    "    \n",
    "    # 3个Filter (3个类别)\n",
    "    {\"name\": \"court_analysis\", \"description\": \"法庭审理综合分析\", \"filters\": [\"judge_name\", \"hearing_location\", \"case_type\"]},\n",
    "    {\"name\": \"temporal_pattern\", \"description\": \"时间模式与结果研究\", \"filters\": [\"hearing_year\", \"judgment_year\", \"verdict\"]},\n",
    "    {\"name\": \"party_analysis\", \"description\": \"当事方特征分析\", \"filters\": [\"plaintiff\", \"defendant\", \"case_type\"]},\n",
    "    \n",
    "    # 4个Filter (1个类别)\n",
    "    {\"name\": \"complex_litigation\", \"description\": \"复杂诉讼案件深度研究\", \"filters\": [\"case_type\", \"judge_name\", \"hearing_location\", \"legal_fees\"]},\n",
    "    \n",
    "    # 5个Filter (1个类别)\n",
    "    {\"name\": \"comprehensive_case_study\", \"description\": \"案件全方位分析\", \"filters\": [\"judge_name\", \"case_type\", \"verdict\", \"hearing_location\", \"legal_fees\"]}\n",
    "]\n",
    "\n",
    "QUERY_TYPES = [\"SFW\", \"SFWT\", \"SFWG\", \"SFWA\", \"SFAG\", \"SFWGA\", \"SFWGAT\"]\n",
    "\n",
    "class SQLGenerator:\n",
    "    def __init__(self, min_result_rows=5):\n",
    "        self.df = globals()['df']\n",
    "        self.attr_dict = globals()['attr_value_dict']\n",
    "        self.numerical_attr = globals()['numerical_attr']\n",
    "        self.non_numerical_attr = globals()['non_numerical_attr']\n",
    "        self.category_attr = globals()['category_attr']\n",
    "        self.formatted_attr = globals()['formatted_attr']\n",
    "        self.attr_desc_dict = globals()['attr_desc_dict']\n",
    "        self.min_rows = min_result_rows\n",
    "        \n",
    "        # 分析数据分布用于放宽策略\n",
    "        self.stats = self._analyze_data()\n",
    "        \n",
    "    def _analyze_data(self):\n",
    "        \"\"\"分析数据分布\"\"\"\n",
    "        stats = {}\n",
    "        for attr in self.attr_dict.keys():\n",
    "            if attr in self.non_numerical_attr:\n",
    "                try:\n",
    "                    value_counts = self.df[attr].value_counts()\n",
    "                    stats[attr] = {\n",
    "                        \"type\": \"categorical\", \n",
    "                        \"top_values\": value_counts.head(5).to_dict()\n",
    "                    }\n",
    "                except:\n",
    "                    stats[attr] = {\"type\": \"categorical\", \"top_values\": {}}\n",
    "        return stats\n",
    "        \n",
    "    def get_filter_value(self, attr):\n",
    "        \"\"\"从attr_value_dict获取值和条件\"\"\"\n",
    "        values = self.attr_dict.get(attr, [])\n",
    "        if not values:\n",
    "            return None, None\n",
    "            \n",
    "        if attr in self.numerical_attr:\n",
    "            val = random.choice(values)\n",
    "            op = random.choice([\"==\", \">\", \">=\", \"<\", \"<=\"])\n",
    "            return val, f\"{attr} {op} {val}\"\n",
    "        else:\n",
    "            val = random.choice(values)\n",
    "            return val, f\"{attr} == '{val}'\"\n",
    "    \n",
    "    def get_filter_value(self, attr, relaxation_level=0):\n",
    "        \"\"\"从attr_value_dict获取值和条件，支持放宽策略\"\"\"\n",
    "        values = self.attr_dict.get(attr, [])\n",
    "        if not values:\n",
    "            return None, None\n",
    "        \n",
    "        if attr in self.numerical_attr:\n",
    "            val = random.choice(values)\n",
    "            \n",
    "            # 根据放宽级别调整操作符\n",
    "            if relaxation_level == 0:\n",
    "                # 正常策略：所有操作符\n",
    "                op = random.choice([\"==\", \">\", \">=\", \"<\", \"<=\"])\n",
    "            elif relaxation_level == 1:\n",
    "                # 第一次放宽：偏向宽松操作符\n",
    "                op = random.choice([\">=\", \"<=\", \"==\", \">\", \"<\"])\n",
    "            elif relaxation_level == 2:\n",
    "                # 第二次放宽：更多宽松操作符\n",
    "                op = random.choice([\">=\", \"<=\", \">=\", \"<=\", \"==\"])\n",
    "            else:\n",
    "                # 最大放宽：只用最宽松的操作符\n",
    "                op = random.choice([\">=\", \"<=\"])\n",
    "                \n",
    "            # 对于特定数值属性的智能调整\n",
    "            if relaxation_level >= 1:\n",
    "                if attr == \"Age\":\n",
    "                    # 年龄放宽：选择较小的值用于>=，较大的值用于<=\n",
    "                    sorted_vals = sorted(values)\n",
    "                    if op in [\">=\", \">\"]:\n",
    "                        val = random.choice(sorted_vals[:len(sorted_vals)//2])\n",
    "                    elif op in [\"<=\", \"<\"]:\n",
    "                        val = random.choice(sorted_vals[len(sorted_vals)//2:])\n",
    "                elif attr == \"Awards\":\n",
    "                    # 奖项放宽：降低阈值\n",
    "                    if op in [\">=\", \">\"]:\n",
    "                        min_awards = min(values)\n",
    "                        val = min_awards if relaxation_level >= 2 else random.choice([v for v in values if v <= min_awards + 2])\n",
    "            \n",
    "            return val, f\"{attr} {op} {val}\"\n",
    "        else:\n",
    "            val = random.choice(values)\n",
    "            return val, f\"{attr} == '{val}'\"\n",
    "    \n",
    "    def apply_filters_with_relaxation(self, filters_config, max_relaxation=3):\n",
    "        \"\"\"应用过滤条件，支持自动放宽策略\"\"\"\n",
    "        \n",
    "        for relaxation_level in range(max_relaxation + 1):\n",
    "            result_indices, applied_conditions = self._try_apply_filters(filters_config, relaxation_level)\n",
    "            \n",
    "            # 如果有结果，返回\n",
    "            if len(result_indices) >= 2:  # 最少2行结果\n",
    "                return result_indices, applied_conditions, relaxation_level\n",
    "        \n",
    "        # 所有级别都失败，返回失败标记\n",
    "        return None, None, \"FAILED_ALL_RELAXATION\"\n",
    "    \n",
    "    def get_filter_value(self, attr, relaxation_level=0):\n",
    "        \"\"\"从attr_value_dict获取值和条件，支持放宽策略\"\"\"\n",
    "        values = self.attr_dict.get(attr, [])\n",
    "        if not values:\n",
    "            return None, None\n",
    "        \n",
    "        if attr in self.numerical_attr:\n",
    "            val = random.choice(values)\n",
    "            \n",
    "            # 根据放宽级别调整操作符，让条件更宽松\n",
    "            if relaxation_level == 0:\n",
    "                # 正常策略：所有操作符\n",
    "                op = random.choice([\"==\", \">\", \">=\", \"<\", \"<=\"])\n",
    "            elif relaxation_level == 1:\n",
    "                # 第一次放宽：偏向宽松操作符，避免等于\n",
    "                op = random.choice([\">=\", \"<=\", \">\", \"<\", \">=\", \"<=\"])\n",
    "            elif relaxation_level == 2:\n",
    "                # 第二次放宽：只用最宽松的操作符\n",
    "                op = random.choice([\">=\", \"<=\"])\n",
    "            else:\n",
    "                # 最大放宽：选择能包含更多数据的阈值\n",
    "                op = random.choice([\">=\", \"<=\"])\n",
    "                \n",
    "            # 对于特定数值属性的智能调整\n",
    "            if relaxation_level >= 1:\n",
    "                if attr == \"Age\":\n",
    "                    # 年龄放宽：选择更极端的值来包含更多人\n",
    "                    sorted_vals = sorted(values)\n",
    "                    if op in [\">=\", \">\"]:\n",
    "                        # 选择较小的年龄值，让更多人满足条件\n",
    "                        val = random.choice(sorted_vals[:len(sorted_vals)//3])\n",
    "                    elif op in [\"<=\", \"<\"]:\n",
    "                        # 选择较大的年龄值，让更多人满足条件\n",
    "                        val = random.choice(sorted_vals[2*len(sorted_vals)//3:])\n",
    "                elif attr == \"Awards\":\n",
    "                    # 奖项放宽：降低门槛\n",
    "                    if op in [\">=\", \">\"]:\n",
    "                        min_awards = min([v for v in values if isinstance(v, (int, float))])\n",
    "                        val = min_awards if relaxation_level >= 2 else random.choice([v for v in values if v <= min_awards + 1])\n",
    "            \n",
    "            return val, f\"{attr} {op} {val}\"\n",
    "        else:\n",
    "            # 非数值属性：选择更常见的值\n",
    "            if relaxation_level >= 1 and attr in self.stats and self.stats[attr][\"type\"] == \"categorical\":\n",
    "                # 选择出现频率更高的值\n",
    "                top_values = list(self.stats[attr][\"top_values\"].keys())[:3]  # 前3个最常见的值\n",
    "                common_values = [v for v in top_values if v in values]\n",
    "                if common_values:\n",
    "                    val = random.choice(common_values)\n",
    "                else:\n",
    "                    val = random.choice(values)\n",
    "            else:\n",
    "                val = random.choice(values)\n",
    "            return val, f\"{attr} == '{val}'\"\n",
    "    \n",
    "    def _try_apply_filters(self, filters_config, relaxation_level):\n",
    "        \"\"\"尝试应用过滤条件，严格保持Filter数量\"\"\"\n",
    "        result_indices = set(range(len(self.df)))\n",
    "        applied_conditions = []\n",
    "        \n",
    "        # 必须使用所有Filter，数量必须与模板定义完全一致\n",
    "        for attr in filters_config:\n",
    "            val, condition = self.get_filter_value(attr, relaxation_level)\n",
    "            if not val or not condition:\n",
    "                # 如果某个Filter无法生成条件，整个查询失败\n",
    "                return [], []\n",
    "                \n",
    "            try:\n",
    "                # 测试过滤效果\n",
    "                if condition.endswith(\"'\"):  # 字符串条件\n",
    "                    mask = self.df[attr].apply(lambda x: \n",
    "                        str(x).strip().lower() == str(val).strip().lower() or\n",
    "                        (isinstance(x, str) and '||' in x and str(val) in x.split('||')))\n",
    "                elif \">=\" in condition:\n",
    "                    mask = self.df[attr] >= val\n",
    "                elif \">\" in condition:\n",
    "                    mask = self.df[attr] > val\n",
    "                elif \"<=\" in condition:\n",
    "                    mask = self.df[attr] <= val\n",
    "                elif \"<\" in condition:\n",
    "                    mask = self.df[attr] < val\n",
    "                elif \"==\" in condition:\n",
    "                    mask = self.df[attr] == val\n",
    "                else:\n",
    "                    # 如果条件格式错误，整个查询失败\n",
    "                    return [], []\n",
    "                \n",
    "                new_indices = set(self.df[mask].index)\n",
    "                result_indices &= new_indices\n",
    "                applied_conditions.append(condition)\n",
    "                \n",
    "            except Exception:\n",
    "                # 如果任何Filter应用失败，整个查询失败\n",
    "                return [], []\n",
    "        \n",
    "        # 验证Filter数量完全匹配\n",
    "        if len(applied_conditions) != len(filters_config):\n",
    "            return [], []\n",
    "        \n",
    "        return list(result_indices), applied_conditions\n",
    "    \n",
    "    def create_schema_sql(self, attrs, table_name=\"Wikiart\"):\n",
    "        \"\"\"生成建表SQL\"\"\"\n",
    "        schema_parts = []\n",
    "        for attr in set(attrs):\n",
    "            if '(' in attr:  # 跳过聚合函数\n",
    "                continue\n",
    "            if attr in self.numerical_attr:\n",
    "                schema_parts.append(f\"    {attr} FLOAT\")\n",
    "            elif attr in self.formatted_attr:\n",
    "                schema_parts.append(f\"    {attr} DATE\")\n",
    "            else:\n",
    "                schema_parts.append(f\"    {attr} VARCHAR(255)\")\n",
    "        \n",
    "        return f\"CREATE TABLE {table_name} (\\n\" + \",\\n\".join(schema_parts) + \"\\n);\"\n",
    "    \n",
    "    def generate_query_sql(self, template, qtype, query_id):\n",
    "        \"\"\"生成单个查询的SQL，带自动放宽机制\"\"\"\n",
    "        # 使用自适应放宽策略\n",
    "        if qtype == \"SFAG\":\n",
    "            indices = list(range(len(self.df)))\n",
    "            conditions = []\n",
    "            relaxation_used = 0\n",
    "        else:\n",
    "            result = self.apply_filters_with_relaxation(template[\"filters\"])\n",
    "            \n",
    "            if result[2] == \"FAILED_ALL_RELAXATION\":\n",
    "                return None\n",
    "            \n",
    "            indices, conditions, relaxation_used = result\n",
    "            \n",
    "            if len(indices) < 2 or not conditions:\n",
    "                return None\n",
    "                \n",
    "        result = self.apply_filters_with_relaxation(template[\"filters\"])\n",
    "        \n",
    "        # 检查是否完全失败\n",
    "        if result[2] == \"FAILED_ALL_RELAXATION\":\n",
    "            # 生成失败标注的SQL注释\n",
    "            header = f\"-- Query {query_id} - {qtype} [GENERATION FAILED]\\n\"\n",
    "            header += f\"-- Template: {template['name']}\\n\"\n",
    "            header += f\"-- Description: {template['description']}\\n\"\n",
    "            header += f\"-- Required Filters: {len(template['filters'])} (STRICTLY MAINTAINED)\\n\"\n",
    "            header += f\"-- Filter List: {', '.join(template['filters'])}\\n\"\n",
    "            header += f\"-- Status: All {len(template['filters'])} filters must be applied but combination yields no results\\n\"\n",
    "            header += f\"-- Reason: Filter combination too restrictive for current dataset even with maximum value relaxation\\n\"\n",
    "            header += f\"-- Semantic Integrity: PRESERVED - No filter reduction allowed\\n\\n\"\n",
    "            \n",
    "            # 创建一个示例SQL结构用于说明\n",
    "            example_sql = f\"-- Required SQL structure (unfulfillable):\\n\"\n",
    "            example_sql += f\"-- CREATE TABLE Wikiart (...columns for {', '.join(template['filters'])}...);\\n\"\n",
    "            example_sql += f\"-- SELECT ... FROM Wikiart WHERE <ALL {len(template['filters'])} FILTERS REQUIRED>;\\n\\n\"\n",
    "            \n",
    "            complete_sql = header + example_sql + \"-\" * 60 + \"\\n\\n\"\n",
    "            return complete_sql\n",
    "        \n",
    "        indices, conditions, relaxation_used = result\n",
    "        \n",
    "        if len(indices) < 2 or not conditions:\n",
    "            return None\n",
    "        \n",
    "        # 基本列选择\n",
    "        available_attrs = list(self.attr_desc_dict.keys())\n",
    "        select_cols = random.sample(available_attrs, random.randint(1, 3))\n",
    "        \n",
    "        # 根据查询类型构建SQL\n",
    "        sql_parts = []\n",
    "        schema_attrs = select_cols.copy()\n",
    "        \n",
    "        if qtype == \"SFW\":\n",
    "            sql_parts.append(f\"SELECT {', '.join(select_cols)}\")\n",
    "            \n",
    "        elif qtype == \"SFWT\":\n",
    "            numeric_cols = [c for c in select_cols if c in self.numerical_attr]\n",
    "            if not numeric_cols:\n",
    "                numeric_cols = [random.choice(self.numerical_attr)]\n",
    "                select_cols.extend(numeric_cols)\n",
    "                schema_attrs.extend(numeric_cols)\n",
    "            order_col = random.choice(numeric_cols)\n",
    "            order_dir = random.choice([\"ASC\", \"DESC\"])\n",
    "            limit_val = random.choice([5, 10, 15, 20])\n",
    "            sql_parts.append(f\"SELECT {', '.join(select_cols)}\")\n",
    "            sql_parts.append(\"FROM Wikiart\")\n",
    "            sql_parts.append(f\"WHERE {' AND '.join(conditions)}\")\n",
    "            sql_parts.append(f\"ORDER BY {order_col} {order_dir}\")\n",
    "            sql_parts.append(f\"LIMIT {limit_val}\")\n",
    "            \n",
    "        elif qtype == \"SFWG\":\n",
    "            group_col = random.choice(self.category_attr)\n",
    "            if group_col not in select_cols:\n",
    "                select_cols.append(group_col)\n",
    "                schema_attrs.append(group_col)\n",
    "            sql_parts.append(f\"SELECT {', '.join(select_cols)}\")\n",
    "            sql_parts.append(\"FROM Wikiart\")\n",
    "            sql_parts.append(f\"WHERE {' AND '.join(conditions)}\")\n",
    "            sql_parts.append(f\"GROUP BY {group_col}\")\n",
    "            \n",
    "        elif qtype == \"SFWA\":\n",
    "            func = random.choice([\"COUNT\", \"MAX\", \"MIN\", \"AVG\", \"SUM\"])\n",
    "            if func == \"COUNT\" and random.random() < 0.3:\n",
    "                agg_col = \"*\"\n",
    "                select_cols = [f\"{func}(*)\"]\n",
    "            else:\n",
    "                agg_col = random.choice(self.numerical_attr)\n",
    "                select_cols = [f\"{func}({agg_col})\"]\n",
    "                schema_attrs = [agg_col]\n",
    "            sql_parts.append(f\"SELECT {', '.join(select_cols)}\")\n",
    "            \n",
    "            \n",
    "        elif qtype == \"SFWGA\":\n",
    "            group_col = random.choice(self.category_attr)\n",
    "            func = random.choice([\"COUNT\", \"MAX\", \"MIN\", \"AVG\", \"SUM\"])\n",
    "            if func == \"COUNT\" and random.random() < 0.3:\n",
    "                agg_col = \"*\"\n",
    "                select_cols = [group_col, f\"{func}(*)\"]\n",
    "            else:\n",
    "                agg_col = random.choice([c for c in self.numerical_attr if c != group_col])\n",
    "                select_cols = [group_col, f\"{func}({agg_col})\"]\n",
    "                schema_attrs = [group_col, agg_col]\n",
    "            sql_parts.append(f\"SELECT {', '.join(select_cols)}\")\n",
    "            sql_parts.append(\"FROM Wikiart\")\n",
    "            sql_parts.append(f\"WHERE {' AND '.join(conditions)}\")\n",
    "            sql_parts.append(f\"GROUP BY {group_col}\")\n",
    "            \n",
    "        elif qtype == \"SFWGAT\":\n",
    "            group_col = random.choice(self.category_attr)\n",
    "            func = random.choice([\"MAX\", \"MIN\", \"AVG\", \"SUM\"])\n",
    "            agg_col = random.choice([c for c in self.numerical_attr if c != group_col])\n",
    "            order_dir = random.choice([\"ASC\", \"DESC\"])\n",
    "            limit_val = random.choice([5, 10, 15])\n",
    "            select_cols = [group_col, f\"{func}({agg_col})\"]\n",
    "            schema_attrs = [group_col, agg_col]\n",
    "            sql_parts.append(f\"SELECT {', '.join(select_cols)}\")\n",
    "            sql_parts.append(\"FROM Wikiart\")\n",
    "            sql_parts.append(f\"WHERE {' AND '.join(conditions)}\")\n",
    "            sql_parts.append(f\"GROUP BY {group_col}\")\n",
    "            sql_parts.append(f\"ORDER BY {agg_col} {order_dir}\")\n",
    "            sql_parts.append(f\"LIMIT {limit_val}\")\n",
    "        \n",
    "        # 添加基本的FROM和WHERE（如果还没有）\n",
    "        if len(sql_parts) == 1:  # 只有SELECT\n",
    "            sql_parts.append(\"FROM Wikiart\")\n",
    "            if conditions:\n",
    "                sql_parts.append(f\"WHERE {' AND '.join(conditions)}\")\n",
    "        \n",
    "        # 添加过滤条件中的属性到schema\n",
    "        for condition in conditions:\n",
    "            attr_name = condition.split()[0]\n",
    "            if attr_name not in schema_attrs:\n",
    "                schema_attrs.append(attr_name)\n",
    "        \n",
    "        # 生成完整SQL\n",
    "        schema_sql = self.create_schema_sql(schema_attrs)\n",
    "        query_sql = \"\\n\".join(sql_parts) + \";\"\n",
    "        \n",
    "        header = f\"-- Query {query_id} - {qtype}\\n\"\n",
    "        header += f\"-- Template: {template['name']}\\n\"\n",
    "        header += f\"-- Description: {template['description']}\\n\"\n",
    "        header += f\"-- Result Rows: {len(indices)}\\n\"\n",
    "        header += f\"-- Filters Applied: {len(conditions)}/{len(template['filters'])} (EXACT MATCH REQUIRED)\"\n",
    "        if len(conditions) != len(template['filters']):\n",
    "            header += \" [ERROR: Filter count mismatch]\"\n",
    "        elif relaxation_used > 0:\n",
    "            header += f\" (Values relaxed {relaxation_used} times)\"\n",
    "        header += \"\\n\\n\"\n",
    "        \n",
    "        complete_sql = header + schema_sql + \"\\n\\n\" + query_sql + \"\\n\\n\" + \"-\" * 60 + \"\\n\\n\"\n",
    "        \n",
    "        return complete_sql\n",
    "\n",
    "def generate_all_sql_files():\n",
    "    \"\"\"生成所有SQL文件，按类别和类型分文件夹\"\"\"\n",
    "    print(\"生成SQL文件...\")\n",
    "    \n",
    "    # 创建主输出目录\n",
    "    base_dir = \"./Fixed_filters/\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    generator = SQLGenerator()\n",
    "    queries_per_type = 5  # 每个类型生成5个查询\n",
    "    \n",
    "    # 统计信息\n",
    "    generation_stats = {}\n",
    "    \n",
    "    # 为每个模板创建文件夹\n",
    "    for template in tqdm(TEMPLATES, desc=\"Processing templates\"):\n",
    "        template_dir = os.path.join(base_dir, template[\"name\"])\n",
    "        os.makedirs(template_dir, exist_ok=True)\n",
    "        \n",
    "        template_stats = {}\n",
    "        \n",
    "        # 为每个查询类型生成SQL文件\n",
    "        for qtype in QUERY_TYPES:\n",
    "            sql_content = []\n",
    "            query_id = 1\n",
    "            \n",
    "            # 生成多个查询\n",
    "            generated = 0\n",
    "            attempts = 0\n",
    "            max_attempts = queries_per_type * 5  # 增加尝试次数\n",
    "            \n",
    "            while generated < queries_per_type and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                \n",
    "                sql = generator.generate_query_sql(template, qtype, query_id)\n",
    "                if sql:\n",
    "                    sql_content.append(sql)\n",
    "                    generated += 1\n",
    "                    query_id += 1\n",
    "            \n",
    "            template_stats[qtype] = generated\n",
    "            \n",
    "            # 保存SQL文件\n",
    "            if sql_content:\n",
    "                filename = os.path.join(template_dir, f\"{qtype}.sql\")\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"-- {template['description']} - {qtype} 查询集合\\n\")\n",
    "                    f.write(f\"-- 模板: {template['name']}\\n\")\n",
    "                    f.write(f\"-- Filter数量: {len(template['filters'])}\\n\")\n",
    "                    f.write(\"-- \" + \"=\" * 60 + \"\\n\\n\")\n",
    "                    f.write(\"\".join(sql_content))\n",
    "            else:\n",
    "                # 创建空文件说明原因\n",
    "                filename = os.path.join(template_dir, f\"{qtype}.sql\")\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"-- {template['description']} - {qtype} 查询集合\\n\")\n",
    "                    f.write(f\"-- 模板: {template['name']}\\n\")\n",
    "                    f.write(f\"-- Filter数量: {len(template['filters'])}\\n\")\n",
    "                    f.write(\"-- 注意: 由于Filter条件过于严格，未能生成有效查询\\n\")\n",
    "                    f.write(\"-- 建议: 可以手动调整Filter条件或降低最小结果行数要求\\n\")\n",
    "        \n",
    "        generation_stats[template[\"name\"]] = template_stats\n",
    "    \n",
    "    # 输出精简统计信息\n",
    "    print(f\"生成完成!\")\n",
    "    \n",
    "    total_queries = 0\n",
    "    failed_queries = 0\n",
    "    \n",
    "    for template_name, type_stats in generation_stats.items():\n",
    "        for qtype, count in type_stats.items():\n",
    "            total_queries += count\n",
    "    \n",
    "    # 统计失败的查询（通过检查SQL文件中的FAILED标记）\n",
    "    for template in TEMPLATES:\n",
    "        template_dir = os.path.join(base_dir, template[\"name\"])\n",
    "        for qtype in QUERY_TYPES:\n",
    "            sql_file = os.path.join(template_dir, f\"{qtype}.sql\")\n",
    "            if os.path.exists(sql_file):\n",
    "                with open(sql_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    failed_queries += content.count(\"[GENERATION FAILED]\")\n",
    "    \n",
    "    success_queries = total_queries - failed_queries\n",
    "    \n",
    "    print(f\"成功: {success_queries} | 失败: {failed_queries} | 总计: {total_queries}\")\n",
    "    print(f\"模板: {len(TEMPLATES)} | 查询类型: {len(QUERY_TYPES)}\")\n",
    "    \n",
    "    if failed_queries > 0:\n",
    "        print(f\"注意: {failed_queries} 个查询因Filter组合过于严格而失败，但保持了完整的Filter数量\")\n",
    "    \n",
    "    print(f\"文件保存在: {base_dir}\")\n",
    "\n",
    "# 执行生成\n",
    "generate_all_sql_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
