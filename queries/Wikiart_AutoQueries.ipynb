{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import openpyxl\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.autonotebook import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义输入输出路径，并加载数据\n",
    "- ```dataset_dir```：标注好的数据集表格路径\n",
    "- ```statistics_output_dir```：统计数据表输出路径，包括属性值、选择率、基数\n",
    "- ```valid_where_output_dir```：所有有效谓词组合的输出路径\n",
    "- **注意**：区分标注好的表格是 .csv 还是 .xlsx 格式，相应改变 pandas 读取形式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = r\"/data2/liujinqi/Benchmark/Query/Wikiart_AutoConstruct/Text/Wikiart_All_Attr_Joint.csv\"\n",
    "statistics_output_dir = r\"/data2/liujinqi/Benchmark/Query/Wikiart_AutoConstruct/Text/Wikiart_Statistics_Text.csv\"\n",
    "valid_where_output_dir = r\"/data2/liujinqi/Benchmark/Query/Wikiart_AutoConstruct/Text/valid_WHERE.json\"\n",
    "# df = pd.read_csv(dataset_dir)\n",
    "df = pd.read_csv(dataset_dir, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义属性，方便后面按不同属性类型设计不同的构造方法\n",
    "- ```attr_desc_dict```：全部属性的集合，以及对应的自然语言描述\n",
    "- ```non_numerical_attr```：非数值属性的集合\n",
    "- ```numerical_attr```：数值属性的集合\n",
    "- ```non_formatted_attr```：非格式化数值属性\n",
    "- ```formatted_attr```：格式化数值属性，如日期\n",
    "- ```category_attr```：固定类别的属性\n",
    "- ```multi_value_attributes```：多值的属性，用\"||\"分隔或者逗号分隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 属性描述字典\n",
    "attr_desc_dict = {\n",
    "    \"Name\": \"\",\n",
    "    \"Nationality\": \"\",\n",
    "    \"Birth_date\": \"\",\n",
    "    \"Death_date\": \"\",\n",
    "    \"Age\": \"\",\n",
    "    \"Century\": \"\",\n",
    "    \"Zodiac\": \"\",\n",
    "    \"Birth_country\": \"\", \n",
    "    \"Birth_city\": \"\",\n",
    "    \"Birth_continent\": \"\",  \n",
    "    \"Death_country\": \"\",\n",
    "    \"Death_city\": \"\",\n",
    "    \"Field\": \"\",\n",
    "    \"Genre\": \"\",\n",
    "    \"Marriage\": \"\",\n",
    "    \"Art_institution\": \"\",\n",
    "    \"Teaching\": \"\",\n",
    "    \"Awards\": \"\",\n",
    "    \"Style\": \"\",\n",
    "    \"Image_genre\": \"\",\n",
    "    \"Color\": \"\",\n",
    "    \"Tone\": \"\",\n",
    "    \"Composition\": \"\",\n",
    "}\n",
    "\n",
    "# 定义表特有的属性\n",
    "text_attributes = [\"Name\", \"Nationality\", \"Birth_date\", \"Death_date\", \"Age\", \n",
    "    \"Century\", \"Zodiac\", \"Birth_country\", \"Birth_city\", \"Birth_continent\", \"Death_country\", \n",
    "    \"Death_city\", \"Field\", \"Genre\", \"Marriage\", \"Art_institution\",  \"Teaching\", \"Awards\"]\n",
    "\n",
    "image_attributes = [\"Style\", \"Image_genre\", \"Color\", \"Tone\", \"Composition\"]\n",
    "# image_attributes = [\"Name\", \"Artwork_URL\", \"Style\", \"Genre\", \"Theme\", \"Object\", \"Color\", \n",
    "#     \"Tone\", \"Composition\", \"Regional_feature\", \"Art_movement\", \"Person_count\"]\n",
    "\n",
    "# 多值属性\n",
    "multi_value_attributes = [\"Genre\", \"Field\", \"Nationality\", \"Art_movement\"]\n",
    "\n",
    "# 属性分类\n",
    "non_numerical_attr = [\"Name\", \"Nationality\", \"Birth_date\", \"Death_date\", \n",
    "    \"Century\", \"Zodiac\", \"Birth_country\", \"Birth_city\", \"Birth_continent\", \"Death_country\", \n",
    "    \"Death_city\", \"Field\", \"Genre\", \"Marriage\", \"Art_institution\", \"Teaching\", \"Style\", \"Image_genre\", \"Color\", \"Tone\", \"Composition\"]\n",
    "\n",
    "# numerical_attr = [\"Person_count\", \"Age\", \"Awards\"]\n",
    "numerical_attr = [\"Age\", \"Awards\"]\n",
    "# non_formatted_attr = [\"Awards\", \"Person_count\", \"Age\"]\n",
    "non_formatted_attr = [\"Awards\", \"Age\"]\n",
    "formatted_attr = [\"Birth_date\", \"Death_date\"]\n",
    "category_attr = [\"Zodiac\", \"Birth_continent\", \"Marriage\", \"Century\", \"Marriage\", \"Teaching\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成统计信息\n",
    "- 属性 | 属性值 | 选择率 | 基数\n",
    "- 用于后续构造 Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = pd.DataFrame()\n",
    "\n",
    "for column in attr_desc_dict.keys():\n",
    "    if column in multi_value_attributes:\n",
    "        expanded_values = []\n",
    "        original_row_count = {} \n",
    "        \n",
    "        for idx, value in enumerate(df[column]):\n",
    "            if pd.isna(value):\n",
    "                expanded_values.append(None)\n",
    "                original_row_count[None] = original_row_count.get(None, 0) + 1\n",
    "            elif isinstance(value, str) and '||' in value:\n",
    "                split_values = [v.strip() for v in value.split('||') if v.strip()]\n",
    "                expanded_values.extend(split_values)\n",
    "                for split_val in split_values:\n",
    "                    original_row_count[split_val] = original_row_count.get(split_val, 0) + 1\n",
    "            else:\n",
    "                expanded_values.append(value)\n",
    "                original_row_count[value] = original_row_count.get(value, 0) + 1\n",
    "        \n",
    "        expanded_series = pd.Series(expanded_values)\n",
    "        value_counts = expanded_series.value_counts()\n",
    "        \n",
    "        selectivities = pd.Series({\n",
    "            k: round(v / len(df), 3) for k, v in original_row_count.items()\n",
    "        })\n",
    "        \n",
    "    else:\n",
    "        value_counts = df[column].value_counts()\n",
    "        selectivities = df[column].value_counts(normalize=True).round(3)\n",
    "    \n",
    "    null_count = df[column].isnull().sum()\n",
    "    \n",
    "    result_df = pd.DataFrame({\n",
    "        f\"{column}\": list(value_counts.index) + [\"(null)\"],\n",
    "        'Count': list(value_counts.values) + [null_count],\n",
    "        'Selectivity': [selectivities.get(val, 0) for val in value_counts.index] + [round(null_count / len(df), 3)]\n",
    "    })\n",
    "    \n",
    "    if statistics.empty:\n",
    "        statistics = result_df\n",
    "    else:\n",
    "        statistics = pd.concat([statistics, result_df], axis=1)\n",
    "\n",
    "statistics.to_csv(statistics_output_dir, index=False)\n",
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 汇总可取属性值，以便后续据此构造 Filter\n",
    "- ```attr_value_dict```：所有可取属性值的集合，每个属性有多个可取属性值，根据统计信息来定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_value_dict = {\n",
    "    \"Name\": [\"Oswald Achenbac\", \"Ben Shahn\", \"Silvestro Lega\", \"Samuel Finley Breese Morse\", \n",
    "             \"Ivan Grohar\", \"Oscar Agustín Alejandro Schulz Solari\", \"Thomas Girtin\"],\n",
    "    \"Nationality\": [\"American\", \"French\", \"British\", \"German\", \"Italian\", \"Japanese\", \n",
    "                    \"Russian\", \"Australia\", \"Canadian\", \"Brazilian\", \"Mexican\"],\n",
    "    \"Birth_date\": [\"1905/4/27\", \"1905/4/28\", \"1905/4/22\", \"1905/4/19\", \"1898/9/12\"],\n",
    "    \"Death_date\": [\"2005/4/22\", \"2011/4/8\", \"1943/1/13\", \"1986/1/1\", \"1963/4/9\"],\n",
    "    \"Age\": [83, 17, 76, 15, 82, 78, 70, 85, 9, 65, 66, 87],\n",
    "    \"Century\": [\"19th-20th\", \"20th\", \"20th-21st\", \"19th\"],\n",
    "    \"Zodiac\": [\"Aries\", \"Taurus\", \"Pisces\", \"Gemini\", \"Leo\", \"Aquarius\"],\n",
    "    \"Birth_country\": [\"United States\", \"France\", \"United Kingdom\", \"Germany\", \"Italy\"],\n",
    "    \"Birth_city\": [\"Paris\", \"London\", \"New York\", \"Philadelphia\", \"Chicago\"],\n",
    "    \"Birth_continent\": [\"Europe\", \"North America\", \"Asia\", \"South America\"],\n",
    "    \"Death_country\": [\"United States\", \"France\", \"United Kingdom\", \"Italy\"],\n",
    "    \"Death_city\": [\"Paris\", \"New York\", \"London\", \"New York City\"],\n",
    "    \"Field\": [\"Painting\", \"Sculpture\", \"Printmaking\", \"Illustration\", \"Photography\"],\n",
    "    \"Genre\": [\"Abstract\", \"Landscape\", \"Portrait\", \"Figurative\", \"Surrealism\"],\n",
    "    \"Marriage\": [\"Married\", \"Unmarried\", \"Remarried\", \"Divorced\", \"Widowed\"],\n",
    "    \"Art_institution\": [\"Royal Academy of Arts\", \"Self-taught\", \"Art Students League of New York\"],\n",
    "    \"Teaching\": [0, 1],\n",
    "    \"Awards\": [0, 1, 2, 3, 4, 5],\n",
    "    \"Style\": [\"Expressionism\", \"Romanticism\", \"Conceptual Art\", \"Abstract Expressionism\", \"Impressionism\", \"Realism\",\n",
    "              \"Surrealism\", \"Pop Art\", \"Art Nouveau (Modern)\", \"Minimalism\", \"Post-Impressionism\"],\n",
    "    \"Image_genre\": [\"Portrait\", \"Landscape\", \"Still Life\", \"Abstract\", \"Sculpture\", \"Figurative\", \"Genre Painting\"],\n",
    "    \"Color\": [\"Earth Tones\", \"Blue\", \"Green\", \"Red\", \"Black And White\", \"Brown\", \"Yellow\", \"Multicolored\", \"White\"],\n",
    "    \"Tone\":[\"Neutral\", \"Bright\", \"Dark\", \"Warm\", \"Soft\", \"Light\"],\n",
    "    \"Composition\": [\"Balanced\", \"Asymmetrical\", \"Centralized\", \"Dynamic\", \"Symmetrical\",\n",
    "                    \"Central Focus\", \"Geometric\", \"Centered\", \"Minimalist\", \"Geometric Arrangement\"],\t\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义查询构造数量\n",
    "- ```max_filters```：WHERE 语句最大 Filter 数量\n",
    "- ```min_rows```：结果表最少行数\n",
    "- ```max_select```：SELECT 语句最大属性数量\n",
    "- ```limit_list```：LIMIT 语句候选值\n",
    "- ```sample_sfw```：SELECT | FROM | WHERE 查询数量\n",
    "- ```sample_sfwt```：SELECT | FROM | WHERE | TOP-K 查询数量\n",
    "- ```sample_sfwg```：SELECT | FROM | WHERE | GROUP BY 查询数量\n",
    "- ```sample_sfwa```：SELECT | FROM | WHERE | AGGREGATION 查询数量\n",
    "- ```sample_sfwga```：SELECT | FROM | WHERE | GROUP BY | AGGREGATION 查询数量\n",
    "- ```sample_sfwgat```：SELECT | FROM | WHERE | GROUP BY | AGGREGATION | TOP-K 查询数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_filters = 5\n",
    "min_rows = 5\n",
    "max_select = 3\n",
    "limit_list = [1, 2, 5, 10, 20, 50]\n",
    "sample_sf = 10  \n",
    "sample_sfw = 20\n",
    "sample_sfwt = 20\n",
    "sample_sfwg = 20\n",
    "sample_sfwa = 20\n",
    "sample_sfga = 10\n",
    "sample_sfwga = 30\n",
    "sample_sfwgat = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义 Filter 具体执行方法\n",
    "- 不同类型的数据有不同的执行方法\n",
    "- 区分非数值型、数值型、日期型\n",
    "- 比较运算包括：大于（只限于数值型）、小于（只限于数值型）、等于（任意类型）\n",
    "- **注意**：需要提前确认好日期型数据的具体格式，做好特殊情况相应处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_numerical_equal_to(value, condition):\n",
    "     #print(f'value:{value}, condition:{condition}')\n",
    "     try:\n",
    "          value = str(value).lower().strip()\n",
    "          condition = str(condition).lower().strip()\n",
    "     except:\n",
    "          print(\"Invalid data (non_numerical_equal_to). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "     return value == condition\n",
    "\n",
    "def number_greater_than(value, condition):\n",
    "     if pd.isna(value):\n",
    "          value = 0.00\n",
    "     try:\n",
    "          value = round(float(value), 2)\n",
    "          condition = round(float(condition), 2)\n",
    "     except:\n",
    "          print(\"Invalid data (number_greater_than). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "     return value > condition\n",
    "\n",
    "def number_less_than(value, condition):\n",
    "     if pd.isna(value):\n",
    "          value = 0.00\n",
    "     try:\n",
    "          value = round(float(value), 2)\n",
    "          condition = round(float(condition), 2)\n",
    "     except:\n",
    "          print(\"Invalid data (number_less_than). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "     return value < condition\n",
    "\n",
    "def number_equal_to(value, condition):\n",
    "     if pd.isna(value):\n",
    "          value = 0.00\n",
    "     try:\n",
    "          value = round(float(value), 2)\n",
    "          condition = round(float(condition), 2)\n",
    "     except:\n",
    "          print(\"Invalid data (number_equal_to). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "     return value == condition\n",
    "\n",
    "def parse_date(date_str):\n",
    "     if pd.isna(date_str):\n",
    "          return None\n",
    "     for fmt in ['%Y/%m/%d', '%Y/%m', '%Y','%Y-%m-%d', '%Y-%m']:\n",
    "          try:\n",
    "               return datetime.strptime(date_str, fmt)\n",
    "          except ValueError:\n",
    "               continue\n",
    "     return None\n",
    "\n",
    "def date_greater_than(value, condition):\n",
    "     date_value = parse_date(value)\n",
    "     condition_date = parse_date(condition)\n",
    "     if date_value is None or condition_date is None:\n",
    "          return False\n",
    "     return date_value > condition_date\n",
    "\n",
    "def date_less_than(value, condition):\n",
    "     date_value = parse_date(value)\n",
    "     condition_date = parse_date(condition)\n",
    "     if date_value is None or condition_date is None:\n",
    "          return False\n",
    "     return date_value < condition_date\n",
    "\n",
    "def date_equal_to(value, condition):\n",
    "     date_value = parse_date(value)\n",
    "     condition_date = parse_date(condition)\n",
    "     if date_value is None or condition_date is None:\n",
    "          return False\n",
    "     return date_value == condition_date\n",
    "\n",
    "def parse_century(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        match = re.match(r\"(\\d+)(?:th|st|nd|rd)(?:-(\\d+)(?:th|st|nd|rd))?\", value)\n",
    "        if not match:\n",
    "            return None\n",
    "\n",
    "        start_century = int(match.group(1))\n",
    "        end_century = int(match.group(2)) if match.group(2) else start_century\n",
    "        return (start_century - 1) * 100, end_century * 100\n",
    "\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def century_greater_than(value, condition):\n",
    "    value_parsed = parse_century(value)\n",
    "    condition_parsed = parse_century(condition)\n",
    "\n",
    "    if value_parsed is None or condition_parsed is None:\n",
    "        return False\n",
    "\n",
    "    return value_parsed[0] > condition_parsed[1]\n",
    "\n",
    "def century_less_than(value, condition):\n",
    "    value_parsed = parse_century(value)\n",
    "    condition_parsed = parse_century(condition)\n",
    "\n",
    "    if value_parsed is None or condition_parsed is None:\n",
    "        return False\n",
    "\n",
    "    return value_parsed[0] < condition_parsed[1]\n",
    "\n",
    "def number_greater_equal(value, condition):\n",
    "    if pd.isna(value):\n",
    "        value = 0.00\n",
    "    try:\n",
    "        value = round(float(value), 2)\n",
    "        condition = round(float(condition), 2)\n",
    "    except:\n",
    "        print(\"Invalid data (number_greater_equal). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "    return value >= condition\n",
    "\n",
    "def number_less_equal(value, condition):\n",
    "    if pd.isna(value):\n",
    "        value = 0.00\n",
    "    try:\n",
    "        value = round(float(value), 2)\n",
    "        condition = round(float(condition), 2)\n",
    "    except:\n",
    "        print(\"Invalid data (number_less_equal). value:[%s] | condition: [%s].\" % (value, condition))\n",
    "    return value <= condition\n",
    "\n",
    "def date_greater_equal(value, condition):\n",
    "    date_value = parse_date(value)\n",
    "    condition_date = parse_date(condition)\n",
    "    if date_value is None or condition_date is None:\n",
    "        return False\n",
    "    return date_value >= condition_date\n",
    "\n",
    "def date_less_equal(value, condition):\n",
    "    date_value = parse_date(value)\n",
    "    condition_date = parse_date(condition)\n",
    "    if date_value is None or condition_date is None:\n",
    "        return False\n",
    "    return date_value <= condition_date\n",
    "\n",
    "def century_greater_equal(value, condition):\n",
    "    value_parsed = parse_century(value)\n",
    "    condition_parsed = parse_century(condition)\n",
    "    if value_parsed is None or condition_parsed is None:\n",
    "        return False\n",
    "    return value_parsed[0] >= condition_parsed[0]\n",
    "\n",
    "def century_less_equal(value, condition):\n",
    "    value_parsed = parse_century(value)\n",
    "    condition_parsed = parse_century(condition)\n",
    "    if value_parsed is None or condition_parsed is None:\n",
    "        return False\n",
    "    return value_parsed[1] <= condition_parsed[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有可能的 Filter\n",
    "- Filter 的可能取值来自于字典：```attr_value_dict```\n",
    "- 用一个列表记录满足 Filter 的行索引\n",
    "- 输入：```dataset_dir```，即原始标注数据表\n",
    "- 输出：```filter_dict.json```，包含各属性，每个属性中包含所有可能的过滤条件及对应满足条件的行索引号\n",
    "- 这一步构建好所有可能的 Filter，后面从此 Filter 集合中选取 Filter 进行排列组合，构造 WHERE 语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 增强的Filter构造函数 - 完全支持多值属性\n",
    "\n",
    "def enhanced_non_numerical_equal_to_with_split(cell, condition, multi_value_attrs, separator='||'):\n",
    "    try:\n",
    "        # 处理空值\n",
    "        if pd.isna(cell) or pd.isna(condition):\n",
    "            return pd.isna(cell) and pd.isna(condition)\n",
    "        \n",
    "        # 转换为字符串并清理\n",
    "        cell_str = str(cell).strip()\n",
    "        condition_str = str(condition).strip()\n",
    "        \n",
    "        # 检查是否包含分隔符（多值）\n",
    "        if separator in cell_str:\n",
    "            # 拆分多值并检查是否包含目标值\n",
    "            cell_values = [v.strip().lower() for v in cell_str.split(separator) if v.strip()]\n",
    "            condition_lower = condition_str.lower()\n",
    "            return condition_lower in cell_values\n",
    "        else:\n",
    "            # 单值比较\n",
    "            return cell_str.lower() == condition_str.lower()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"比较出错 - cell: [{cell}], condition: [{condition}], error: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_multi_value_statistics(df, column, multi_value_attrs, separator='||'):\n",
    "    \"\"\"\n",
    "    获取多值属性的详细统计信息\n",
    "    \n",
    "    Returns:\n",
    "    dict: 包含各种统计信息的字典\n",
    "    \"\"\"\n",
    "    if column not in multi_value_attrs:\n",
    "        return {\"is_multi_value\": False}\n",
    "    \n",
    "    # 分析多值情况\n",
    "    multi_value_rows = 0\n",
    "    total_values = 0\n",
    "    value_distribution = {}\n",
    "    \n",
    "    for cell_value in df[column].dropna():\n",
    "        if isinstance(cell_value, str) and separator in cell_value:\n",
    "            multi_value_rows += 1\n",
    "            values = [v.strip() for v in cell_value.split(separator) if v.strip()]\n",
    "            total_values += len(values)\n",
    "            \n",
    "            for val in values:\n",
    "                value_distribution[val] = value_distribution.get(val, 0) + 1\n",
    "        else:\n",
    "            total_values += 1\n",
    "            val = str(cell_value).strip()\n",
    "            value_distribution[val] = value_distribution.get(val, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        \"is_multi_value\": True,\n",
    "        \"multi_value_rows\": multi_value_rows,\n",
    "        \"total_rows\": len(df[column].dropna()),\n",
    "        \"total_expanded_values\": total_values,\n",
    "        \"unique_values\": len(value_distribution),\n",
    "        \"value_distribution\": value_distribution,\n",
    "        \"avg_values_per_row\": total_values / len(df[column].dropna()) if len(df[column].dropna()) > 0 else 0\n",
    "    }\n",
    "    \n",
    "# 重新读取数据确保一致性\n",
    "df = pd.read_csv(dataset_dir, encoding='ISO-8859-1')\n",
    "\n",
    "enhanced_filter_dict = {}\n",
    "\n",
    "for key in attr_value_dict.keys():\n",
    "    enhanced_filter_dict[key] = {}\n",
    "    print(f\"处理属性: {key}\")\n",
    "    \n",
    "    # 获取多值属性统计\n",
    "    multi_stats = get_multi_value_statistics(df, key, multi_value_attributes)\n",
    "    if multi_stats[\"is_multi_value\"]:\n",
    "        print(f\"  多值属性 - 唯一值: {multi_stats['unique_values']}, 平均每行值数: {multi_stats['avg_values_per_row']:.2f}\")\n",
    "\n",
    "for key, value in tqdm(enhanced_filter_dict.items(), desc=\"构建Filter\"):\n",
    "    condition_dict = {}\n",
    "\n",
    "    ###### 非数值属性，只取等于操作 ######\n",
    "    if key in non_numerical_attr and key not in formatted_attr:\n",
    "        for possible_value in attr_value_dict[key]:\n",
    "            # 使用增强的比较函数\n",
    "            def enhanced_equal_compare(cell):\n",
    "                return enhanced_non_numerical_equal_to_with_split(\n",
    "                    cell, possible_value, multi_value_attributes\n",
    "                )\n",
    "\n",
    "            result = df[key].apply(enhanced_equal_compare)\n",
    "            result_indices = df[result].index.tolist()\n",
    "            \n",
    "            # 为多值属性添加额外的信息\n",
    "            condition_key = f\"=='{possible_value}'\"\n",
    "            \n",
    "            condition_dict[condition_key] = result_indices\n",
    "            \n",
    "            # 记录统计信息\n",
    "            if len(result_indices) > 0:\n",
    "                selectivity = len(result_indices) / len(df)\n",
    "                print(f\"    {possible_value}: {len(result_indices)} 行 (选择率: {selectivity:.3f})\")\n",
    "\n",
    "    ###### 数值属性和日期属性的处理保持不变 ######\n",
    "    elif key in numerical_attr or key in formatted_attr:\n",
    "        for possible_value in attr_value_dict[key]:\n",
    "            \n",
    "            result_indices_less = []\n",
    "            result_indices_greater = []\n",
    "            result_indices_equal = []\n",
    "            result_indices_less_equal = []\n",
    "            result_indices_greater_equal = []\n",
    "            \n",
    "            # 日期属性处理\n",
    "            if key in formatted_attr:\n",
    "                if key == \"Birth_date\" or key == \"Death_date\":\n",
    "                    result_index_less = df[key].apply(date_less_than, condition=possible_value)\n",
    "                    result_index_greater = df[key].apply(date_greater_than, condition=possible_value)\n",
    "                    result_index_equal = df[key].apply(date_equal_to, condition=possible_value)\n",
    "                    result_index_less_equal = df[key].apply(date_less_equal, condition=possible_value)\n",
    "                    result_index_greater_equal = df[key].apply(date_greater_equal, condition=possible_value)\n",
    "                    \n",
    "                    result_indices_less = df[result_index_less].index.tolist()\n",
    "                    result_indices_greater = df[result_index_greater].index.tolist()\n",
    "                    result_indices_equal = df[result_index_equal].index.tolist()\n",
    "                    result_indices_less_equal = df[result_index_less_equal].index.tolist()\n",
    "                    result_indices_greater_equal = df[result_index_greater_equal].index.tolist()\n",
    "            \n",
    "            # 纯数值属性处理\n",
    "            elif key in non_formatted_attr:\n",
    "                result_index_less = df[key].apply(number_less_than, condition=possible_value)\n",
    "                result_index_greater = df[key].apply(number_greater_than, condition=possible_value)\n",
    "                result_index_equal = df[key].apply(number_equal_to, condition=possible_value)\n",
    "                result_index_less_equal = df[key].apply(number_less_equal, condition=possible_value)\n",
    "                result_index_greater_equal = df[key].apply(number_greater_equal, condition=possible_value)\n",
    "                \n",
    "                result_indices_less = df[result_index_less].index.tolist()\n",
    "                result_indices_greater = df[result_index_greater].index.tolist()\n",
    "                result_indices_equal = df[result_index_equal].index.tolist()\n",
    "                result_indices_less_equal = df[result_index_less_equal].index.tolist()\n",
    "                result_indices_greater_equal = df[result_index_greater_equal].index.tolist()\n",
    "\n",
    "            # 添加所有5种比较操作\n",
    "            condition_dict[f\"<{possible_value}\"] = result_indices_less\n",
    "            condition_dict[f\">{possible_value}\"] = result_indices_greater\n",
    "            condition_dict[f\"=={possible_value}\"] = result_indices_equal\n",
    "            condition_dict[f\"<={possible_value}\"] = result_indices_less_equal\n",
    "            condition_dict[f\">={possible_value}\"] = result_indices_greater_equal\n",
    "\n",
    "    ###### 特殊处理Century属性（保持原有逻辑）######\n",
    "    if key == \"Century\":\n",
    "        for possible_value in attr_value_dict[key]:\n",
    "            result_index_less = df[key].apply(century_less_than, condition=possible_value)\n",
    "            result_index_greater = df[key].apply(century_greater_than, condition=possible_value)\n",
    "            result_index_less_equal = df[key].apply(century_less_equal, condition=possible_value)\n",
    "            result_index_greater_equal = df[key].apply(century_greater_equal, condition=possible_value)\n",
    "            \n",
    "            def century_equal_to(value, condition):\n",
    "                if pd.isna(value) or pd.isna(condition):\n",
    "                    return False\n",
    "                return value == condition\n",
    "            \n",
    "            result_index_equal = df[key].apply(century_equal_to, condition=possible_value)\n",
    "            \n",
    "            result_indices_less = df[result_index_less].index.tolist()\n",
    "            result_indices_greater = df[result_index_greater].index.tolist()\n",
    "            result_indices_equal = df[result_index_equal].index.tolist()\n",
    "            result_indices_less_equal = df[result_index_less_equal].index.tolist()\n",
    "            result_indices_greater_equal = df[result_index_greater_equal].index.tolist()\n",
    "            \n",
    "            condition_dict[f\"<'{possible_value}'\"] = result_indices_less\n",
    "            condition_dict[f\">'{possible_value}'\"] = result_indices_greater\n",
    "            condition_dict[f\"=='{possible_value}'\"] = result_indices_equal\n",
    "            condition_dict[f\"<='{possible_value}'\"] = result_indices_less_equal\n",
    "            condition_dict[f\">='{possible_value}'\"] = result_indices_greater_equal\n",
    "    \n",
    "    enhanced_filter_dict[key] = condition_dict\n",
    "\n",
    "# 保存增强的Filter字典\n",
    "with open(\"./filter_dict.json\", 'w') as f:\n",
    "    json.dump(enhanced_filter_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"增强的Filter字典构建完成！\")\n",
    "\n",
    "print(f\"\\n文件已保存: ./filter_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前问题：\"Birth_date\": {}, \"Death_date\": {}, \"Influenced_by\": {}, \"Influenced_on\": {}, \"Teaching\": {}, \"Image_genre\": {}是空值\n",
    "\n",
    "其中，Influence_by，Influence_on，Artwork_URL这三个属性值缺失，要处理一下数值的问题\n",
    "\n",
    "需要处理的是：有||值，图片和文本属性要区分，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有 Filter 的可能排列组合，构造 WHERE 语句\n",
    "- ```balanced_sample()```：控制均匀采样 Filter\n",
    "     - 保证每个属性出现的次数是平衡的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 均匀采样函数 ######\n",
    "def balanced_sample(filters, sample_size=10, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "    col_to_filters = defaultdict(list)\n",
    "    for filter_item in filters:\n",
    "        col = filter_item[0]\n",
    "        col_to_filters[col].append(filter_item)\n",
    "    \n",
    "    unique_cols = list(col_to_filters.keys())\n",
    "    num_cols = len(unique_cols)\n",
    "    \n",
    "    if num_cols == 0:\n",
    "        return []\n",
    "    \n",
    "    base_num = sample_size // num_cols\n",
    "    remainder = sample_size % num_cols\n",
    "    \n",
    "    col_sample_counts = {col: base_num for col in unique_cols}\n",
    "\n",
    "    for col in random.sample(unique_cols, remainder):\n",
    "        col_sample_counts[col] += 1\n",
    "    \n",
    "    sampled_filters = []\n",
    "    overflow = 0\n",
    "    \n",
    "    for col in unique_cols:\n",
    "        available = len(col_to_filters[col])\n",
    "        required = col_sample_counts[col]\n",
    "        if available >= required:\n",
    "            sampled = random.sample(col_to_filters[col], required)\n",
    "            sampled_filters.extend(sampled)\n",
    "        else:\n",
    "            sampled = col_to_filters[col]\n",
    "            sampled_filters.extend(sampled)\n",
    "            overflow += (required - available)\n",
    "    \n",
    "    if overflow > 0:\n",
    "        remaining_cols = [col for col in unique_cols if len(col_to_filters[col]) > col_sample_counts[col]]\n",
    "        while overflow > 0 and remaining_cols:\n",
    "            for col in remaining_cols.copy():\n",
    "                available = len(col_to_filters[col])\n",
    "                current = col_sample_counts[col]\n",
    "                if available > current:\n",
    "                    sampled = random.sample(\n",
    "                        list(set(col_to_filters[col]) - set(sampled_filters)), 1\n",
    "                    )\n",
    "                    sampled_filters.extend(sampled)\n",
    "                    col_sample_counts[col] += 1\n",
    "                    overflow -= 1\n",
    "                    if overflow == 0:\n",
    "                        break\n",
    "                else:\n",
    "                    remaining_cols.remove(col)\n",
    "\n",
    "    return sampled_filters[:sample_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有 Filter 的可能排列组合，构造 WHERE 语句\n",
    "- 枚举所有可能的 Filter 排列组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./filter_dict.json\", 'r') as f:\n",
    "     filter_dict = json.load(f)\n",
    "\n",
    "filters = []\n",
    "for col, conditions in filter_dict.items():\n",
    "     for cond, indices in conditions.items():\n",
    "          filters.append((col, cond, set(indices)))\n",
    "\n",
    "# filters = random.sample(filters, 20)\n",
    "filters = balanced_sample(filters, sample_size=20, random_seed=42)\n",
    "\n",
    "all_combinations = []\n",
    "for n in tqdm(range(1, max_filters + 1)):\n",
    "     all_combinations.extend(itertools.permutations(filters, n))\n",
    "\n",
    "with open(\"./all_combinations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_combinations, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有 Filter 的可能排列组合，构造 WHERE 语句\n",
    "- ```select_combinations_with_ratio()```：按不同 Filter 数量的比例构造 WHERE 语句\n",
    "- 1 - 5 个 Filter 的比例为：2 : 3 : 3 : 1 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_combinations_with_ratio(all_combinations, total=1000, ratio=[2, 3, 3, 1, 1], min_len=1, max_len=5):\n",
    "\n",
    "    length_to_combinations = defaultdict(list)\n",
    "    for combo in all_combinations:\n",
    "        l = len(combo)\n",
    "        if min_len <= l <= max_len:\n",
    "            length_to_combinations[l].append(combo)\n",
    "\n",
    "    total_ratio = sum(ratio)\n",
    "    \n",
    "    samples_per_length = {}\n",
    "    remaining_total = total\n",
    "    \n",
    "    for i, l in enumerate(range(min_len, max_len + 1)):\n",
    "        expected_samples = int(total * ratio[i] / total_ratio)\n",
    "        available_samples = len(length_to_combinations[l])\n",
    "        if available_samples < expected_samples:\n",
    "            samples_per_length[l] = available_samples\n",
    "            remaining_total -= available_samples\n",
    "            ratio[i] = 0\n",
    "        else:\n",
    "            samples_per_length[l] = expected_samples\n",
    "            remaining_total -= expected_samples\n",
    "    \n",
    "    if remaining_total > 0:\n",
    "        for i, l in enumerate(range(min_len, max_len + 1)):\n",
    "            if ratio[i] > 0:\n",
    "                possible_to_add = len(length_to_combinations[l]) - samples_per_length[l]\n",
    "                if possible_to_add > 0:\n",
    "                    additional_samples = min(possible_to_add, remaining_total)\n",
    "                    samples_per_length[l] += additional_samples\n",
    "                    remaining_total -= additional_samples\n",
    "                if remaining_total == 0:\n",
    "                    break\n",
    "\n",
    "    if remaining_total != 0:\n",
    "        raise ValueError(f\"The number of combinations cannot meet the requirements, with {remaining_total} combinations remaining.\")\n",
    "    \n",
    "    selected = []\n",
    "    for l in range(min_len, max_len + 1):\n",
    "        available = length_to_combinations[l]\n",
    "        required = samples_per_length[l]\n",
    "        selected += random.sample(available, required)\n",
    "    \n",
    "    random.shuffle(selected)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 枚举所有 Filter 的可能排列组合，构造 WHERE 语句\n",
    "- 析取 + 合取 + 析取与合取混合\n",
    "- 去掉等效表达式\n",
    "- 每个查询的结果表至少包含 ```min_rows``` 行 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./all_combinations.pkl\", \"rb\") as f:\n",
    "     all_combinations = pickle.load(f)\n",
    "\n",
    "def normalize_expression(expression):\n",
    "\n",
    "    def sort_expression(expr):\n",
    "        if \" AND \" in expr:\n",
    "            parts = sorted(expr.strip(\"()\").split(\" AND \"))\n",
    "            return f\"({' AND '.join(parts)})\"\n",
    "        elif \" OR \" in expr:\n",
    "            parts = sorted(expr.strip(\"()\").split(\" OR \"))\n",
    "            return f\"({' OR '.join(parts)})\"\n",
    "        return expr\n",
    "    \n",
    "    if \" AND \" in expression or \" OR \" in expression:\n",
    "        return sort_expression(expression)\n",
    "    return expression\n",
    "\n",
    "valid_where = []\n",
    "seen_expressions = set()\n",
    "\n",
    "# sampled_combinations = random.sample(all_combinations, 1000)\n",
    "sampled_combinations = select_combinations_with_ratio(all_combinations, total=1000, ratio=[2, 3, 3, 1, 1], min_len=1, max_len=max_filters)\n",
    "\n",
    "for combo in tqdm(sampled_combinations):\n",
    "    condition_sets = [set(item[2]) for item in combo]\n",
    "    predicates = [f\"{col}{cond}\" for col, cond, _ in combo]\n",
    "\n",
    "    for op in itertools.product([\"and\", \"or\"], repeat=len(condition_sets) - 1):\n",
    "    # for op in itertools.product([\"and\"], repeat=len(condition_sets) - 1):\n",
    "\n",
    "        current_sets = [condition_sets[0]]\n",
    "        current_predicates = [predicates[0]]\n",
    "        \n",
    "        for i, logic in enumerate(op):\n",
    "            if logic == \"and\":\n",
    "                current_sets[-1] &= condition_sets[i + 1]\n",
    "                current_predicates[-1] = f\"({current_predicates[-1]} AND {predicates[i + 1]})\"\n",
    "            elif logic == \"or\":\n",
    "                current_sets.append(condition_sets[i + 1])\n",
    "                current_predicates.append(predicates[i + 1])\n",
    "        \n",
    "        final_result = set().union(*current_sets)\n",
    "        if len(final_result) >= min_rows:\n",
    "            expression = \" OR \".join(current_predicates)\n",
    "\n",
    "            normalized_expression = normalize_expression(expression)\n",
    "            if normalized_expression not in seen_expressions:\n",
    "                seen_expressions.add(normalized_expression)\n",
    "\n",
    "                combo_list = [[c[0], c[1]] for c in combo]\n",
    "                query_dict = {\n",
    "                    \"WHERE Indices\": list(final_result),\n",
    "                    \"WHERE Total Rows\": len(final_result),\n",
    "                    \"Combination\": combo_list,\n",
    "                    \"Operators\": list(op),\n",
    "                    \"WHERE\": expression\n",
    "                }\n",
    "                valid_where.append(query_dict)\n",
    "                # if len(valid_where) > 1000:\n",
    "                #         with open(\"/data/sunzhaoze/benchmark/valid_WHERE.json\", 'a') as f:\n",
    "                #             json.dump(valid_where, f, ensure_ascii=False, indent=4, separators=(',', ': '))\n",
    "                #         valid_where.clear()\n",
    "\n",
    "    if valid_where:\n",
    "        with open(valid_where_output_dir, 'w') as f:\n",
    "            json.dump(valid_where, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义表头 SCHEMA\n",
    "- ```create_schema()```：SCHEMA 定义函数，数据共三种类型：VARCHAR(255) | DATE | FLOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema(attr_desc_dict, query_attr_list):\n",
    "     schema = {}\n",
    "     difference = set(attr_desc_dict.keys()) - set(query_attr_list)\n",
    "     redundant_attr_list = random.sample(list(difference), random.randint(0, 4))\n",
    "     schema_attr_list = query_attr_list + redundant_attr_list\n",
    "     for key in schema_attr_list:\n",
    "          if key in non_numerical_attr:\n",
    "               schema[key] = [\"VARCHAR(255)\", attr_desc_dict[key]]\n",
    "          elif key in formatted_attr:\n",
    "               schema[key] = [\"DATE\", attr_desc_dict[key]]\n",
    "          elif key in non_formatted_attr:\n",
    "               schema[key] = [\"FLOAT\", attr_desc_dict[key]]\n",
    "     return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### json 自定义格式化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_json_dump(data, filename):\n",
    "    json_str = json.dumps(data, ensure_ascii=False)\n",
    "    json_str = json_str.replace(\", {\", \",\\n{\").replace(\"{\\\"\", \"{\\n\\t\\\"\").replace(\"]},\\n\", \"]\\n},\\n\").replace(\"e},\\n\", \"e\\n},\\n\").replace(\"}},\\n\", \"}\\n},\\n\")\n",
    "    json_str = re.sub(r',\\s*\"([^\"]+)\":', r', \\n\\t\"\\1\":', json_str)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sf_queries():\n",
    "    valid_sf = []\n",
    "    \n",
    "    for i in tqdm(range(sample_sf)):\n",
    "        # 随机选择1到max_select个列\n",
    "        selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_select))\n",
    "        \n",
    "        # 创建查询字典\n",
    "        query_dict = {\n",
    "            \"Type\": \"SF\",\n",
    "            \"SELECT\": selected_columns,\n",
    "            \"WHERE Indices\": list(range(len(df))),  # 包含所有行\n",
    "            \"WHERE Total Rows\": len(df),            # 总行数\n",
    "            \"Combination\": [],                      # 没有Filter组合\n",
    "            \"Operators\": [],                        # 没有操作符\n",
    "            \"WHERE\": \"None\"                         # 没有WHERE条件\n",
    "        }\n",
    "        \n",
    "        # 创建SCHEMA\n",
    "        query_attr_list = selected_columns.copy()\n",
    "        query_dict[\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "        \n",
    "        valid_sf.append(query_dict)\n",
    "    \n",
    "    return valid_sf\n",
    "valid_sf = create_sf_queries()\n",
    "custom_json_dump(valid_sf, \"./SELECT_FROM.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_sfw = json.load(f)\n",
    "\n",
    "for i in tqdm(range(0, len(valid_sfw))):\n",
    "    selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_select))\n",
    "    valid_sfw[i][\"Type\"] = \"SFW\"\n",
    "    valid_sfw[i][\"SELECT\"] = selected_columns\n",
    "    query_attr_list = [i for i in selected_columns]\n",
    "    for k in valid_sfw[i][\"Combination\"]:\n",
    "         if k[0] not in query_attr_list:\n",
    "             query_attr_list += [k[0]]\n",
    "    valid_sfw[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "\n",
    "sampled_valid_sfw = random.sample(valid_sfw, sample_sfw)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE.json\", 'w') as f:\n",
    "#      json.dump(sampled_valid_sfw, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfw, \"./SELECT_FROM_WHERE.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | GROUP BY\n",
    "- 返回多个表，每个组一个表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_where = json.load(f)\n",
    "\n",
    "valid_sfwg = []\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "     # while True:\n",
    "     #      selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_groupby))\n",
    "     #      if not (set(selected_columns) & set(formatted_attr)):\n",
    "     #           break\n",
    "     selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_select))\n",
    "     valid_groupby_attrs = [attr for attr in category_attr if attr not in multi_value_attributes]\n",
    "     groupby_columns = random.sample(valid_groupby_attrs, 1)\n",
    "     row_indices = valid_where[i].get(\"WHERE Indices\", [])\n",
    "     # 过滤掉无效的索引\n",
    "     row_indices = [idx for idx in row_indices if idx < len(df)]\n",
    "     if not row_indices:  # 如果没有有效索引，跳过\n",
    "          continue\n",
    "     filtered_df = df.loc[row_indices]\n",
    "     grouped = filtered_df.groupby(groupby_columns)\n",
    "     remaining_rows = grouped.size().reset_index()\n",
    "     if len(remaining_rows) > (min_rows // 2):\n",
    "          valid_where[i][\"Type\"] = \"SFWG\"\n",
    "          valid_where[i][\"SELECT\"] = selected_columns + groupby_columns if groupby_columns not in selected_columns else selected_columns\n",
    "          valid_where[i][\"GROUP BY Total Rows (Groups)\"] = len(remaining_rows)\n",
    "          valid_where[i][\"GROUP BY\"] = groupby_columns\n",
    "          query_attr_list = [i for i in groupby_columns]\n",
    "          for k in valid_where[i][\"Combination\"]:\n",
    "               if k[0] not in query_attr_list:\n",
    "                    query_attr_list += [k[0]]\n",
    "          valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "          valid_sfwg.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwg = random.sample(valid_sfwg, sample_sfwg)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_GROUPBY.json\", 'w') as f:\n",
    "#      json.dump(sampled_valid_sfwg, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwg, \"./SELECT_FROM_WHERE_GROUPBY.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | AGGREGATION\n",
    "- 支持的聚合函数：SUM | MAX | MIN | AVG | COUNT\n",
    "- TODO：加入语义聚合\n",
    "- 只在一个属性上进行聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_where = json.load(f)\n",
    "\n",
    "valid_sfwa = []\n",
    "aggregation_functions = ['COUNT', 'MAX', 'MIN', 'AVG', 'SUM']\n",
    "max_COUNT = len(valid_where) // 4\n",
    "\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "     numerical = False\n",
    "     while True:\n",
    "          if max_COUNT > 0:\n",
    "               selected_aggregation_column = random.sample(list(attr_value_dict.keys()), 1)\n",
    "               if selected_aggregation_column not in formatted_attr:\n",
    "                    break\n",
    "          else:\n",
    "               selected_aggregation_column = random.sample(list(non_formatted_attr), 1)\n",
    "               break\n",
    "          \n",
    "     if selected_aggregation_column[0] in non_formatted_attr:\n",
    "          function = random.choice(aggregation_functions[1:])\n",
    "          numerical = True\n",
    "     else:\n",
    "          max_COUNT -= 1\n",
    "          function = 'COUNT'\n",
    "     if function == \"COUNT\" and random.uniform(0, 1) > 0.5:\n",
    "          selected_aggregation_column = [\"*\"]\n",
    "     valid_where[i][\"Type\"] = \"SFWA\"\n",
    "     valid_where[i][\"SELECT\"] = [f\"{function}({selected_aggregation_column[0]})\"]\n",
    "     query_attr_list = [i for i in selected_aggregation_column]\n",
    "     for k in valid_where[i][\"Combination\"]:\n",
    "          if k[0] not in query_attr_list:\n",
    "               query_attr_list += [k[0]]\n",
    "     valid_where[i][\"AGGREGATION\"] = selected_aggregation_column\n",
    "     valid_where[i][\"AGGREGATION Function\"] = function\n",
    "     valid_where[i][\"Numerical\"] = numerical\n",
    "     valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "     valid_sfwa.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwa = random.sample(valid_sfwa, sample_sfwa)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_AGGREGATION.json\", 'w') as f:\n",
    "#      json.dump(sampled_valid_sfwa, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwa, \"./SELECT_FROM_WHERE_AGGREGATION.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | GROUP BY | AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sfag = []\n",
    "aggregation_functions = ['COUNT', 'MAX', 'MIN', 'AVG', 'SUM']\n",
    "sample_sfag = 20  # 定义生成数量\n",
    "max_COUNT = sample_sfag // 4\n",
    "\n",
    "numerical_columns = numerical_attr \n",
    "\n",
    "for i in tqdm(range(sample_sfag)):\n",
    "    # 选择分组列（排除多值属性）\n",
    "    valid_groupby_attrs = [attr for attr in category_attr if attr not in multi_value_attributes]\n",
    "    selected_groupby_columns = random.sample(valid_groupby_attrs, 1)\n",
    "    \n",
    "    # 选择聚合函数和列\n",
    "    if max_COUNT > 0:\n",
    "        # 随机决定是否使用COUNT函数\n",
    "        if random.uniform(0, 1) < 0.25:  # 25%概率使用COUNT\n",
    "            max_COUNT -= 1\n",
    "            function = 'COUNT'\n",
    "            numerical = False\n",
    "            # COUNT可以应用于任何列\n",
    "            available_columns = list(set(attr_value_dict.keys()) - set(selected_groupby_columns))\n",
    "            selected_aggregation_column = random.sample(available_columns, 1)\n",
    "        else:\n",
    "            # 使用数值型聚合函数，只从数值型列中选择\n",
    "            function = random.choice(aggregation_functions[1:])  # 排除COUNT\n",
    "            numerical = True\n",
    "            available_numerical_columns = list(set(numerical_columns) - set(selected_groupby_columns))\n",
    "            if available_numerical_columns:\n",
    "                selected_aggregation_column = random.sample(available_numerical_columns, 1)\n",
    "            else:\n",
    "                function = 'COUNT'\n",
    "                numerical = False\n",
    "                available_columns = list(set(attr_value_dict.keys()) - set(selected_groupby_columns))\n",
    "                selected_aggregation_column = random.sample(available_columns, 1)\n",
    "    else:\n",
    "        function = random.choice(aggregation_functions[1:])  # 排除COUNT\n",
    "        numerical = True\n",
    "        # 确保选择的列是数值型的且不是分组列\n",
    "        available_numerical_columns = list(set(numerical_columns) - set(selected_groupby_columns))\n",
    "        if available_numerical_columns:\n",
    "            selected_aggregation_column = random.sample(available_numerical_columns, 1)\n",
    "        else:\n",
    "            # 如果没有可用的数值型列，跳过此次迭代\n",
    "            continue\n",
    "    \n",
    "    # COUNT函数可以使用*\n",
    "    if function == \"COUNT\" and random.uniform(0, 1) > 0.5:\n",
    "        selected_aggregation_column = [\"*\"]\n",
    "    \n",
    "    query_dict = {\n",
    "        \"Type\": \"SFAG\",\n",
    "        \"GROUP BY\": selected_groupby_columns,\n",
    "        \"SELECT\": selected_groupby_columns + [f\"{function}({selected_aggregation_column[0]})\"],\n",
    "        \"WHERE Indices\": list(range(len(df))),  # 包含所有行\n",
    "        \"WHERE Total Rows\": len(df),            # 总行数\n",
    "        \"Combination\": [],                      # 没有Filter组合\n",
    "        \"Operators\": [],                        # 没有操作符\n",
    "        \"WHERE\": \"None\",                        # 没有WHERE条件\n",
    "        \"AGGREGATION\": selected_aggregation_column,\n",
    "        \"AGGREGATION Function\": \"AVG\" if function == \"MEAN\" else function,\n",
    "        \"Numerical\": numerical\n",
    "    }\n",
    "    \n",
    "    # 创建SCHEMA\n",
    "    query_attr_list = selected_aggregation_column + selected_groupby_columns\n",
    "    query_dict[\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "    \n",
    "    valid_sfag.append(query_dict)\n",
    "\n",
    "custom_json_dump(valid_sfag, \"./SELECT_FROM_AGGREGATION_GROUPBY.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | GROUP BY | AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_column_to_float(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(pd.to_numeric, errors='coerce')\n",
    "    return df\n",
    "\n",
    "for i in non_formatted_attr:\n",
    "    convert_column_to_float(df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_where = json.load(f)\n",
    "\n",
    "valid_sfwga = []\n",
    "aggregation_functions = ['COUNT', 'MAX', 'MIN', 'AVG', 'SUM']\n",
    "max_COUNT = sample_sfwga // 4\n",
    "\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "     row_indices = valid_where[i].get(\"WHERE Indices\", [])\n",
    "     # 过滤掉无效的索引\n",
    "     row_indices = [idx for idx in row_indices if idx < len(df)]\n",
    "     if not row_indices:  # 如果没有有效索引，跳过\n",
    "          continue\n",
    "     filtered_df = df.loc[row_indices]\n",
    "     # while True:\n",
    "     #      selected_groupby_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_groupby))\n",
    "     #      if not set(selected_groupby_columns) & set(formatted_attr):\n",
    "     #           break\n",
    "     # 排除多值属性\n",
    "     valid_groupby_attrs = [attr for attr in category_attr if attr not in multi_value_attributes]\n",
    "     selected_groupby_columns = random.sample(valid_groupby_attrs, 1)\n",
    "     grouped = filtered_df.groupby(selected_groupby_columns)\n",
    "     remaining_rows = grouped.size().reset_index(name='Group Size')\n",
    "\n",
    "     if len(remaining_rows) > (min_rows // 2):\n",
    "          while True:\n",
    "               if max_COUNT > 0:\n",
    "                    selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "                    if selected_aggregation_column not in formatted_attr:\n",
    "                         break\n",
    "               else:\n",
    "                    selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "                    break\n",
    "               \n",
    "          if selected_aggregation_column[0] in non_formatted_attr:\n",
    "               function = random.choice(aggregation_functions[1:])\n",
    "               numerical = True\n",
    "          else:\n",
    "               max_COUNT -= 1\n",
    "               function = 'COUNT'\n",
    "               numerical = False\n",
    "          \n",
    "          if function == \"COUNT\" and random.uniform(0, 1) > 0.5:\n",
    "               selected_aggregation_column = [\"*\"]\n",
    "          pandas_func = 'mean' if function == 'AVG' else function.lower()\n",
    "          agg_result = grouped[selected_aggregation_column].agg(pandas_func) if numerical else grouped.size()\n",
    "          valid_where[i][\"Type\"] = \"SFWGA\"\n",
    "          valid_where[i][\"GROUP BY\"] = selected_groupby_columns\n",
    "          valid_where[i][\"GROUP BY Total Rows\"] = len(remaining_rows)\n",
    "          valid_where[i][\"SELECT\"] = selected_groupby_columns + [f\"{function}({selected_aggregation_column[0]})\"]\n",
    "\n",
    "          valid_where[i][\"AGGREGATION\"] = selected_aggregation_column\n",
    "          if function == \"MEAN\":\n",
    "               valid_where[i][\"AGGREGATION Function\"] = \"AVG\"\n",
    "          else:\n",
    "               valid_where[i][\"AGGREGATION Function\"] = function\n",
    "          valid_where[i][\"Numerical\"] = numerical\n",
    "          query_attr_list = [i for i in selected_aggregation_column]\n",
    "          for k in valid_where[i][\"Combination\"]:\n",
    "               if k[0] not in query_attr_list:\n",
    "                    query_attr_list += [k[0]]\n",
    "          for k in selected_groupby_columns:\n",
    "               if k not in query_attr_list:\n",
    "                    query_attr_list += [k]\n",
    "          valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "          valid_sfwga.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwga = random.sample(valid_sfwga, sample_sfwga)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION.json\", 'w') as f:\n",
    "#      json.dump(sampled_valid_sfwga, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwga, \"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | TOP-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "    valid_where = json.load(f)\n",
    "\n",
    "valid_sfwt = []\n",
    "order_options = ['ASC', 'DESC']\n",
    "\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "    row_indices = valid_where[i].get(\"WHERE Indices\", []) # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices] # 过滤掉无效的索引 row_indices = [idx for idx in row_indices if idx < len(df)] if not row_indices:  # 如果没有有效索引，跳过     continue filtered_df = df.loc[row_indices]\n",
    "    # 过滤掉无效的索引\n",
    "    row_indices = [idx for idx in row_indices if idx < len(df)]\n",
    "    if not row_indices:  # 如果没有有效索引，跳过\n",
    "        continue\n",
    "    filtered_df = df.loc[row_indices]\n",
    "    while True:\n",
    "        selected_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_select))\n",
    "        if set(selected_columns) & set(non_formatted_attr):\n",
    "            order_column = random.choice(list(set(selected_columns) & set(non_formatted_attr)))\n",
    "            break\n",
    "\n",
    "    order_type = random.choice(order_options)\n",
    "\n",
    "    limit_value = min(random.sample(limit_list, 1)[0], len(filtered_df) // 2)\n",
    "    valid_where[i][\"Type\"] = \"SFWT\"\n",
    "    valid_where[i][\"SELECT\"] = selected_columns\n",
    "    valid_where[i][\"LIMIT\"] = limit_value\n",
    "    valid_where[i][\"ORDER BY\"] = [order_column, order_type]\n",
    "    query_attr_list = [i for i in selected_columns]\n",
    "    for k in valid_where[i][\"Combination\"]:\n",
    "         if k[0] not in query_attr_list:\n",
    "             query_attr_list += [k[0]]\n",
    "    valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "\n",
    "    valid_sfwt.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwt = random.sample(valid_sfwt, sample_sfwt)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_TOPK.json\", 'w') as f:\n",
    "#     json.dump(sampled_valid_sfwt, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwt, \"./SELECT_FROM_WHERE_TOPK.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 SELECT | FROM | WHERE | GROUP BY | AGGREGATION | TOP-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_where_output_dir, 'r') as f:\n",
    "     valid_where = json.load(f)\n",
    "\n",
    "valid_sfwgat = []\n",
    "aggregation_functions = ['COUNT', 'MAX', 'MIN', 'AVG', 'SUM']\n",
    "order_options = ['ASC', 'DESC']\n",
    "max_COUNT = sample_sfwgat // 4\n",
    "\n",
    "for i in tqdm(range(0, len(valid_where))):\n",
    "     row_indices = valid_where[i].get(\"WHERE Indices\", [])\n",
    "     # 过滤掉无效的索引\n",
    "     row_indices = [idx for idx in row_indices if idx < len(df)]\n",
    "     if not row_indices:  # 如果没有有效索引，跳过\n",
    "          continue\n",
    "     filtered_df = df.loc[row_indices]\n",
    "\n",
    "     # while True:\n",
    "     #      selected_groupby_columns = random.sample(list(attr_value_dict.keys()), random.randint(1, max_groupby))\n",
    "     #      if not set(selected_groupby_columns) & set(formatted_attr):\n",
    "     #           break\n",
    "     valid_groupby_attrs = [attr for attr in category_attr if attr not in multi_value_attributes]\n",
    "     selected_groupby_columns = random.sample(valid_groupby_attrs, 1)\n",
    "     grouped = filtered_df.groupby(selected_groupby_columns)\n",
    "     remaining_rows = grouped.size().reset_index(name='Group Size')\n",
    "\n",
    "     if len(remaining_rows) > (min_rows // 2):\n",
    "          while True:\n",
    "               if max_COUNT > 0:\n",
    "                    selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "                    if selected_aggregation_column not in formatted_attr:\n",
    "                         break\n",
    "               else:\n",
    "                    selected_aggregation_column = random.sample(list(set(attr_value_dict.keys())-set(selected_groupby_columns)), 1)\n",
    "                    break\n",
    "          if selected_aggregation_column[0] in non_formatted_attr:\n",
    "               function = random.choice(aggregation_functions[1:])\n",
    "               numerical = True\n",
    "          else:\n",
    "               max_COUNT -= 1\n",
    "               function = 'COUNT'\n",
    "               numerical = False\n",
    "\n",
    "          if function == \"COUNT\" and random.uniform(0, 1) > 0.5:\n",
    "               selected_aggregation_column = [\"*\"]\n",
    "\n",
    "          if selected_aggregation_column == [\"*\"]:\n",
    "               agg_result = grouped.transform(lambda x: x.notna().sum())\n",
    "          else:\n",
    "               agg_result = grouped[selected_aggregation_column[0]].transform(lambda x: x.notna().sum())\n",
    "\n",
    "          # agg_result = grouped[selected_aggregation_column].agg(function.lower()) if numerical else grouped.size()\n",
    "          select_set = set(selected_groupby_columns).union(set(selected_aggregation_column))\n",
    "          order_set = select_set & set(non_formatted_attr)\n",
    "          if not order_set:\n",
    "               continue\n",
    "          order_column = random.choice(list(order_set))\n",
    "          order_type = random.choice(order_options)\n",
    "          limit_value = min(random.sample(limit_list, 1)[0], len(agg_result // 2))\n",
    "          valid_where[i][\"Type\"] = \"SFWGAT\"\n",
    "          valid_where[i][\"GROUP BY\"] = selected_groupby_columns\n",
    "          valid_where[i][\"GROUP BY Total Rows\"] = len(remaining_rows)\n",
    "          valid_where[i][\"SELECT\"] = selected_groupby_columns + [f\"{function}({selected_aggregation_column[0]})\"]\n",
    "          valid_where[i][\"AGGREGATION\"] = selected_aggregation_column\n",
    "          if function == \"MEAN\":\n",
    "               valid_where[i][\"AGGREGATION Function\"] = \"AVG\"\n",
    "          else:\n",
    "               valid_where[i][\"AGGREGATION Function\"] = function\n",
    "          valid_where[i][\"Numerical\"] = numerical\n",
    "          valid_where[i][\"LIMIT\"] = limit_value\n",
    "          valid_where[i][\"ORDER BY\"] = [order_column, order_type]\n",
    "          query_attr_list = [i for i in selected_aggregation_column]\n",
    "          for k in valid_where[i][\"Combination\"]:\n",
    "               if k[0] not in query_attr_list:\n",
    "                    query_attr_list += [k[0]]\n",
    "          for k in selected_groupby_columns:\n",
    "               if k not in query_attr_list:\n",
    "                    query_attr_list += [k]\n",
    "          valid_where[i][\"SCHEMA\"] = create_schema(attr_desc_dict, query_attr_list)\n",
    "                    \n",
    "          valid_sfwgat.append(valid_where[i])\n",
    "\n",
    "sampled_valid_sfwgat = random.sample(valid_sfwgat, sample_sfwgat)\n",
    "\n",
    "# with open(\"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION_TOPK.json\", 'w') as f:\n",
    "#     json.dump(sampled_valid_sfwgat, f, ensure_ascii=False)\n",
    "custom_json_dump(sampled_valid_sfwgat, \"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION_TOPK.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#### 根据构造的查询结构生成对应的SQL语句\n",
    "def generate_sql_query(query_dict, table_name=\"Wikiart\"):\n",
    "    \"\"\"\n",
    "    根据查询字典生成对应的SQL语句（支持SF类型）\n",
    "    \"\"\"\n",
    "    query_type = query_dict.get(\"Type\", \"\")\n",
    "    \n",
    "    # 构建SELECT子句\n",
    "    select_clause = \"SELECT \" + \", \".join(query_dict[\"SELECT\"])\n",
    "    \n",
    "    # 构建FROM子句\n",
    "    from_clause = f\"FROM {table_name}\"\n",
    "    \n",
    "    # 构建WHERE子句（SF类型没有WHERE）\n",
    "    where_clause = \"\"\n",
    "    if \"WHERE\" in query_dict and query_dict[\"WHERE\"] != \"None\":\n",
    "        where_clause = f\"WHERE {query_dict['WHERE']}\"\n",
    "    \n",
    "    # 构建GROUP BY子句\n",
    "    group_by_clause = \"\"\n",
    "    if \"GROUP BY\" in query_dict:\n",
    "        group_by_clause = f\"GROUP BY {', '.join(query_dict['GROUP BY'])}\"\n",
    "    \n",
    "    # 构建ORDER BY子句\n",
    "    order_by_clause = \"\"\n",
    "    if \"ORDER BY\" in query_dict:\n",
    "        order_col, order_type = query_dict[\"ORDER BY\"]\n",
    "        order_by_clause = f\"ORDER BY {order_col} {order_type}\"\n",
    "    \n",
    "    # 构建LIMIT子句\n",
    "    limit_clause = \"\"\n",
    "    if \"LIMIT\" in query_dict:\n",
    "        limit_clause = f\"LIMIT {query_dict['LIMIT']}\"\n",
    "    \n",
    "    # 组合完整SQL\n",
    "    sql_parts = [select_clause, from_clause]\n",
    "    if where_clause:\n",
    "        sql_parts.append(where_clause)\n",
    "    if group_by_clause:\n",
    "        sql_parts.append(group_by_clause)\n",
    "    if order_by_clause:\n",
    "        sql_parts.append(order_by_clause)\n",
    "    if limit_clause:\n",
    "        sql_parts.append(limit_clause)\n",
    "    \n",
    "    return \"\\n\".join(sql_parts) + \";\"\n",
    "\n",
    "def generate_schema_sql(schema_dict, table_name=\"Wikiart\"):\n",
    "    \"\"\"\n",
    "    根据SCHEMA字典生成建表SQL语句\n",
    "    \"\"\"\n",
    "    create_table = f\"CREATE TABLE {table_name} (\\n\"\n",
    "    columns = []\n",
    "    \n",
    "    for col_name, (data_type, description) in schema_dict.items():\n",
    "        comment = f\" COMMENT '{description}'\" if description else \"\"\n",
    "        columns.append(f\"    {col_name} {data_type}{comment}\")\n",
    "    \n",
    "    create_table += \",\\n\".join(columns)\n",
    "    create_table += \"\\n);\"\n",
    "    \n",
    "    return create_table\n",
    "\n",
    "def save_queries_with_sql(input_files, output_dir=\"./sql_queries/\"):\n",
    "    \"\"\"\n",
    "    读取所有生成的查询文件，为每个查询生成对应的SQL语句并保存（支持SF类型）\n",
    "    修正：使用正确的换行符格式\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for file_path in input_files:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"文件不存在: {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            queries = json.load(f)\n",
    "        \n",
    "        # 为每个查询生成SQL\n",
    "        for i, query in enumerate(queries):\n",
    "            # 生成建表SQL\n",
    "            schema_sql = generate_schema_sql(query[\"SCHEMA\"])\n",
    "            \n",
    "            # 生成查询SQL\n",
    "            query_sql = generate_sql_query(query)\n",
    "            \n",
    "            # 计算SELECT数量\n",
    "            select_count = len(query.get(\"SELECT\", []))\n",
    "            \n",
    "            # 计算Filter数量 - 使用Combination字段获取最准确的数量\n",
    "            filter_count = len(query.get(\"Combination\", []))\n",
    "            \n",
    "            # 组合完整SQL - 修正：使用正确的换行符 \\n 而不是 \\\\n\n",
    "            complete_sql = f\"-- Query {i+1} ({query['Type']})\\n\"\n",
    "            complete_sql += f\"-- Total Rows: {query.get('WHERE Total Rows', 'N/A')}\\n\"\n",
    "            complete_sql += f\"-- SELECT: {select_count}\\n\"\n",
    "            complete_sql += f\"-- FILTER: {filter_count}\\n\\n\"\n",
    "            complete_sql += schema_sql + \"\\n\\n\"\n",
    "            complete_sql += query_sql + \"\\n\"\n",
    "            complete_sql += \"-\" * 50 + \"\\n\\n\"\n",
    "            \n",
    "            query[\"SQL\"] = {\n",
    "                \"schema\": schema_sql,\n",
    "                \"query\": query_sql,\n",
    "                \"complete\": complete_sql\n",
    "            }\n",
    "        \n",
    "        # 保存包含SQL的查询文件\n",
    "        output_file = output_dir + os.path.basename(file_path).replace('.json', '_with_sql.json')\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(queries, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # 单独保存纯SQL文件\n",
    "        sql_file = output_dir + os.path.basename(file_path).replace('.json', '.sql')\n",
    "        with open(sql_file, 'w', encoding='utf-8') as f:\n",
    "            for query in queries:\n",
    "                f.write(query[\"SQL\"][\"complete\"])\n",
    "        \n",
    "        print(f\"已生成SQL文件: {sql_file}\")\n",
    "\n",
    "# 使用示例：为所有查询类型生成SQL\n",
    "query_files = [\n",
    "    \"./SELECT_FROM.json\",\n",
    "    \"./SELECT_FROM_WHERE.json\",\n",
    "    \"./SELECT_FROM_WHERE_TOPK.json\", \n",
    "    \"./SELECT_FROM_WHERE_GROUPBY.json\",\n",
    "    \"./SELECT_FROM_WHERE_AGGREGATION.json\",\n",
    "    \"./SELECT_FROM_AGGREGATION_GROUPBY.json\",\n",
    "    \"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION.json\",\n",
    "    \"./SELECT_FROM_WHERE_GROUPBY_AGGREGATION_TOPK.json\"\n",
    "]\n",
    "\n",
    "# 生成所有SQL查询\n",
    "save_queries_with_sql(query_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固定Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SQL文件生成器 - 图文结合的Filter模板版本 (修复换行符)\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 改进的语义模板定义 - 结合图片和文字属性\n",
    "TEMPLATES = [\n",
    "    # 1个Filter (2个类别) - 可以是纯文字或纯图片属性\n",
    "    {\n",
    "        \"name\": \"nationality_focus\", \n",
    "        \"description\": \"特定国籍艺术家分析\", \n",
    "        \"filters\": [\"Nationality\"],\n",
    "        \"use_case\": \"研究某个国家的艺术家特征和创作倾向\",\n",
    "        \"semantic_columns\": [\"Nationality\", \"Name\", \"Age\", \"Field\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"style_focus\", \n",
    "        \"description\": \"特定艺术风格研究\", \n",
    "        \"filters\": [\"Style\"],\n",
    "        \"use_case\": \"分析特定艺术风格的视觉特征和表现手法\",\n",
    "        \"semantic_columns\": [ \"Name\",  \"Theme\"]\n",
    "    },\n",
    "    \n",
    "    # 2个Filter (3个类别) - 必须包含至少一个图片属性和一个文字属性\n",
    "    {\n",
    "        \"name\": \"nationality_style\", \n",
    "        \"description\": \"国籍与艺术风格关联分析\", \n",
    "        \"filters\": [\"Nationality\", \"Style\"],\n",
    "        \"use_case\": \"探索不同国家艺术家偏好的艺术风格，分析地域文化对艺术表现的影响\",\n",
    "        \"semantic_columns\": [\"Nationality\",  \"Name\", \"Birth_continent\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"century_color\", \n",
    "        \"description\": \"历史时期与色彩运用研究\", \n",
    "        \"filters\": [\"Century\", \"Color\"],\n",
    "        \"use_case\": \"分析不同历史时期艺术作品的色彩特征和演变趋势\",\n",
    "        \"semantic_columns\": [\"Century\",  \"Name\", \"Age\", \"Style\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"age_theme\", \n",
    "        \"description\": \"艺术家年龄与作品主题关系\", \n",
    "        \"filters\": [\"Age\", \"Theme\"],\n",
    "        \"use_case\": \"研究艺术家人生阶段对创作主题选择的影响\",\n",
    "        \"semantic_columns\": [\"Age\",  \"Name\", \"Century\"]\n",
    "    },\n",
    "    \n",
    "    # 3个Filter (3个类别) - 必须包含至少一个图片属性和一个文字属性\n",
    "    {\n",
    "        \"name\": \"nationality_century_style\", \n",
    "        \"description\": \"国家-时代-风格综合分析\", \n",
    "        \"filters\": [\"Nationality\", \"Century\", \"Style\"],\n",
    "        \"use_case\": \"深度分析特定时期特定国家的主流艺术风格和特征\",\n",
    "        \"semantic_columns\": [\"Nationality\", \"Century\",  \"Name\", \"Birth_continent\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"marriage_age_tone\", \n",
    "        \"description\": \"婚姻状态与艺术表达情感分析\", \n",
    "        \"filters\": [\"Marriage\", \"Age\", \"Tone\"],\n",
    "        \"use_case\": \"探索艺术家的人生状态（婚姻、年龄）对作品情感色调的影响\",\n",
    "        \"semantic_columns\": [\"Marriage\", \"Age\",  \"Name\", \"Theme\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"continent_field_composition\", \n",
    "        \"description\": \"地域文化与艺术构图研究\", \n",
    "        \"filters\": [\"Birth_continent\", \"Field\", ],\n",
    "        \"use_case\": \"分析不同大洲的文化背景对艺术领域和构图方式的影响\",\n",
    "        \"semantic_columns\": [\"Birth_continent\", \"Field\",  \"Name\", \"Nationality\"]\n",
    "    },\n",
    "    \n",
    "    # 4个Filter (1个类别) - 必须包含至少一个图片属性和一个文字属性\n",
    "    {\n",
    "        \"name\": \"european_painting_masters\", \n",
    "        \"description\": \"欧洲绘画大师深度研究\", \n",
    "        \"filters\": [\"Birth_continent\", \"Field\", \"Age\", \"Style\"],\n",
    "        \"use_case\": \"专门研究欧洲绘画领域的成熟艺术家，分析其风格特征和创作规律\",\n",
    "        \"semantic_columns\": [\"Birth_continent\", \"Field\", \"Age\", \"Style\", \"Name\", \"Awards\"]\n",
    "    },\n",
    "    \n",
    "    # 5个Filter (1个类别) - 必须包含至少一个图片属性和一个文字属性  \n",
    "    {\n",
    "        \"name\": \"elite_artist_profile\", \n",
    "        \"description\": \"精英艺术家全方位画像\", \n",
    "        \"filters\": [\"Nationality\", \"Birth_continent\", \"Age\", \"Style\", \"Color\"],\n",
    "        \"use_case\": \"构建顶级艺术家的完整画像，包括地理背景、人生阶段、艺术风格和色彩偏好\",\n",
    "        \"semantic_columns\": [\"Nationality\", \"Birth_continent\", \"Age\", \"Style\",  \"Name\", \"Awards\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "QUERY_TYPES = [\"SFW\", \"SFWT\", \"SFWG\", \"SFWA\", \"SFAG\", \"SFWGA\", \"SFWGAT\"]\n",
    "\n",
    "class SQLGenerator:\n",
    "    def __init__(self, min_result_rows=5):\n",
    "        self.df = globals()['df']\n",
    "        self.attr_dict = globals()['attr_value_dict']\n",
    "        self.numerical_attr = globals()['numerical_attr']\n",
    "        self.non_numerical_attr = globals()['non_numerical_attr']\n",
    "        self.category_attr = globals()['category_attr']\n",
    "        self.formatted_attr = globals()['formatted_attr']\n",
    "        self.attr_desc_dict = globals()['attr_desc_dict']\n",
    "        self.min_rows = min_result_rows\n",
    "        \n",
    "        # 属性分类\n",
    "        self.pure_text_attributes = {\"Nationality\", \"Birth_date\", \"Death_date\", \"Age\", \"Century\", \n",
    "                                   \"Zodiac\", \"Birth_country\", \"Birth_city\", \"Birth_continent\", \n",
    "                                   \"Death_country\", \"Death_city\", \"Field\", \"Marriage\", \n",
    "                                   \"Art_institution\", \"Teaching\", \"Awards\"}\n",
    "        \n",
    "        self.pure_image_attributes = {\"Style\", \"Image_genre\", \"Color\", \"Tone\", \"Composition\"}\n",
    "        \n",
    "        self.common_attributes = {\"Name\"}\n",
    "        \n",
    "        # 分析数据分布用于放宽策略\n",
    "        self.stats = self._analyze_data()\n",
    "        \n",
    "    def _analyze_data(self):\n",
    "        \"\"\"分析数据分布\"\"\"\n",
    "        stats = {}\n",
    "        for attr in self.attr_dict.keys():\n",
    "            if attr in self.non_numerical_attr:\n",
    "                try:\n",
    "                    value_counts = self.df[attr].value_counts()\n",
    "                    stats[attr] = {\n",
    "                        \"type\": \"categorical\", \n",
    "                        \"top_values\": value_counts.head(5).to_dict()\n",
    "                    }\n",
    "                except:\n",
    "                    stats[attr] = {\"type\": \"categorical\", \"top_values\": {}}\n",
    "        return stats\n",
    "    \n",
    "    def get_filter_value(self, attr, relaxation_level=0):\n",
    "        \"\"\"从attr_value_dict获取值和条件，支持放宽策略\"\"\"\n",
    "        values = self.attr_dict.get(attr, [])\n",
    "        if not values:\n",
    "            return None, None\n",
    "        \n",
    "        if attr in self.numerical_attr:\n",
    "            val = random.choice(values)\n",
    "            \n",
    "            if relaxation_level == 0:\n",
    "                op = random.choice([\"==\", \">\", \">=\", \"<\", \"<=\"])\n",
    "            elif relaxation_level == 1:\n",
    "                op = random.choice([\">=\", \"<=\", \">\", \"<\", \">=\", \"<=\"])\n",
    "            elif relaxation_level == 2:\n",
    "                op = random.choice([\">=\", \"<=\"])\n",
    "            else:\n",
    "                op = random.choice([\">=\", \"<=\"])\n",
    "                \n",
    "            if relaxation_level >= 1:\n",
    "                if attr == \"Age\":\n",
    "                    sorted_vals = sorted(values)\n",
    "                    if op in [\">=\", \">\"]:\n",
    "                        val = random.choice(sorted_vals[:len(sorted_vals)//3])\n",
    "                    elif op in [\"<=\", \"<\"]:\n",
    "                        val = random.choice(sorted_vals[2*len(sorted_vals)//3:])\n",
    "                elif attr == \"Awards\":\n",
    "                    if op in [\">=\", \">\"]:\n",
    "                        min_awards = min([v for v in values if isinstance(v, (int, float))])\n",
    "                        val = min_awards if relaxation_level >= 2 else random.choice([v for v in values if v <= min_awards + 1])\n",
    "                elif attr == \"Person_count\":\n",
    "                    if op in [\">=\", \">\"]:\n",
    "                        val = random.choice([v for v in values if v <= 2])\n",
    "                    elif op in [\"<=\", \"<\"]:\n",
    "                        val = random.choice([v for v in values if v >= 3])\n",
    "            \n",
    "            return val, f\"{attr} {op} {val}\"\n",
    "        else:\n",
    "            if relaxation_level >= 1 and attr in self.stats and self.stats[attr][\"type\"] == \"categorical\":\n",
    "                top_values = list(self.stats[attr][\"top_values\"].keys())[:3]\n",
    "                common_values = [v for v in top_values if v in values]\n",
    "                if common_values:\n",
    "                    val = random.choice(common_values)\n",
    "                else:\n",
    "                    val = random.choice(values)\n",
    "            else:\n",
    "                val = random.choice(values)\n",
    "            return val, f\"{attr} == '{val}'\"\n",
    "    \n",
    "    def apply_filters_with_relaxation(self, filters_config, max_relaxation=3):\n",
    "        \"\"\"应用过滤条件，支持自动放宽策略\"\"\"\n",
    "        for relaxation_level in range(max_relaxation + 1):\n",
    "            result_indices, applied_conditions = self._try_apply_filters(filters_config, relaxation_level)\n",
    "            if len(result_indices) >= 2:\n",
    "                return result_indices, applied_conditions, relaxation_level\n",
    "        return None, None, \"FAILED_ALL_RELAXATION\"\n",
    "    \n",
    "    def _try_apply_filters(self, filters_config, relaxation_level):\n",
    "        \"\"\"尝试应用过滤条件，严格保持Filter数量\"\"\"\n",
    "        result_indices = set(range(len(self.df)))\n",
    "        applied_conditions = []\n",
    "        \n",
    "        for attr in filters_config:\n",
    "            val, condition = self.get_filter_value(attr, relaxation_level)\n",
    "            if not val or not condition:\n",
    "                return [], []\n",
    "                \n",
    "            try:\n",
    "                if condition.endswith(\"'\"):\n",
    "                    mask = self.df[attr].apply(lambda x: \n",
    "                        str(x).strip().lower() == str(val).strip().lower() or\n",
    "                        (isinstance(x, str) and '||' in x and str(val) in x.split('||')))\n",
    "                elif \">=\" in condition:\n",
    "                    mask = self.df[attr] >= val\n",
    "                elif \">\" in condition:\n",
    "                    mask = self.df[attr] > val\n",
    "                elif \"<=\" in condition:\n",
    "                    mask = self.df[attr] <= val\n",
    "                elif \"<\" in condition:\n",
    "                    mask = self.df[attr] < val\n",
    "                elif \"==\" in condition:\n",
    "                    mask = self.df[attr] == val\n",
    "                else:\n",
    "                    return [], []\n",
    "                \n",
    "                new_indices = set(self.df[mask].index)\n",
    "                result_indices &= new_indices\n",
    "                applied_conditions.append(condition)\n",
    "                \n",
    "            except Exception:\n",
    "                return [], []\n",
    "        \n",
    "        if len(applied_conditions) != len(filters_config):\n",
    "            return [], []\n",
    "        \n",
    "        return list(result_indices), applied_conditions\n",
    "    \n",
    "    def create_schema_sql(self, attrs, table_name=\"Wikiart\"):\n",
    "        \"\"\"生成建表SQL\"\"\"\n",
    "        schema_parts = []\n",
    "        for attr in set(attrs):\n",
    "            if '(' in attr:\n",
    "                continue\n",
    "            if attr in self.numerical_attr:\n",
    "                schema_parts.append(f\"    {attr} FLOAT\")\n",
    "            elif attr in self.formatted_attr:\n",
    "                schema_parts.append(f\"    {attr} DATE\")\n",
    "            else:\n",
    "                schema_parts.append(f\"    {attr} VARCHAR(255)\")\n",
    "        \n",
    "        return f\"CREATE TABLE {table_name} (\\n\" + \",\\n\".join(schema_parts) + \"\\n);\"\n",
    "    \n",
    "    def validate_template_requirements(self, template):\n",
    "        \"\"\"验证模板是否符合图文结合要求\"\"\"\n",
    "        filters = set(template[\"filters\"])\n",
    "        filter_count = len(filters)\n",
    "        \n",
    "        if filter_count >= 2:\n",
    "            has_image_attr = bool(filters & self.pure_image_attributes)\n",
    "            has_text_attr = bool(filters & self.pure_text_attributes)\n",
    "            return has_image_attr and has_text_attr\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_semantic_columns(self, template, qtype):\n",
    "        \"\"\"根据模板和查询类型选择语义相关的列\"\"\"\n",
    "        semantic_cols = template.get(\"semantic_columns\", template[\"filters\"])\n",
    "        \n",
    "        if qtype == \"SFW\":\n",
    "            return semantic_cols[:3]\n",
    "        elif qtype in [\"SFWT\", \"SFWG\"]:\n",
    "            return semantic_cols[:2]\n",
    "        elif qtype in [\"SFWA\", \"SFAG\", \"SFWGA\", \"SFWGAT\"]:\n",
    "            return semantic_cols[:2]\n",
    "        \n",
    "        return semantic_cols[:3]\n",
    "    \n",
    "    def get_semantic_group_column(self, template):\n",
    "        \"\"\"为模板选择语义相关的分组列\"\"\"\n",
    "        filters = set(template[\"filters\"])\n",
    "        \n",
    "        relevant_categories = [attr for attr in self.category_attr if attr in filters]\n",
    "        if relevant_categories:\n",
    "            return random.choice(relevant_categories)\n",
    "        \n",
    "        semantic_groups = {\n",
    "            \"nationality_focus\": [\"Nationality\", \"Birth_continent\"],\n",
    "            \"style_focus\": [\"Style\", \"Image_genre\"],\n",
    "            \"nationality_style\": [\"Nationality\", \"Style\"],\n",
    "            \"century_color\": [\"Century\", \"Style\"],\n",
    "            \"age_theme\": [\"Century\", \"Marriage\"],\n",
    "            \"nationality_century_style\": [\"Nationality\", \"Century\"],\n",
    "            \"marriage_age_tone\": [\"Marriage\", \"Century\"],\n",
    "            \"continent_field_composition\": [\"Birth_continent\", \"Field\"],\n",
    "            \"european_painting_masters\": [\"Field\", \"Century\"],\n",
    "            \"elite_artist_profile\": [\"Nationality\", \"Birth_continent\"]\n",
    "        }\n",
    "        \n",
    "        preferred = semantic_groups.get(template[\"name\"], self.category_attr)\n",
    "        available = [col for col in preferred if col in self.category_attr]\n",
    "        \n",
    "        return random.choice(available) if available else random.choice(self.category_attr)\n",
    "    \n",
    "    def get_semantic_aggregation_column(self, template):\n",
    "        \"\"\"为模板选择语义相关的聚合列\"\"\"\n",
    "        filters = set(template[\"filters\"])\n",
    "        \n",
    "        relevant_numerics = [attr for attr in self.numerical_attr if attr in filters]\n",
    "        if relevant_numerics:\n",
    "            return random.choice(relevant_numerics)\n",
    "        \n",
    "        semantic_aggs = {\n",
    "            \"nationality_focus\": [\"Age\", \"Awards\"],\n",
    "            \"style_focus\": [\"Style\", \"Image_genre\"],\n",
    "            \"nationality_style\": [\"Age\", \"Awards\"],\n",
    "            \"century_color\": [\"Age\", \"Person_count\"],\n",
    "            \"age_theme\": [\"Age\", \"Person_count\"],\n",
    "            \"nationality_century_style\": [\"Age\", \"Awards\"],\n",
    "            \"marriage_age_tone\": [\"Age\", \"Person_count\"],\n",
    "            \"continent_field_composition\": [\"Age\", \"Awards\"],\n",
    "            \"european_painting_masters\": [\"Age\", \"Awards\"],\n",
    "            \"elite_artist_profile\": [\"Age\", \"Awards\"]\n",
    "        }\n",
    "        \n",
    "        preferred = semantic_aggs.get(template[\"name\"], self.numerical_attr)\n",
    "        available = [col for col in preferred if col in self.numerical_attr]\n",
    "        \n",
    "        return random.choice(available) if available else random.choice(self.numerical_attr)\n",
    "    \n",
    "    def generate_query_sql(self, template, qtype, query_id):\n",
    "        \"\"\"生成单个查询的SQL，带语义化改进\"\"\"\n",
    "        \n",
    "        if not self.validate_template_requirements(template):\n",
    "            return None\n",
    "        # SFAG 不需要WHERE条件，直接使用所有行\n",
    "        if qtype == \"SFAG\":\n",
    "            indices = list(range(len(self.df)))\n",
    "            conditions = []\n",
    "            relaxation_used = 0\n",
    "        else:\n",
    "            result = self.apply_filters_with_relaxation(template[\"filters\"])\n",
    "            \n",
    "            if result[2] == \"FAILED_ALL_RELAXATION\":\n",
    "                return None\n",
    "            \n",
    "            indices, conditions, relaxation_used = result\n",
    "            \n",
    "            if len(indices) < 2 or not conditions:\n",
    "                return None\n",
    "            \n",
    "        result = self.apply_filters_with_relaxation(template[\"filters\"])\n",
    "        \n",
    "        if result[2] == \"FAILED_ALL_RELAXATION\":\n",
    "            return None\n",
    "        \n",
    "        indices, conditions, relaxation_used = result\n",
    "        \n",
    "        if len(indices) < 2 or not conditions:\n",
    "            return None\n",
    "        \n",
    "        select_cols = self.get_semantic_columns(template, qtype)\n",
    "        schema_attrs = select_cols.copy()\n",
    "        \n",
    "        sql_parts = []\n",
    "        \n",
    "        if qtype == \"SFW\":\n",
    "            sql_parts.append(f\"SELECT {', '.join(select_cols)}\")\n",
    "            \n",
    "        elif qtype == \"SFWT\":\n",
    "            numeric_cols = [c for c in select_cols if c in self.numerical_attr]\n",
    "            if not numeric_cols:\n",
    "                numeric_col = self.get_semantic_aggregation_column(template)\n",
    "                select_cols.append(numeric_col)\n",
    "                schema_attrs.append(numeric_col)\n",
    "                numeric_cols = [numeric_col]\n",
    "            \n",
    "            order_col = random.choice(numeric_cols)\n",
    "            order_dir = random.choice([\"ASC\", \"DESC\"])\n",
    "            limit_val = random.choice([5, 10, 15, 20])\n",
    "            \n",
    "            sql_parts.extend([\n",
    "                f\"SELECT {', '.join(select_cols)}\",\n",
    "                \"FROM Wikiart\",\n",
    "                f\"WHERE {' AND '.join(conditions)}\",\n",
    "                f\"ORDER BY {order_col} {order_dir}\",\n",
    "                f\"LIMIT {limit_val}\"\n",
    "            ])\n",
    "            \n",
    "        elif qtype == \"SFWG\":\n",
    "            group_col = self.get_semantic_group_column(template)\n",
    "            if group_col not in select_cols:\n",
    "                select_cols.append(group_col)\n",
    "                schema_attrs.append(group_col)\n",
    "            \n",
    "            sql_parts.extend([\n",
    "                f\"SELECT {', '.join(select_cols)}\",\n",
    "                \"FROM Wikiart\",\n",
    "                f\"WHERE {' AND '.join(conditions)}\",\n",
    "                f\"GROUP BY {group_col}\"\n",
    "            ])\n",
    "            \n",
    "        elif qtype == \"SFWA\":\n",
    "            func = random.choice([\"COUNT\", \"MAX\", \"MIN\", \"AVG\", \"SUM\"])\n",
    "            if func == \"COUNT\" and random.random() < 0.3:\n",
    "                select_cols = [f\"{func}(*)\"]\n",
    "                schema_attrs = template[\"filters\"]\n",
    "            else:\n",
    "                agg_col = self.get_semantic_aggregation_column(template)\n",
    "                select_cols = [f\"{func}({agg_col})\"]\n",
    "                schema_attrs = [agg_col] + template[\"filters\"]\n",
    "            \n",
    "            sql_parts.append(f\"SELECT {', '.join(select_cols)}\")\n",
    "        \n",
    "        elif qtype == \"SFAG\":\n",
    "            group_col = self.get_semantic_group_column(template)\n",
    "            func = random.choice([\"COUNT\", \"MAX\", \"MIN\", \"AVG\", \"SUM\"])\n",
    "            \n",
    "            if func == \"COUNT\" and random.random() < 0.3:\n",
    "                select_cols = [group_col, f\"{func}(*)\"]\n",
    "                schema_attrs = [group_col] + template[\"filters\"]\n",
    "            else:\n",
    "                agg_col = self.get_semantic_aggregation_column(template)\n",
    "                select_cols = [group_col, f\"{func}({agg_col})\"]\n",
    "                schema_attrs = [group_col, agg_col] + template[\"filters\"]\n",
    "            \n",
    "            sql_parts.extend([\n",
    "                f\"SELECT {', '.join(select_cols)}\",\n",
    "                \"FROM Wikiart\",\n",
    "                f\"GROUP BY {group_col}\"\n",
    "            ])\n",
    "        \n",
    "        elif qtype == \"SFWGA\":\n",
    "            group_col = self.get_semantic_group_column(template)\n",
    "            func = random.choice([\"COUNT\", \"MAX\", \"MIN\", \"AVG\", \"SUM\"])\n",
    "            \n",
    "            if func == \"COUNT\" and random.random() < 0.3:\n",
    "                select_cols = [group_col, f\"{func}(*)\"]\n",
    "                schema_attrs = [group_col] + template[\"filters\"]\n",
    "            else:\n",
    "                agg_col = self.get_semantic_aggregation_column(template)\n",
    "                select_cols = [group_col, f\"{func}({agg_col})\"]\n",
    "                schema_attrs = [group_col, agg_col] + template[\"filters\"]\n",
    "            \n",
    "            sql_parts.extend([\n",
    "                f\"SELECT {', '.join(select_cols)}\",\n",
    "                \"FROM Wikiart\",\n",
    "                f\"WHERE {' AND '.join(conditions)}\",\n",
    "                f\"GROUP BY {group_col}\"\n",
    "            ])\n",
    "            \n",
    "        elif qtype == \"SFWGAT\":\n",
    "            group_col = self.get_semantic_group_column(template)\n",
    "            func = random.choice([\"MAX\", \"MIN\", \"AVG\", \"SUM\"])\n",
    "            agg_col = self.get_semantic_aggregation_column(template)\n",
    "            order_dir = random.choice([\"ASC\", \"DESC\"])\n",
    "            limit_val = random.choice([5, 10, 15])\n",
    "            \n",
    "            select_cols = [group_col, f\"{func}({agg_col})\"]\n",
    "            schema_attrs = [group_col, agg_col] + template[\"filters\"]\n",
    "            \n",
    "            sql_parts.extend([\n",
    "                f\"SELECT {', '.join(select_cols)}\",\n",
    "                \"FROM Wikiart\",\n",
    "                f\"WHERE {' AND '.join(conditions)}\",\n",
    "                f\"GROUP BY {group_col}\",\n",
    "                f\"ORDER BY {func}({agg_col}) {order_dir}\",\n",
    "                f\"LIMIT {limit_val}\"\n",
    "            ])\n",
    "        \n",
    "        # 添加基本的FROM和WHERE（如果还没有）\n",
    "        if len(sql_parts) == 1:  # 只有SELECT\n",
    "            sql_parts.extend([\n",
    "                \"FROM Wikiart\",\n",
    "                f\"WHERE {' AND '.join(conditions)}\"\n",
    "            ])\n",
    "        \n",
    "        # 确保schema包含所有用到的属性\n",
    "        for condition in conditions:\n",
    "            attr_name = condition.split()[0]\n",
    "            if attr_name not in schema_attrs:\n",
    "                schema_attrs.append(attr_name)\n",
    "        \n",
    "        # 生成完整SQL\n",
    "        schema_sql = self.create_schema_sql(schema_attrs)\n",
    "        query_sql = \"\\n\".join(sql_parts) + \";\"\n",
    "        \n",
    "        # 分析模板的图文属性组合\n",
    "        template_filters = set(template[\"filters\"])\n",
    "        image_filters = template_filters & self.pure_image_attributes\n",
    "        text_filters = template_filters & self.pure_text_attributes\n",
    "        common_filters = template_filters & self.common_attributes\n",
    "        \n",
    "        header = f\"-- Query {query_id} - {qtype}\\n\"\n",
    "        header += f\"-- Template: {template['name']}\\n\"\n",
    "        header += f\"-- Description: {template['description']}\\n\"\n",
    "        header += f\"-- Use Case: {template['use_case']}\\n\"\n",
    "        header += f\"-- Result Rows: {len(indices)}\\n\"\n",
    "        header += f\"-- Filter Composition:\\n\"\n",
    "        header += f\"--   ├─ Image Attributes: {', '.join(image_filters) if image_filters else 'None'}\\n\"\n",
    "        header += f\"--   ├─ Text Attributes: {', '.join(text_filters) if text_filters else 'None'}\\n\"\n",
    "        header += f\"--   └─ Common Attributes: {', '.join(common_filters) if common_filters else 'None'}\\n\"\n",
    "        header += f\"-- Filters Applied: {len(conditions)}/{len(template['filters'])} (EXACT MATCH REQUIRED)\"\n",
    "        if relaxation_used > 0:\n",
    "            header += f\" (Values relaxed {relaxation_used} times)\"\n",
    "        header += \"\\n\\n\"\n",
    "        \n",
    "        complete_sql = header + schema_sql + \"\\n\\n\" + query_sql + \"\\n\\n\" + \"-\" * 60 + \"\\n\\n\"\n",
    "        \n",
    "        return complete_sql\n",
    "\n",
    "def validate_all_templates():\n",
    "    \"\"\"验证所有模板是否符合图文结合要求\"\"\"\n",
    "    pure_text_attributes = {\"Nationality\", \"Birth_date\", \"Death_date\", \"Age\", \"Century\", \n",
    "                           \"Zodiac\", \"Birth_country\", \"Birth_city\", \"Birth_continent\", \n",
    "                           \"Death_country\", \"Death_city\", \"Field\", \"Marriage\", \n",
    "                           \"Art_institution\", \"Teaching\", \"Awards\"}\n",
    "    \n",
    "    pure_image_attributes = { \"Style\", \"Image_genre\", \"Color\", \"Tone\", \"Composition\"}\n",
    "    \n",
    "    valid_templates = 0\n",
    "    total_templates = len(TEMPLATES)\n",
    "    \n",
    "    for template in TEMPLATES:\n",
    "        filters = set(template[\"filters\"])\n",
    "        filter_count = len(filters)\n",
    "        \n",
    "        if filter_count >= 2:\n",
    "            has_image_attr = bool(filters & pure_image_attributes)\n",
    "            has_text_attr = bool(filters & pure_text_attributes)\n",
    "            if has_image_attr and has_text_attr:\n",
    "                valid_templates += 1\n",
    "        else:\n",
    "            valid_templates += 1\n",
    "    \n",
    "    return valid_templates == total_templates\n",
    "\n",
    "def generate_all_sql_files():\n",
    "    \"\"\"生成所有SQL文件，按类别和类型分文件夹\"\"\"\n",
    "    \n",
    "    if not validate_all_templates():\n",
    "        return\n",
    "    \n",
    "    base_dir = \"./Image_Text_Combined_Filters/\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    generator = SQLGenerator()\n",
    "    queries_per_type = 5\n",
    "    \n",
    "    total_generated = 0\n",
    "    \n",
    "    for template in tqdm(TEMPLATES, desc=\"生成SQL\"):\n",
    "        template_dir = os.path.join(base_dir, template[\"name\"])\n",
    "        os.makedirs(template_dir, exist_ok=True)\n",
    "        \n",
    "        template_count = 0\n",
    "        \n",
    "        for qtype in QUERY_TYPES:\n",
    "            sql_content = []\n",
    "            query_id = 1\n",
    "            \n",
    "            generated = 0\n",
    "            attempts = 0\n",
    "            max_attempts = queries_per_type * 5\n",
    "            \n",
    "            while generated < queries_per_type and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                \n",
    "                sql = generator.generate_query_sql(template, qtype, query_id)\n",
    "                if sql:\n",
    "                    sql_content.append(sql)\n",
    "                    generated += 1\n",
    "                    query_id += 1\n",
    "            \n",
    "            template_count += generated\n",
    "            \n",
    "            filename = os.path.join(template_dir, f\"{qtype}.sql\")\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"-- {template['description']} - {qtype} 查询集合\\n\")\n",
    "                f.write(f\"-- 模板: {template['name']}\\n\")\n",
    "                f.write(f\"-- Filter数量: {len(template['filters'])}\\n\")\n",
    "                f.write(f\"-- 用途: {template['use_case']}\\n\")\n",
    "                f.write(\"-- \" + \"=\" * 60 + \"\\n\\n\")\n",
    "                \n",
    "                if sql_content:\n",
    "                    f.write(\"\".join(sql_content))\n",
    "                else:\n",
    "                    f.write(\"-- 注意: 未能生成有效查询\\n\")\n",
    "        \n",
    "        total_generated += template_count\n",
    "    \n",
    "    print(f\"完成! 共生成 {total_generated} 个查询，保存在: {base_dir}\")\n",
    "\n",
    "# 执行生成\n",
    "generate_all_sql_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
